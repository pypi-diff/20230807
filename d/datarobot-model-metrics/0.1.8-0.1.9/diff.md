# Comparing `tmp/datarobot_model_metrics-0.1.8-py2.py3-none-any.whl.zip` & `tmp/datarobot_model_metrics-0.1.9-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,24 +1,24 @@
-Zip file size: 29456 bytes, number of entries: 22
--rw-r--r--  2.0 unx      385 b- defN 23-Jul-24 14:24 dmm/__init__.py
--rw-r--r--  2.0 unx     9663 b- defN 23-Jul-24 14:24 dmm/batch_metric_evaluator.py
--rw-r--r--  2.0 unx      771 b- defN 23-Jul-24 14:24 dmm/constants.py
--rw-r--r--  2.0 unx     6955 b- defN 23-Jul-24 14:24 dmm/example_data_helper.py
--rw-r--r--  2.0 unx    14267 b- defN 23-Jul-24 14:24 dmm/metric_evaluator.py
--rw-r--r--  2.0 unx     3428 b- defN 23-Jul-24 14:24 dmm/time_bucket.py
--rw-r--r--  2.0 unx     1690 b- defN 23-Jul-24 14:24 dmm/utils.py
--rw-r--r--  2.0 unx      525 b- defN 23-Jul-24 14:24 dmm/data_source/__init__.py
--rw-r--r--  2.0 unx     2096 b- defN 23-Jul-24 14:24 dmm/data_source/data_source_base.py
--rw-r--r--  2.0 unx     3113 b- defN 23-Jul-24 14:24 dmm/data_source/dataframe_source.py
--rw-r--r--  2.0 unx    60597 b- defN 23-Jul-24 14:24 dmm/data_source/datarobot_source.py
--rw-r--r--  2.0 unx     3003 b- defN 23-Jul-24 14:24 dmm/data_source/generator_source.py
--rw-r--r--  2.0 unx      396 b- defN 23-Jul-24 14:24 dmm/metric/__init__.py
--rw-r--r--  2.0 unx     1620 b- defN 23-Jul-24 14:24 dmm/metric/asymmetric_error.py
--rw-r--r--  2.0 unx      320 b- defN 23-Jul-24 14:24 dmm/metric/median_absolute_error.py
--rw-r--r--  2.0 unx     3569 b- defN 23-Jul-24 14:24 dmm/metric/metric_base.py
--rw-r--r--  2.0 unx      957 b- defN 23-Jul-24 14:24 dmm/metric/missing_values.py
--rw-r--r--  2.0 unx     2014 b- defN 23-Jul-24 14:24 dmm/metric/sklearn_metric.py
--rw-r--r--  2.0 unx      886 b- defN 23-Jul-24 14:25 datarobot_model_metrics-0.1.8.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Jul-24 14:25 datarobot_model_metrics-0.1.8.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 23-Jul-24 14:25 datarobot_model_metrics-0.1.8.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1857 b- defN 23-Jul-24 14:25 datarobot_model_metrics-0.1.8.dist-info/RECORD
-22 files, 118226 bytes uncompressed, 26430 bytes compressed:  77.6%
+Zip file size: 29638 bytes, number of entries: 22
+-rw-r--r--  2.0 unx      385 b- defN 23-Aug-07 16:30 dmm/__init__.py
+-rw-r--r--  2.0 unx     9663 b- defN 23-Aug-07 16:30 dmm/batch_metric_evaluator.py
+-rw-r--r--  2.0 unx      771 b- defN 23-Aug-07 16:30 dmm/constants.py
+-rw-r--r--  2.0 unx     6955 b- defN 23-Aug-07 16:30 dmm/example_data_helper.py
+-rw-r--r--  2.0 unx    14267 b- defN 23-Aug-07 16:30 dmm/metric_evaluator.py
+-rw-r--r--  2.0 unx     3428 b- defN 23-Aug-07 16:30 dmm/time_bucket.py
+-rw-r--r--  2.0 unx     1690 b- defN 23-Aug-07 16:30 dmm/utils.py
+-rw-r--r--  2.0 unx      525 b- defN 23-Aug-07 16:30 dmm/data_source/__init__.py
+-rw-r--r--  2.0 unx     2096 b- defN 23-Aug-07 16:30 dmm/data_source/data_source_base.py
+-rw-r--r--  2.0 unx     3113 b- defN 23-Aug-07 16:30 dmm/data_source/dataframe_source.py
+-rw-r--r--  2.0 unx    62285 b- defN 23-Aug-07 16:30 dmm/data_source/datarobot_source.py
+-rw-r--r--  2.0 unx     3003 b- defN 23-Aug-07 16:30 dmm/data_source/generator_source.py
+-rw-r--r--  2.0 unx      396 b- defN 23-Aug-07 16:30 dmm/metric/__init__.py
+-rw-r--r--  2.0 unx     1620 b- defN 23-Aug-07 16:30 dmm/metric/asymmetric_error.py
+-rw-r--r--  2.0 unx      320 b- defN 23-Aug-07 16:30 dmm/metric/median_absolute_error.py
+-rw-r--r--  2.0 unx     3569 b- defN 23-Aug-07 16:30 dmm/metric/metric_base.py
+-rw-r--r--  2.0 unx      957 b- defN 23-Aug-07 16:30 dmm/metric/missing_values.py
+-rw-r--r--  2.0 unx     2014 b- defN 23-Aug-07 16:30 dmm/metric/sklearn_metric.py
+-rw-r--r--  2.0 unx      886 b- defN 23-Aug-07 16:32 datarobot_model_metrics-0.1.9.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-Aug-07 16:32 datarobot_model_metrics-0.1.9.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 23-Aug-07 16:32 datarobot_model_metrics-0.1.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1857 b- defN 23-Aug-07 16:32 datarobot_model_metrics-0.1.9.dist-info/RECORD
+22 files, 119914 bytes uncompressed, 26612 bytes compressed:  77.8%
```

## zipnote {}

```diff
@@ -48,20 +48,20 @@
 
 Filename: dmm/metric/missing_values.py
 Comment: 
 
 Filename: dmm/metric/sklearn_metric.py
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.8.dist-info/METADATA
+Filename: datarobot_model_metrics-0.1.9.dist-info/METADATA
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.8.dist-info/WHEEL
+Filename: datarobot_model_metrics-0.1.9.dist-info/WHEEL
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.8.dist-info/top_level.txt
+Filename: datarobot_model_metrics-0.1.9.dist-info/top_level.txt
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.8.dist-info/RECORD
+Filename: datarobot_model_metrics-0.1.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dmm/data_source/datarobot_source.py

```diff
@@ -48,27 +48,29 @@
         token: str,
         deployment_id: str,
         start: datetime.datetime,
         end: datetime.datetime,
         model_id: str = None,
         max_rows: int = 10000,
         delete_exports: bool = False,
+        actuals_with_matched_predictions: bool = True,
     ):
         super().__init__(max_rows)
         if max_rows <= 0:
             raise Exception(f"max_rows must be > 0, got {max_rows}")
 
         self._deployment_id = deployment_id
         self._start = start
         self._end = end
         self._model_id = model_id
         self._delete_exports = delete_exports
         self._current_chunk_id = 0
         self._prev_chunk_datetime = None
         self._actuals_caches = {}
+        self._actuals_with_matched_predictions = actuals_with_matched_predictions
 
         api = DataRobotApiClient(token, base_url)
         self._api = api
         self._deployment = Deployment(self._api, deployment_id)
         self._prediction_data_provider = self._get_prediction_data_provider()
         self._actuals_data_provider = self._get_actuals_data_provider()
         self._association_id = api.get_association_id(deployment_id)
@@ -219,14 +221,15 @@
             deployment_id=self._deployment_id,
             start=self._start,
             end=self._end,
             model_id=self._model_id,
             time_bucket=self._time_bucket,
             max_rows=self.max_rows,
             delete_exports=self._delete_exports,
+            only_matched_predictions=self._actuals_with_matched_predictions,
         )
 
     def _get_prediction_data_provider(self) -> PredictionDataExportProvider:
         """
         Retrieves a new instance of the PredictionDataExportProvider class.
         """
         return PredictionDataExportProvider(
@@ -422,14 +425,15 @@
         base_url: str,
         token: str,
         deployment_id: str,
         batch_ids: List[str],
         model_id: str = None,
         max_rows: int = 10000,
         delete_exports: bool = False,
+        actuals_with_matched_predictions: bool = True,
     ):
         super().__init__(max_rows)
         if max_rows <= 0:
             raise Exception(f"max_rows must be > 0, got {max_rows}")
 
         self._deployment_id = deployment_id
         self._model_id = model_id
@@ -440,14 +444,15 @@
         api = DataRobotApiClient(token, base_url)
         self._api = api
         self._deployment = Deployment(self._api, deployment_id)
         self._start = self._deployment.created_at
         self._end = datetime.datetime.utcnow().replace(tzinfo=pytz.UTC)
         self._delete_exports = delete_exports
         self._prediction_data_provider = self._get_prediction_data_provider()
+        self._actuals_with_matched_predictions = actuals_with_matched_predictions
 
     def init(self, time_bucket: TimeBucket):
         pass
 
     def reset(self):
         self._prediction_data_provider = self._get_prediction_data_provider()
 
@@ -592,27 +597,29 @@
         end: datetime.datetime,
         timestamp_col: str,
         max_rows: int,
         time_bucket: TimeBucket,
         start_export: Callable,
         get_export_dataset_ids: Callable,
         delete_exports: bool,
+        actuals_with_matched_predictions: bool = True,
     ):
         self._api = api
         self._deployment_id = deployment_id
         self._model_id = model_id
         self._deployment = Deployment(self._api, deployment_id)
         self._start = start
         self._end = end
         self._time_bucket = time_bucket
         self._max_rows = max_rows
         self._timestamp_col = timestamp_col
         self._start_export = start_export
         self._get_export_dataset_ids = get_export_dataset_ids
         self._delete_exports = delete_exports
+        self._actuals_with_matched_predictions = actuals_with_matched_predictions
         self._time_bucket_chunks = self._get_time_bucket_chunks()
 
         self._current_chunk_id = 0
         self._prev_bucket_id = None
 
     def set_new_export_parameters(
         self,
@@ -644,14 +651,15 @@
                         model_id=self._model_id,
                         start_dt=self._start,
                         end_dt=self._end,
                         start_export=self._start_export,
                         get_export_dataset_ids=self._get_export_dataset_ids,
                         sort_column=self._timestamp_col,
                         delete_exports=self._delete_exports,
+                        actuals_with_matched_predictions=self._actuals_with_matched_predictions,
                     )
                 )
             ),
         )
 
     def _update_chunk_info(self, bucket_id: any):
         """
@@ -739,25 +747,27 @@
         model_id: str | None,
         start: datetime.datetime,
         end: datetime.datetime,
         max_rows: int,
         start_export: Callable,
         get_export_dataset_ids: Callable,
         delete_exports: bool,
+        actuals_with_matched_predictions: bool = True,
     ):
         self._api = api
         self._deployment_id = deployment_id
         self._model_id = model_id
         self._deployment = Deployment(self._api, deployment_id)
         self._start = start
         self._end = end
         self._max_rows = max_rows
         self._start_export = start_export
         self._get_export_dataset_ids = get_export_dataset_ids
         self._delete_exports = delete_exports
+        self._actuals_with_matched_predictions = actuals_with_matched_predictions
         self._batch_bucket_chunks = self._get_batch_bucket_chunks()
 
         self._current_chunk_id = 0
         self._prev_bucket_id = None
 
     def set_new_export_parameters(self, max_rows: int) -> DataRobotBatchExportBase:
         self._max_rows = max_rows
@@ -779,14 +789,15 @@
                         deployment_id=self._deployment_id,
                         model_id=self._model_id,
                         start_dt=self._start,
                         end_dt=self._end,
                         start_export=self._start_export,
                         get_export_dataset_ids=self._get_export_dataset_ids,
                         delete_exports=self._delete_exports,
+                        actuals_with_matched_predictions=self._actuals_with_matched_predictions,
                     )
                 )
             ),
         )
 
     def _update_chunk_info(self, bucket_id: str):
         """
@@ -861,14 +872,15 @@
         start: datetime.datetime,
         end: datetime.datetime,
         model_id: str = None,
         timestamp_col: str = ColumnName.TIMESTAMP,
         max_rows: int = 10000,
         time_bucket: TimeBucket = TimeBucket.ALL,
         delete_exports: bool = False,
+        only_matched_predictions: bool = True,
     ):
         if max_rows <= 0:
             raise Exception(f"max_rows must be > 0, got {max_rows}")
 
         super().__init__(
             api=api,
             deployment_id=deployment_id,
@@ -877,14 +889,15 @@
             end=end,
             timestamp_col=timestamp_col,
             max_rows=max_rows,
             time_bucket=time_bucket,
             start_export=api.start_actuals_export,
             get_export_dataset_ids=api.get_actuals_export_dataset_ids,
             delete_exports=delete_exports,
+            actuals_with_matched_predictions=only_matched_predictions,
         )
 
     def get_data(
         self, return_original_column_names: bool = False
     ) -> (pd.DataFrame, int):
         """
         Method to return a chunk of actuals data that can be sent to a metric object to be transformed.
@@ -1147,26 +1160,28 @@
 
     def start_prediction_data_export(
         self,
         deployment_id: str,
         start: datetime.datetime,
         end: datetime.datetime,
         model_id: Union[str, None],
+        **kwargs,
     ) -> requests.Response:
         return self._start_prediction_data_export(
             deployment_id, start, end, model_id, batch_ids=None
         )
 
     def start_batch_prediction_data_export(
         self,
         batch_ids: List[str],
         deployment_id: str,
         start: datetime.datetime,
         end: datetime.datetime,
         model_id: Union[str, None],
+        **kwargs,
     ) -> requests.Response:
         return self._start_prediction_data_export(
             deployment_id, start, end, model_id, batch_ids=batch_ids
         )
 
     def _start_prediction_data_export(
         self,
@@ -1203,31 +1218,40 @@
 
     def start_actuals_export(
         self,
         deployment_id: str,
         start: datetime.datetime,
         end: datetime.datetime,
         model_id: Union[str, None],
+        only_matched_predictions: bool = True,
+        **kwargs,
     ) -> requests.Response:
         """
         Starts actuals data export between start (inclusive) and end (exclusive).
         We cannot assume anything about the order of returned records.
 
         Parameters
         ----------
         deployment_id: str
         start: datetime
             Inclusive start of the time range.
         end: datetime
             Exclusive end of the time range.
         model_id: Optional[str]
+        only_matched_predictions : bool
+            If true, exports actuals with matching predictions only.
         """
         return self._client.post(
             f"deployments/{deployment_id}/actualsDataExports/",
-            data={"start": start, "end": end, "modelId": model_id},
+            data={
+                "start": start,
+                "end": end,
+                "modelId": model_id,
+                "onlyMatchedPredictions": only_matched_predictions,
+            },
         )
 
     def start_training_data_export(
         self,
         deployment_id: str,
         model_id: Union[str, None],
     ) -> requests.Response:
@@ -1417,24 +1441,26 @@
         model_id: str | None,
         start_dt: datetime.datetime,
         end_dt: datetime.datetime,
         start_export: Callable,
         get_export_dataset_ids: Callable,
         delete_exports: bool = False,
         sort_column: Optional[str] = None,
+        actuals_with_matched_predictions: bool = True,
     ):
         self._deployment_id = deployment_id
         self._model_id = model_id
         self._start_dt = start_dt
         self._end_dt = end_dt
         self._sort_column = sort_column
         self._api = api
         self._start_export = start_export
         self._get_export_dataset_ids = get_export_dataset_ids
         self._delete_exports = delete_exports
+        self._actuals_with_matched_predictions = actuals_with_matched_predictions
 
     def __iter__(self) -> Generator[pd.DataFrame, None, None]:
         """
         Returns an iterator over chunks for time range defined in the constructor. It returns data sorted by timestamp.
         The way it works is as follows
         - it is a generator to lazily fetch data from DR API,
         - it will try to perform a full export (start_dt to end_dt),
@@ -1506,27 +1532,32 @@
 
     def _get_export_response(
         self, from_dt: datetime.datetime, suggested_interval: datetime.timedelta
     ) -> (requests.Response, datetime.timedelta):
         # we try to increase interval to mitigate too aggressive shrinking
         interval = min(self._end_dt - from_dt, 2 * suggested_interval)
         response = self._start_export(
-            self._deployment_id,
-            from_dt,
-            from_dt + interval,
-            self._model_id,
+            deployment_id=self._deployment_id,
+            start=from_dt,
+            end=from_dt + interval,
+            model_id=self._model_id,
+            only_matched_predictions=self._actuals_with_matched_predictions,
         )
 
         while response.status_code == 413:
             interval /= 2
             logger.info(
                 f"requested too many rows. trying smaller interval {from_dt} - {from_dt + interval}"
             )
             response = self._start_export(
-                self._deployment_id, from_dt, from_dt + interval, self._model_id
+                deployment_id=self._deployment_id,
+                start=from_dt,
+                end=from_dt + interval,
+                model_id=self._model_id,
+                only_matched_predictions=self._actuals_with_matched_predictions,
             )
 
         if response.status_code != 202:
             raise Exception(
                 f"DataRobot export API failed with {response.status_code}: {response.text}"
             )
         return response, interval
```

## Comparing `datarobot_model_metrics-0.1.8.dist-info/METADATA` & `datarobot_model_metrics-0.1.9.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: datarobot-model-metrics
-Version: 0.1.8
+Version: 0.1.9
 Summary: datarobot-model-metrics is a framework to compute model ML metrics
 Home-page: https://github.com/datarobot/datarobot-model-metrics
 Author: DataRobot
 Author-email: info@datarobot.com
 License: DataRobot
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

## Comparing `datarobot_model_metrics-0.1.8.dist-info/RECORD` & `datarobot_model_metrics-0.1.9.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -4,19 +4,19 @@
 dmm/example_data_helper.py,sha256=oLcOTo-dNV0Pyapq9G6e7DZoVjfIZMWj7QrkBx61Mj4,6955
 dmm/metric_evaluator.py,sha256=0JZlhxb0Kcfx52HreUrAYDVfHP4uL2iiIldUVWhDkzM,14267
 dmm/time_bucket.py,sha256=vgnKcX8PSqfw5TC8rRxO1Xp25jUBTjzGkKINuEBj7Sg,3428
 dmm/utils.py,sha256=kaZ7erHZofSwqZL1ddd8GXK3wrZfvYZ7I0ziz2CgbL0,1690
 dmm/data_source/__init__.py,sha256=rsK8s1quedmNYchnUodsc4gUXAENM-N_rVVohhX9HMA,525
 dmm/data_source/data_source_base.py,sha256=5zJ9SZYs1ufbku11GYa4PQRY1lwOOsXbx8kDkiG1VMk,2096
 dmm/data_source/dataframe_source.py,sha256=ptAxMx-fraAYt0ra0uR7nGDMAnBrSMDisz7EjKQb3so,3113
-dmm/data_source/datarobot_source.py,sha256=5HVm2LBZ89n_FhEoN0nAUaPFyNB2mu4ccSbtcj0XFBI,60597
+dmm/data_source/datarobot_source.py,sha256=FkpmoELB-YuW3uElRQIyyUwUBiWo8E2KOdprh8dys6k,62285
 dmm/data_source/generator_source.py,sha256=O-GsreX8mQognj3u6LQxHkZ3yWj3sOgx_dojvObVdlI,3003
 dmm/metric/__init__.py,sha256=nIX41kZJKfQztTw93CXVLRbgTt5W1qhdK1uakWxL9CE,396
 dmm/metric/asymmetric_error.py,sha256=h4J7nzXw9BurtGrkoe4oe7QBI2l69XeB9CiGfpcKUxc,1620
 dmm/metric/median_absolute_error.py,sha256=qV0NpJdF6daTzJOaGpd8KAPG3gZWPS43FZxOb-rR0kE,320
 dmm/metric/metric_base.py,sha256=Rzsju5eZhoyafxo7ErFmO76sDcsvkb3SyODDwcNudy4,3569
 dmm/metric/missing_values.py,sha256=i9ujXCuOWEPrUteFXTCDGX6SM8RVd7cQoI6byQPga4E,957
 dmm/metric/sklearn_metric.py,sha256=Bv4ukOSZyOKjXK-_4b7KKmlkOVrYnwSX5GST_Hf-qpc,2014
-datarobot_model_metrics-0.1.8.dist-info/METADATA,sha256=HRAimo1LoC9Ag9xao3baZPNmC0UjO9zfQ6D6ErsTqOo,886
-datarobot_model_metrics-0.1.8.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-datarobot_model_metrics-0.1.8.dist-info/top_level.txt,sha256=69FbTyYFh17OyfaIppCUhlu4QG-prAaQ6ovJ_X0SNG8,4
-datarobot_model_metrics-0.1.8.dist-info/RECORD,,
+datarobot_model_metrics-0.1.9.dist-info/METADATA,sha256=3bJ79dgGahbJyiCNW4xt5tbQxHLrN6kjyrwD93E_jYI,886
+datarobot_model_metrics-0.1.9.dist-info/WHEEL,sha256=k3vXr0c0OitO0k9eCWBlI2yTYnpb_n_I2SGzrrfY7HY,110
+datarobot_model_metrics-0.1.9.dist-info/top_level.txt,sha256=69FbTyYFh17OyfaIppCUhlu4QG-prAaQ6ovJ_X0SNG8,4
+datarobot_model_metrics-0.1.9.dist-info/RECORD,,
```

