# Comparing `tmp/secretflow_lite-1.0.0b3-cp38-cp38-manylinux2014_x86_64.whl.zip` & `tmp/secretflow_lite-1.1.0b0.dev0-cp38-cp38-macosx_11_0_arm64.whl.zip`

## zipinfo {}

```diff
@@ -1,341 +1,341 @@
-Zip file size: 1375388 bytes, number of entries: 339
--rw-r--r--  2.0 unx     1321 b- defN 23-Jul-10 03:11 secretflow/__init__.py
--rw-r--r--  2.0 unx     6478 b- defN 23-Jul-10 03:11 secretflow/cli.py
--rw-r--r--  2.0 unx      607 b- defN 23-Jul-10 03:11 secretflow/version.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/component/__init__.py
--rw-r--r--  2.0 unx      628 b- defN 23-Jul-10 03:11 secretflow/component/__main__.py
--rw-r--r--  2.0 unx    30621 b- defN 23-Jul-10 03:11 secretflow/component/component.py
--rw-r--r--  2.0 unx    16462 b- defN 23-Jul-10 03:11 secretflow/component/data_utils.py
--rw-r--r--  2.0 unx     3761 b- defN 23-Jul-10 03:11 secretflow/component/entry.py
--rw-r--r--  2.0 unx     9676 b- defN 23-Jul-10 03:11 secretflow/component/eval_param_reader.py
--rw-r--r--  2.0 unx     3377 b- defN 23-Jul-10 03:11 secretflow/component/i18n.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/component/feature/__init__.py
--rw-r--r--  2.0 unx     8843 b- defN 23-Jul-10 03:11 secretflow/component/feature/vert_woe_binning.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/component/ml/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/component/ml/boost/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/component/ml/boost/sgb/__init__.py
--rw-r--r--  2.0 unx    12633 b- defN 23-Jul-10 03:11 secretflow/component/ml/boost/sgb/sgb.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/component/ml/boost/ss_xgb/__init__.py
--rw-r--r--  2.0 unx    11694 b- defN 23-Jul-10 03:11 secretflow/component/ml/boost/ss_xgb/ss_xgb.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/component/ml/eval/__init__.py
--rw-r--r--  2.0 unx    13883 b- defN 23-Jul-10 03:11 secretflow/component/ml/eval/biclassification_eval.py
--rw-r--r--  2.0 unx     7298 b- defN 23-Jul-10 03:11 secretflow/component/ml/eval/prediction_bias_eval.py
--rw-r--r--  2.0 unx     3798 b- defN 23-Jul-10 03:11 secretflow/component/ml/eval/ss_pvalue.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/component/ml/linear/__init__.py
--rw-r--r--  2.0 unx    10720 b- defN 23-Jul-10 03:11 secretflow/component/ml/linear/ss_sgd.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/component/preprocessing/__init__.py
--rw-r--r--  2.0 unx     2748 b- defN 23-Jul-10 03:11 secretflow/component/preprocessing/feature_filter.py
--rw-r--r--  2.0 unx     7266 b- defN 23-Jul-10 03:11 secretflow/component/preprocessing/psi.py
--rw-r--r--  2.0 unx     3729 b- defN 23-Jul-10 03:11 secretflow/component/preprocessing/train_test_split.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/component/stats/__init__.py
--rw-r--r--  2.0 unx     3843 b- defN 23-Jul-10 03:11 secretflow/component/stats/ss_pearsonr.py
--rw-r--r--  2.0 unx     3621 b- defN 23-Jul-10 03:11 secretflow/component/stats/ss_vif.py
--rw-r--r--  2.0 unx     3406 b- defN 23-Jul-10 03:11 secretflow/component/stats/table_statistics.py
--rw-r--r--  2.0 unx      754 b- defN 23-Jul-10 03:11 secretflow/data/__init__.py
--rw-r--r--  2.0 unx    14806 b- defN 23-Jul-10 03:11 secretflow/data/base.py
--rw-r--r--  2.0 unx     1108 b- defN 23-Jul-10 03:11 secretflow/data/math_utils.py
--rw-r--r--  2.0 unx    24878 b- defN 23-Jul-10 03:11 secretflow/data/ndarray.py
--rw-r--r--  2.0 unx     5420 b- defN 23-Jul-10 03:11 secretflow/data/split.py
--rw-r--r--  2.0 unx      694 b- defN 23-Jul-10 03:11 secretflow/data/horizontal/__init__.py
--rw-r--r--  2.0 unx    15042 b- defN 23-Jul-10 03:11 secretflow/data/horizontal/dataframe.py
--rw-r--r--  2.0 unx     2135 b- defN 23-Jul-10 03:11 secretflow/data/horizontal/io.py
--rw-r--r--  2.0 unx     1555 b- defN 23-Jul-10 03:11 secretflow/data/horizontal/sampler.py
--rw-r--r--  2.0 unx      627 b- defN 23-Jul-10 03:11 secretflow/data/io/__init__.py
--rw-r--r--  2.0 unx     2112 b- defN 23-Jul-10 03:11 secretflow/data/io/oss.py
--rw-r--r--  2.0 unx     2501 b- defN 23-Jul-10 03:11 secretflow/data/io/util.py
--rw-r--r--  2.0 unx      691 b- defN 23-Jul-10 03:11 secretflow/data/mix/__init__.py
--rw-r--r--  2.0 unx    15054 b- defN 23-Jul-10 03:11 secretflow/data/mix/dataframe.py
--rw-r--r--  2.0 unx      694 b- defN 23-Jul-10 03:11 secretflow/data/vertical/__init__.py
--rw-r--r--  2.0 unx    22944 b- defN 23-Jul-10 03:11 secretflow/data/vertical/dataframe.py
--rw-r--r--  2.0 unx     6550 b- defN 23-Jul-10 03:11 secretflow/data/vertical/io.py
--rw-r--r--  2.0 unx      745 b- defN 23-Jul-10 03:11 secretflow/device/__init__.py
--rw-r--r--  2.0 unx    22241 b- defN 23-Jul-10 03:11 secretflow/device/driver.py
--rw-r--r--  2.0 unx     2981 b- defN 23-Jul-10 03:11 secretflow/device/global_state.py
--rw-r--r--  2.0 unx    10795 b- defN 23-Jul-10 03:11 secretflow/device/link.py
--rw-r--r--  2.0 unx     8100 b- defN 23-Jul-10 03:11 secretflow/device/proxy.py
--rw-r--r--  2.0 unx      939 b- defN 23-Jul-10 03:11 secretflow/device/device/__init__.py
--rw-r--r--  2.0 unx      889 b- defN 23-Jul-10 03:11 secretflow/device/device/_utils.py
--rw-r--r--  2.0 unx     2264 b- defN 23-Jul-10 03:11 secretflow/device/device/base.py
--rw-r--r--  2.0 unx    25648 b- defN 23-Jul-10 03:11 secretflow/device/device/heu.py
--rw-r--r--  2.0 unx     6881 b- defN 23-Jul-10 03:11 secretflow/device/device/heu_object.py
--rw-r--r--  2.0 unx     4706 b- defN 23-Jul-10 03:11 secretflow/device/device/pyu.py
--rw-r--r--  2.0 unx     3589 b- defN 23-Jul-10 03:11 secretflow/device/device/register.py
--rw-r--r--  2.0 unx    75769 b- defN 23-Jul-10 03:11 secretflow/device/device/spu.py
--rw-r--r--  2.0 unx     7667 b- defN 23-Jul-10 03:11 secretflow/device/device/teeu.py
--rw-r--r--  2.0 unx     2921 b- defN 23-Jul-10 03:11 secretflow/device/device/type_traits.py
--rw-r--r--  2.0 unx      638 b- defN 23-Jul-10 03:11 secretflow/device/kernels/__init__.py
--rw-r--r--  2.0 unx     5716 b- defN 23-Jul-10 03:11 secretflow/device/kernels/heu.py
--rw-r--r--  2.0 unx     6842 b- defN 23-Jul-10 03:11 secretflow/device/kernels/pyu.py
--rw-r--r--  2.0 unx    17461 b- defN 23-Jul-10 03:11 secretflow/device/kernels/spu.py
--rw-r--r--  2.0 unx      923 b- defN 23-Jul-10 03:11 secretflow/device/kernels/teeu.py
--rw-r--r--  2.0 unx      784 b- defN 23-Jul-10 03:11 secretflow/distributed/__init__.py
--rw-r--r--  2.0 unx     6373 b- defN 23-Jul-10 03:11 secretflow/distributed/primitive.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/kuscia/__init__.py
--rw-r--r--  2.0 unx     3329 b- defN 23-Jul-10 03:11 secretflow/kuscia/datamesh.py
--rw-r--r--  2.0 unx     8151 b- defN 23-Jul-10 03:11 secretflow/kuscia/entry.py
--rw-r--r--  2.0 unx     3651 b- defN 23-Jul-10 03:11 secretflow/kuscia/ray_config.py
--rw-r--r--  2.0 unx     5509 b- defN 23-Jul-10 03:11 secretflow/kuscia/sf_config.py
--rw-r--r--  2.0 unx     4555 b- defN 23-Jul-10 03:11 secretflow/kuscia/task_config.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/core/__init__.py
--rw-r--r--  2.0 unx     2925 b- defN 23-Jul-10 03:11 secretflow/ml/boost/core/data_preprocess.py
--rw-r--r--  2.0 unx     3161 b- defN 23-Jul-10 03:11 secretflow/ml/boost/core/order_map_tools.py
--rw-r--r--  2.0 unx      654 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/__init__.py
--rw-r--r--  2.0 unx     9580 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/homo_booster.py
--rw-r--r--  2.0 unx     7462 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/homo_booster_worker.py
--rw-r--r--  2.0 unx    11134 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/homo_decision_tree.py
--rw-r--r--  2.0 unx     3043 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/tree_param.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/boost_core/__init__.py
--rw-r--r--  2.0 unx     4365 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/boost_core/callback.py
--rw-r--r--  2.0 unx     7248 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/boost_core/core.py
--rw-r--r--  2.0 unx     9292 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/boost_core/training.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/tree_core/__init__.py
--rw-r--r--  2.0 unx     4429 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/tree_core/criterion.py
--rw-r--r--  2.0 unx    18576 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/tree_core/decision_tree.py
--rw-r--r--  2.0 unx     9371 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/tree_core/feature_histogram.py
--rw-r--r--  2.0 unx     2535 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/tree_core/feature_importance.py
--rw-r--r--  2.0 unx     4218 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/tree_core/loss_function.py
--rw-r--r--  2.0 unx     2060 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/tree_core/node.py
--rw-r--r--  2.0 unx     9846 b- defN 23-Jul-10 03:11 secretflow/ml/boost/homo_boost/tree_core/splitter.py
--rw-r--r--  2.0 unx      712 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/__init__.py
--rw-r--r--  2.0 unx     7830 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/model.py
--rw-r--r--  2.0 unx    19476 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/sgb.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/cache/__init__.py
--rw-r--r--  2.0 unx     2642 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/cache/level_cache.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/distributed_tree/__init__.py
--rw-r--r--  2.0 unx     3817 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/distributed_tree/distributed_tree.py
--rw-r--r--  2.0 unx     2768 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/distributed_tree/split_tree.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/label_holder/__init__.py
--rw-r--r--  2.0 unx     9653 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/label_holder/label_holder.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/preprocessing/__init__.py
--rw-r--r--  2.0 unx     1238 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/preprocessing/params.py
--rw-r--r--  2.0 unx     2432 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/preprocessing/preprocessing.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/__init__.py
--rw-r--r--  2.0 unx     3706 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/boost.py
--rw-r--r--  2.0 unx     2506 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/bucket_sum.py
--rw-r--r--  2.0 unx     1705 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/grad.py
--rw-r--r--  2.0 unx     2807 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/node_select.py
--rw-r--r--  2.0 unx     1476 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/pred.py
--rw-r--r--  2.0 unx      765 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/random.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/split_tree_trainer/__init__.py
--rw-r--r--  2.0 unx     2851 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/split_tree_trainer/order_map_context.py
--rw-r--r--  2.0 unx     2787 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/split_tree_trainer/shuffler.py
--rw-r--r--  2.0 unx     4141 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/split_tree_trainer/split_tree_trainer.py
--rw-r--r--  2.0 unx     5057 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/core/split_tree_trainer/splitter.py
--rw-r--r--  2.0 unx      644 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/__init__.py
--rw-r--r--  2.0 unx     5223 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/factory.py
--rw-r--r--  2.0 unx     5072 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/params.py
--rw-r--r--  2.0 unx     1450 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/sgb_actor.py
--rw-r--r--  2.0 unx      683 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/booster/__init__.py
--rw-r--r--  2.0 unx     6065 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/booster/global_ordermap_booster.py
--rw-r--r--  2.0 unx      983 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/__init__.py
--rw-r--r--  2.0 unx     2284 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/component.py
--rw-r--r--  2.0 unx     3194 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/logging.py
--rw-r--r--  2.0 unx      780 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/__init__.py
--rw-r--r--  2.0 unx     6180 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/bucket_sum_calculator.py
--rw-r--r--  2.0 unx     6549 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/leaf_wise_bucket_sum_calculator.py
--rw-r--r--  2.0 unx      661 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/cache/__init__.py
--rw-r--r--  2.0 unx     3580 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/cache/level_wise_cache.py
--rw-r--r--  2.0 unx     1934 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/cache/node_bucket_sum_cache_internal.py
--rw-r--r--  2.0 unx     3901 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/cache/node_wise_bucket_sum_cache.py
--rw-r--r--  2.0 unx      666 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/data_preprocessor/__init__.py
--rw-r--r--  2.0 unx     1265 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/data_preprocessor/data_preprocessor.py
--rw-r--r--  2.0 unx      669 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/gradient_encryptor/__init__.py
--rw-r--r--  2.0 unx     6283 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/gradient_encryptor/gradient_encryptor.py
--rw-r--r--  2.0 unx      651 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/leaf_manager/__init__.py
--rw-r--r--  2.0 unx     1567 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/leaf_manager/leaf_actor.py
--rw-r--r--  2.0 unx     3307 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/leaf_manager/leaf_manager.py
--rw-r--r--  2.0 unx      654 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/loss_computer/__init__.py
--rw-r--r--  2.0 unx     8734 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/loss_computer/loss_computer.py
--rw-r--r--  2.0 unx      654 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/model_builder/__init__.py
--rw-r--r--  2.0 unx     2872 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/model_builder/model_builder.py
--rw-r--r--  2.0 unx      654 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/node_selector/__init__.py
--rw-r--r--  2.0 unx     5985 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/node_selector/node_selector.py
--rw-r--r--  2.0 unx      664 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/order_map_manager/__init__.py
--rw-r--r--  2.0 unx     4029 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_actor.py
--rw-r--r--  2.0 unx     5568 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_manager.py
--rw-r--r--  2.0 unx      638 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/sampler/__init__.py
--rw-r--r--  2.0 unx     2600 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/sampler/sample_actor.py
--rw-r--r--  2.0 unx    11117 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/sampler/sampler.py
--rw-r--r--  2.0 unx      641 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/shuffler/__init__.py
--rw-r--r--  2.0 unx     4063 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/shuffler/shuffler.py
--rw-r--r--  2.0 unx     2032 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/shuffler/worker_shuffler.py
--rw-r--r--  2.0 unx      749 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/__init__.py
--rw-r--r--  2.0 unx     3286 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_heap.py
--rw-r--r--  2.0 unx     3225 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_manager.py
--rw-r--r--  2.0 unx      651 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/split_finder/__init__.py
--rw-r--r--  2.0 unx     3568 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/split_finder/leaf_wise_split_finder.py
--rw-r--r--  2.0 unx     4482 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/split_finder/split_finder.py
--rw-r--r--  2.0 unx      667 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/__init__.py
--rw-r--r--  2.0 unx     4546 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/split_tree_actor.py
--rw-r--r--  2.0 unx     6505 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/split_tree_builder.py
--rw-r--r--  2.0 unx      226 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/tree_trainer/__init__.py
--rw-r--r--  2.0 unx    13819 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/tree_trainer/leaf_wise_tree_trainer.py
--rw-r--r--  2.0 unx    12185 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/tree_trainer/level_wise_tree_trainer.py
--rw-r--r--  2.0 unx     1294 b- defN 23-Jul-10 03:11 secretflow/ml/boost/sgb_v/factory/components/tree_trainer/tree_trainer.py
--rw-r--r--  2.0 unx      661 b- defN 23-Jul-10 03:11 secretflow/ml/boost/ss_xgb_v/__init__.py
--rw-r--r--  2.0 unx    19581 b- defN 23-Jul-10 03:11 secretflow/ml/boost/ss_xgb_v/model.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 03:11 secretflow/ml/boost/ss_xgb_v/core/__init__.py
--rw-r--r--  2.0 unx     8801 b- defN 23-Jul-10 03:11 secretflow/ml/boost/ss_xgb_v/core/node_split.py
--rw-r--r--  2.0 unx     7747 b- defN 23-Jul-10 03:11 secretflow/ml/boost/ss_xgb_v/core/tree_worker.py
--rw-r--r--  2.0 unx      980 b- defN 23-Jul-10 03:11 secretflow/ml/boost/ss_xgb_v/core/xgb_tree.py
--rw-r--r--  2.0 unx     1013 b- defN 23-Jul-10 03:11 secretflow/ml/linear/__init__.py
--rw-r--r--  2.0 unx    12354 b- defN 23-Jul-10 03:11 secretflow/ml/linear/fl_lr_mix.py
--rw-r--r--  2.0 unx    19143 b- defN 23-Jul-10 03:11 secretflow/ml/linear/fl_lr_v.py
--rw-r--r--  2.0 unx     1373 b- defN 23-Jul-10 03:11 secretflow/ml/linear/linear_model.py
--rw-r--r--  2.0 unx      673 b- defN 23-Jul-10 03:11 secretflow/ml/linear/hess_sgd/__init__.py
--rw-r--r--  2.0 unx    13354 b- defN 23-Jul-10 03:11 secretflow/ml/linear/hess_sgd/model.py
--rw-r--r--  2.0 unx      639 b- defN 23-Jul-10 03:11 secretflow/ml/linear/ss_glm/__init__.py
--rw-r--r--  2.0 unx    23754 b- defN 23-Jul-10 03:11 secretflow/ml/linear/ss_glm/model.py
--rw-r--r--  2.0 unx      233 b- defN 23-Jul-10 03:11 secretflow/ml/linear/ss_glm/core/__init__.py
--rw-r--r--  2.0 unx     5287 b- defN 23-Jul-10 03:11 secretflow/ml/linear/ss_glm/core/distribution.py
--rw-r--r--  2.0 unx     2950 b- defN 23-Jul-10 03:11 secretflow/ml/linear/ss_glm/core/link.py
--rw-r--r--  2.0 unx      653 b- defN 23-Jul-10 03:11 secretflow/ml/linear/ss_sgd/__init__.py
--rw-r--r--  2.0 unx    21895 b- defN 23-Jul-10 03:11 secretflow/ml/linear/ss_sgd/model.py
--rw-r--r--  2.0 unx      698 b- defN 23-Jul-10 03:11 secretflow/ml/nn/__init__.py
--rw-r--r--  2.0 unx     8067 b- defN 23-Jul-10 03:11 secretflow/ml/nn/metrics.py
--rw-r--r--  2.0 unx     3018 b- defN 23-Jul-10 03:11 secretflow/ml/nn/utils.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/nn/applications/__init__.py
--rw-r--r--  2.0 unx     5261 b- defN 23-Jul-10 03:11 secretflow/ml/nn/applications/sl_deep_fm.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/__init__.py
--rw-r--r--  2.0 unx     1743 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/compress.py
--rw-r--r--  2.0 unx    30826 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/fl_model.py
--rw-r--r--  2.0 unx     2152 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/strategy_dispatcher.py
--rw-r--r--  2.0 unx     3279 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/utils.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/tensorflow/__init__.py
--rw-r--r--  2.0 unx    12678 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/tensorflow/fl_base.py
--rw-r--r--  2.0 unx     4369 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/tensorflow/sampler.py
--rw-r--r--  2.0 unx      904 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/tensorflow/strategy/__init__.py
--rw-r--r--  2.0 unx     4071 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_avg_g.py
--rw-r--r--  2.0 unx     4105 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_avg_u.py
--rw-r--r--  2.0 unx     3834 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_avg_w.py
--rw-r--r--  2.0 unx     4677 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_prox.py
--rw-r--r--  2.0 unx     5579 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_scr.py
--rw-r--r--  2.0 unx     5453 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_stc.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/torch/__init__.py
--rw-r--r--  2.0 unx    13393 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/torch/fl_base.py
--rw-r--r--  2.0 unx     3605 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/torch/sampler.py
--rw-r--r--  2.0 unx      904 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/torch/strategy/__init__.py
--rw-r--r--  2.0 unx     3482 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/torch/strategy/fed_avg_g.py
--rw-r--r--  2.0 unx     3324 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/torch/strategy/fed_avg_u.py
--rw-r--r--  2.0 unx     3381 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/torch/strategy/fed_avg_w.py
--rw-r--r--  2.0 unx     3838 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/torch/strategy/fed_prox.py
--rw-r--r--  2.0 unx     5249 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/torch/strategy/fed_scr.py
--rw-r--r--  2.0 unx     5258 b- defN 23-Jul-10 03:11 secretflow/ml/nn/fl/backend/torch/strategy/fed_stc.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/__init__.py
--rw-r--r--  2.0 unx    30963 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/sl_model.py
--rw-r--r--  2.0 unx     2395 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/strategy_dispatcher.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/agglayer/__init__.py
--rw-r--r--  2.0 unx    16475 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/agglayer/agg_layer.py
--rw-r--r--  2.0 unx     3450 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/agglayer/agg_method.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/tensorflow/__init__.py
--rw-r--r--  2.0 unx    27962 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/tensorflow/sl_base.py
--rw-r--r--  2.0 unx     4487 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/tensorflow/utils.py
--rw-r--r--  2.0 unx      753 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/tensorflow/strategy/__init__.py
--rw-r--r--  2.0 unx     6210 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/tensorflow/strategy/split_async.py
--rw-r--r--  2.0 unx     5140 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/tensorflow/strategy/split_state_async.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/torch/__init__.py
--rw-r--r--  2.0 unx    24562 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/torch/sl_base.py
--rw-r--r--  2.0 unx      652 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/torch/strategy/__init__.py
--rw-r--r--  2.0 unx     3569 b- defN 23-Jul-10 03:11 secretflow/ml/nn/sl/backend/torch/strategy/split_nn.py
--rw-r--r--  2.0 unx      923 b- defN 23-Jul-10 03:11 secretflow/preprocessing/__init__.py
--rw-r--r--  2.0 unx     1231 b- defN 23-Jul-10 03:11 secretflow/preprocessing/base.py
--rw-r--r--  2.0 unx    11845 b- defN 23-Jul-10 03:11 secretflow/preprocessing/discretization.py
--rw-r--r--  2.0 unx    13154 b- defN 23-Jul-10 03:11 secretflow/preprocessing/encoder.py
--rw-r--r--  2.0 unx    14116 b- defN 23-Jul-10 03:11 secretflow/preprocessing/scaler.py
--rw-r--r--  2.0 unx     5162 b- defN 23-Jul-10 03:11 secretflow/preprocessing/transformer.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/__init__.py
--rw-r--r--  2.0 unx     7218 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/homo_binning.py
--rw-r--r--  2.0 unx    11274 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/homo_binning_base.py
--rw-r--r--  2.0 unx    12444 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/vert_woe_binning.py
--rw-r--r--  2.0 unx    24804 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/vert_woe_binning_pyu.py
--rw-r--r--  2.0 unx     3647 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/vert_woe_substitution.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/kernels/__init__.py
--rw-r--r--  2.0 unx     2524 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/kernels/base_binning.py
--rw-r--r--  2.0 unx     5281 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/kernels/chi_merge.py
--rw-r--r--  2.0 unx     5291 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/kernels/quantile_binning.py
--rw-r--r--  2.0 unx     6517 b- defN 23-Jul-10 03:11 secretflow/preprocessing/binning/kernels/quantile_summaries.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/protos/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/protos/component/__init__.py
--rw-r--r--  2.0 unx    22818 b- defN 23-Jul-10 03:11 secretflow/protos/component/cluster_pb2.py
--rw-r--r--  2.0 unx    35269 b- defN 23-Jul-10 03:11 secretflow/protos/component/comp_pb2.py
--rw-r--r--  2.0 unx    20329 b- defN 23-Jul-10 03:11 secretflow/protos/component/data_pb2.py
--rw-r--r--  2.0 unx     7199 b- defN 23-Jul-10 03:11 secretflow/protos/component/evaluation_pb2.py
--rw-r--r--  2.0 unx    24482 b- defN 23-Jul-10 03:11 secretflow/protos/component/report_pb2.py
--rw-r--r--  2.0 unx      941 b- defN 23-Jul-10 03:11 secretflow/security/__init__.py
--rw-r--r--  2.0 unx    11579 b- defN 23-Jul-10 03:11 secretflow/security/diffie_hellman.py
--rw-r--r--  2.0 unx      946 b- defN 23-Jul-10 03:11 secretflow/security/aggregation/__init__.py
--rw-r--r--  2.0 unx     1039 b- defN 23-Jul-10 03:11 secretflow/security/aggregation/_utils.py
--rw-r--r--  2.0 unx     1072 b- defN 23-Jul-10 03:11 secretflow/security/aggregation/aggregator.py
--rw-r--r--  2.0 unx     4474 b- defN 23-Jul-10 03:11 secretflow/security/aggregation/plain_aggregator.py
--rw-r--r--  2.0 unx    12148 b- defN 23-Jul-10 03:11 secretflow/security/aggregation/secure_aggregator.py
--rw-r--r--  2.0 unx     5009 b- defN 23-Jul-10 03:11 secretflow/security/aggregation/sparse_plain_aggregator.py
--rw-r--r--  2.0 unx     4179 b- defN 23-Jul-10 03:11 secretflow/security/aggregation/spu_aggregator.py
--rw-r--r--  2.0 unx      858 b- defN 23-Jul-10 03:11 secretflow/security/compare/__init__.py
--rw-r--r--  2.0 unx     1066 b- defN 23-Jul-10 03:11 secretflow/security/compare/comparator.py
--rw-r--r--  2.0 unx     2220 b- defN 23-Jul-10 03:11 secretflow/security/compare/device_comparator.py
--rw-r--r--  2.0 unx     1871 b- defN 23-Jul-10 03:11 secretflow/security/compare/plain_comparator.py
--rw-r--r--  2.0 unx     1762 b- defN 23-Jul-10 03:11 secretflow/security/compare/spu_comparator.py
--rw-r--r--  2.0 unx      948 b- defN 23-Jul-10 03:11 secretflow/security/privacy/__init__.py
--rw-r--r--  2.0 unx  2222040 b- defN 23-Jul-10 03:16 secretflow/security/privacy/_lib.cpython-38-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx     3012 b- defN 23-Jul-10 03:11 secretflow/security/privacy/strategy.py
--rw-r--r--  2.0 unx     1855 b- defN 23-Jul-10 03:11 secretflow/security/privacy/strategy_fl.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/security/privacy/accounting/__init__.py
--rw-r--r--  2.0 unx     1814 b- defN 23-Jul-10 03:11 secretflow/security/privacy/accounting/budget_accountant.py
--rw-r--r--  2.0 unx     2508 b- defN 23-Jul-10 03:11 secretflow/security/privacy/accounting/gdp_accountant.py
--rw-r--r--  2.0 unx     3670 b- defN 23-Jul-10 03:11 secretflow/security/privacy/accounting/log_utils.py
--rw-r--r--  2.0 unx     5702 b- defN 23-Jul-10 03:11 secretflow/security/privacy/accounting/rdp_accountant.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/security/privacy/mechanism/__init__.py
--rw-r--r--  2.0 unx     2567 b- defN 23-Jul-10 03:11 secretflow/security/privacy/mechanism/label_dp.py
--rw-r--r--  2.0 unx     5420 b- defN 23-Jul-10 03:11 secretflow/security/privacy/mechanism/mechanism_fl.py
--rw-r--r--  2.0 unx      668 b- defN 23-Jul-10 03:11 secretflow/security/privacy/mechanism/tensorflow/__init__.py
--rw-r--r--  2.0 unx     2750 b- defN 23-Jul-10 03:11 secretflow/security/privacy/mechanism/tensorflow/layers.py
--rw-r--r--  2.0 unx      668 b- defN 23-Jul-10 03:11 secretflow/security/privacy/mechanism/torch/__init__.py
--rw-r--r--  2.0 unx     2115 b- defN 23-Jul-10 03:11 secretflow/security/privacy/mechanism/torch/layers.py
--rw-r--r--  2.0 unx     1195 b- defN 23-Jul-10 03:11 secretflow/stats/__init__.py
--rw-r--r--  2.0 unx     4320 b- defN 23-Jul-10 03:11 secretflow/stats/biclassification_eval.py
--rw-r--r--  2.0 unx     3404 b- defN 23-Jul-10 03:11 secretflow/stats/prediction_bias_eval.py
--rw-r--r--  2.0 unx     2386 b- defN 23-Jul-10 03:11 secretflow/stats/psi_eval.py
--rw-r--r--  2.0 unx     2995 b- defN 23-Jul-10 03:11 secretflow/stats/regression_eval.py
--rw-r--r--  2.0 unx     4421 b- defN 23-Jul-10 03:11 secretflow/stats/score_card.py
--rw-r--r--  2.0 unx     2421 b- defN 23-Jul-10 03:11 secretflow/stats/ss_pearsonr_v.py
--rw-r--r--  2.0 unx     7265 b- defN 23-Jul-10 03:11 secretflow/stats/ss_pvalue_v.py
--rw-r--r--  2.0 unx     3631 b- defN 23-Jul-10 03:11 secretflow/stats/ss_vif_v.py
--rw-r--r--  2.0 unx     3450 b- defN 23-Jul-10 03:11 secretflow/stats/table_statistics.py
--rw-r--r--  2.0 unx      750 b- defN 23-Jul-10 03:11 secretflow/stats/core/__init__.py
--rw-r--r--  2.0 unx    21056 b- defN 23-Jul-10 03:11 secretflow/stats/core/biclassification_eval_core.py
--rw-r--r--  2.0 unx     7200 b- defN 23-Jul-10 03:11 secretflow/stats/core/prediction_bias_core.py
--rw-r--r--  2.0 unx     2802 b- defN 23-Jul-10 03:11 secretflow/stats/core/psi_core.py
--rw-r--r--  2.0 unx     1776 b- defN 23-Jul-10 03:11 secretflow/stats/core/utils.py
--rw-r--r--  2.0 unx      662 b- defN 23-Jul-10 03:11 secretflow/utils/__init__.py
--rw-r--r--  2.0 unx    13370 b- defN 23-Jul-10 03:11 secretflow/utils/cloudpickle.py
--rw-r--r--  2.0 unx      400 b- defN 23-Jul-10 03:11 secretflow/utils/communicate.py
--rw-r--r--  2.0 unx    11981 b- defN 23-Jul-10 03:11 secretflow/utils/compressor.py
--rw-r--r--  2.0 unx     1084 b- defN 23-Jul-10 03:11 secretflow/utils/errors.py
--rw-r--r--  2.0 unx      870 b- defN 23-Jul-10 03:11 secretflow/utils/hash.py
--rw-r--r--  2.0 unx      978 b- defN 23-Jul-10 03:11 secretflow/utils/io.py
--rw-r--r--  2.0 unx      964 b- defN 23-Jul-10 03:11 secretflow/utils/logging.py
--rw-r--r--  2.0 unx     2842 b- defN 23-Jul-10 03:11 secretflow/utils/ndarray_bigint.py
--rw-r--r--  2.0 unx     2489 b- defN 23-Jul-10 03:11 secretflow/utils/ndarray_encoding.py
--rw-r--r--  2.0 unx      803 b- defN 23-Jul-10 03:11 secretflow/utils/random.py
--rw-r--r--  2.0 unx     1330 b- defN 23-Jul-10 03:11 secretflow/utils/ray_compatibility.py
--rw-r--r--  2.0 unx     3930 b- defN 23-Jul-10 03:11 secretflow/utils/sigmoid.py
--rw-r--r--  2.0 unx     3308 b- defN 23-Jul-10 03:11 secretflow/utils/testing.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jul-10 03:11 secretflow/utils/simulation/__init__.py
--rw-r--r--  2.0 unx    30743 b- defN 23-Jul-10 03:11 secretflow/utils/simulation/datasets.py
--rw-r--r--  2.0 unx     8941 b- defN 23-Jul-10 03:11 secretflow/utils/simulation/tf_gnn_model.py
--rw-r--r--  2.0 unx      769 b- defN 23-Jul-10 03:11 secretflow/utils/simulation/data/__init__.py
--rw-r--r--  2.0 unx     2548 b- defN 23-Jul-10 03:11 secretflow/utils/simulation/data/_utils.py
--rw-r--r--  2.0 unx     5389 b- defN 23-Jul-10 03:11 secretflow/utils/simulation/data/dataframe.py
--rw-r--r--  2.0 unx     4093 b- defN 23-Jul-10 03:11 secretflow/utils/simulation/data/ndarray.py
--rw-r--r--  2.0 unx    11356 b- defN 23-Jul-10 03:16 secretflow_lite-1.0.0b3.dist-info/LICENSE
--rw-r--r--  2.0 unx     3799 b- defN 23-Jul-10 03:16 secretflow_lite-1.0.0b3.dist-info/METADATA
--rw-r--r--  2.0 unx      111 b- defN 23-Jul-10 03:16 secretflow_lite-1.0.0b3.dist-info/WHEEL
--rw-r--r--  2.0 unx       50 b- defN 23-Jul-10 03:16 secretflow_lite-1.0.0b3.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       44 b- defN 23-Jul-10 03:16 secretflow_lite-1.0.0b3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    35076 b- defN 23-Jul-10 03:16 secretflow_lite-1.0.0b3.dist-info/RECORD
-339 files, 4073700 bytes uncompressed, 1317882 bytes compressed:  67.7%
+Zip file size: 1647517 bytes, number of entries: 339
+-rw-r--r--  2.0 unx     1321 b- defN 23-Aug-07 06:04 secretflow/__init__.py
+-rw-r--r--  2.0 unx     6478 b- defN 23-Aug-07 06:04 secretflow/cli.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Aug-07 06:04 secretflow/version.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/component/__init__.py
+-rw-r--r--  2.0 unx      628 b- defN 23-Aug-07 06:04 secretflow/component/__main__.py
+-rw-r--r--  2.0 unx    30749 b- defN 23-Aug-07 06:04 secretflow/component/component.py
+-rw-r--r--  2.0 unx    16550 b- defN 23-Aug-07 06:04 secretflow/component/data_utils.py
+-rw-r--r--  2.0 unx     3761 b- defN 23-Aug-07 06:04 secretflow/component/entry.py
+-rw-r--r--  2.0 unx     9711 b- defN 23-Aug-07 06:04 secretflow/component/eval_param_reader.py
+-rw-r--r--  2.0 unx     3377 b- defN 23-Aug-07 06:04 secretflow/component/i18n.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/component/feature/__init__.py
+-rw-r--r--  2.0 unx     8835 b- defN 23-Aug-07 06:04 secretflow/component/feature/vert_woe_binning.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/component/ml/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/component/ml/boost/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/component/ml/boost/sgb/__init__.py
+-rw-r--r--  2.0 unx    16460 b- defN 23-Aug-07 06:04 secretflow/component/ml/boost/sgb/sgb.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/component/ml/boost/ss_xgb/__init__.py
+-rw-r--r--  2.0 unx    11712 b- defN 23-Aug-07 06:04 secretflow/component/ml/boost/ss_xgb/ss_xgb.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/component/ml/eval/__init__.py
+-rw-r--r--  2.0 unx    13637 b- defN 23-Aug-07 06:04 secretflow/component/ml/eval/biclassification_eval.py
+-rw-r--r--  2.0 unx     7211 b- defN 23-Aug-07 06:04 secretflow/component/ml/eval/prediction_bias_eval.py
+-rw-r--r--  2.0 unx     3783 b- defN 23-Aug-07 06:04 secretflow/component/ml/eval/ss_pvalue.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/component/ml/linear/__init__.py
+-rw-r--r--  2.0 unx    10724 b- defN 23-Aug-07 06:04 secretflow/component/ml/linear/ss_sgd.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/component/preprocessing/__init__.py
+-rw-r--r--  2.0 unx     2739 b- defN 23-Aug-07 06:04 secretflow/component/preprocessing/feature_filter.py
+-rw-r--r--  2.0 unx     7294 b- defN 23-Aug-07 06:04 secretflow/component/preprocessing/psi.py
+-rw-r--r--  2.0 unx     3739 b- defN 23-Aug-07 06:04 secretflow/component/preprocessing/train_test_split.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/component/stats/__init__.py
+-rw-r--r--  2.0 unx     3865 b- defN 23-Aug-07 06:04 secretflow/component/stats/ss_pearsonr.py
+-rw-r--r--  2.0 unx     3637 b- defN 23-Aug-07 06:04 secretflow/component/stats/ss_vif.py
+-rw-r--r--  2.0 unx     3556 b- defN 23-Aug-07 06:04 secretflow/component/stats/table_statistics.py
+-rw-r--r--  2.0 unx      754 b- defN 23-Aug-07 06:04 secretflow/data/__init__.py
+-rw-r--r--  2.0 unx    14806 b- defN 23-Aug-07 06:04 secretflow/data/base.py
+-rw-r--r--  2.0 unx     1108 b- defN 23-Aug-07 06:04 secretflow/data/math_utils.py
+-rw-r--r--  2.0 unx    24878 b- defN 23-Aug-07 06:04 secretflow/data/ndarray.py
+-rw-r--r--  2.0 unx     5420 b- defN 23-Aug-07 06:04 secretflow/data/split.py
+-rw-r--r--  2.0 unx      694 b- defN 23-Aug-07 06:04 secretflow/data/horizontal/__init__.py
+-rw-r--r--  2.0 unx    15042 b- defN 23-Aug-07 06:04 secretflow/data/horizontal/dataframe.py
+-rw-r--r--  2.0 unx     2135 b- defN 23-Aug-07 06:04 secretflow/data/horizontal/io.py
+-rw-r--r--  2.0 unx     1555 b- defN 23-Aug-07 06:04 secretflow/data/horizontal/sampler.py
+-rw-r--r--  2.0 unx      627 b- defN 23-Aug-07 06:04 secretflow/data/io/__init__.py
+-rw-r--r--  2.0 unx     2112 b- defN 23-Aug-07 06:04 secretflow/data/io/oss.py
+-rw-r--r--  2.0 unx     2501 b- defN 23-Aug-07 06:04 secretflow/data/io/util.py
+-rw-r--r--  2.0 unx      691 b- defN 23-Aug-07 06:04 secretflow/data/mix/__init__.py
+-rw-r--r--  2.0 unx    15054 b- defN 23-Aug-07 06:04 secretflow/data/mix/dataframe.py
+-rw-r--r--  2.0 unx      694 b- defN 23-Aug-07 06:04 secretflow/data/vertical/__init__.py
+-rw-r--r--  2.0 unx    22944 b- defN 23-Aug-07 06:04 secretflow/data/vertical/dataframe.py
+-rw-r--r--  2.0 unx     6550 b- defN 23-Aug-07 06:04 secretflow/data/vertical/io.py
+-rw-r--r--  2.0 unx      745 b- defN 23-Aug-07 06:04 secretflow/device/__init__.py
+-rw-r--r--  2.0 unx    22478 b- defN 23-Aug-07 06:04 secretflow/device/driver.py
+-rw-r--r--  2.0 unx     2981 b- defN 23-Aug-07 06:04 secretflow/device/global_state.py
+-rw-r--r--  2.0 unx    10795 b- defN 23-Aug-07 06:04 secretflow/device/link.py
+-rw-r--r--  2.0 unx     8100 b- defN 23-Aug-07 06:04 secretflow/device/proxy.py
+-rw-r--r--  2.0 unx      939 b- defN 23-Aug-07 06:04 secretflow/device/device/__init__.py
+-rw-r--r--  2.0 unx      889 b- defN 23-Aug-07 06:04 secretflow/device/device/_utils.py
+-rw-r--r--  2.0 unx     2264 b- defN 23-Aug-07 06:04 secretflow/device/device/base.py
+-rw-r--r--  2.0 unx    25805 b- defN 23-Aug-07 06:04 secretflow/device/device/heu.py
+-rw-r--r--  2.0 unx     6881 b- defN 23-Aug-07 06:04 secretflow/device/device/heu_object.py
+-rw-r--r--  2.0 unx     4706 b- defN 23-Aug-07 06:04 secretflow/device/device/pyu.py
+-rw-r--r--  2.0 unx     3589 b- defN 23-Aug-07 06:04 secretflow/device/device/register.py
+-rw-r--r--  2.0 unx    85696 b- defN 23-Aug-07 06:04 secretflow/device/device/spu.py
+-rw-r--r--  2.0 unx     7667 b- defN 23-Aug-07 06:04 secretflow/device/device/teeu.py
+-rw-r--r--  2.0 unx     2921 b- defN 23-Aug-07 06:04 secretflow/device/device/type_traits.py
+-rw-r--r--  2.0 unx      638 b- defN 23-Aug-07 06:04 secretflow/device/kernels/__init__.py
+-rw-r--r--  2.0 unx     5758 b- defN 23-Aug-07 06:04 secretflow/device/kernels/heu.py
+-rw-r--r--  2.0 unx     7323 b- defN 23-Aug-07 06:04 secretflow/device/kernels/pyu.py
+-rw-r--r--  2.0 unx    18937 b- defN 23-Aug-07 06:04 secretflow/device/kernels/spu.py
+-rw-r--r--  2.0 unx      923 b- defN 23-Aug-07 06:04 secretflow/device/kernels/teeu.py
+-rw-r--r--  2.0 unx      784 b- defN 23-Aug-07 06:04 secretflow/distributed/__init__.py
+-rw-r--r--  2.0 unx     6373 b- defN 23-Aug-07 06:04 secretflow/distributed/primitive.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/kuscia/__init__.py
+-rw-r--r--  2.0 unx     3329 b- defN 23-Aug-07 06:04 secretflow/kuscia/datamesh.py
+-rw-r--r--  2.0 unx     8151 b- defN 23-Aug-07 06:04 secretflow/kuscia/entry.py
+-rw-r--r--  2.0 unx     3651 b- defN 23-Aug-07 06:04 secretflow/kuscia/ray_config.py
+-rw-r--r--  2.0 unx     5509 b- defN 23-Aug-07 06:04 secretflow/kuscia/sf_config.py
+-rw-r--r--  2.0 unx     4555 b- defN 23-Aug-07 06:04 secretflow/kuscia/task_config.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/boost/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/boost/core/__init__.py
+-rw-r--r--  2.0 unx     2925 b- defN 23-Aug-07 06:04 secretflow/ml/boost/core/data_preprocess.py
+-rw-r--r--  2.0 unx     3161 b- defN 23-Aug-07 06:04 secretflow/ml/boost/core/order_map_tools.py
+-rw-r--r--  2.0 unx      654 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/__init__.py
+-rw-r--r--  2.0 unx     9580 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/homo_booster.py
+-rw-r--r--  2.0 unx     7462 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/homo_booster_worker.py
+-rw-r--r--  2.0 unx    11134 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/homo_decision_tree.py
+-rw-r--r--  2.0 unx     3043 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/tree_param.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/boost_core/__init__.py
+-rw-r--r--  2.0 unx     4365 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/boost_core/callback.py
+-rw-r--r--  2.0 unx     7248 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/boost_core/core.py
+-rw-r--r--  2.0 unx     9292 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/boost_core/training.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/tree_core/__init__.py
+-rw-r--r--  2.0 unx     4429 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/tree_core/criterion.py
+-rw-r--r--  2.0 unx    18576 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/tree_core/decision_tree.py
+-rw-r--r--  2.0 unx     9371 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/tree_core/feature_histogram.py
+-rw-r--r--  2.0 unx     2535 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/tree_core/feature_importance.py
+-rw-r--r--  2.0 unx     4218 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/tree_core/loss_function.py
+-rw-r--r--  2.0 unx     2060 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/tree_core/node.py
+-rw-r--r--  2.0 unx     9846 b- defN 23-Aug-07 06:04 secretflow/ml/boost/homo_boost/tree_core/splitter.py
+-rw-r--r--  2.0 unx      865 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/__init__.py
+-rw-r--r--  2.0 unx     7816 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/model.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/__init__.py
+-rw-r--r--  2.0 unx    10152 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/params.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/cache/__init__.py
+-rw-r--r--  2.0 unx     2642 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/cache/level_cache.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/distributed_tree/__init__.py
+-rw-r--r--  2.0 unx     3817 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/distributed_tree/distributed_tree.py
+-rw-r--r--  2.0 unx     2768 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/distributed_tree/split_tree.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/__init__.py
+-rw-r--r--  2.0 unx     3706 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/boost.py
+-rw-r--r--  2.0 unx     2506 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/bucket_sum.py
+-rw-r--r--  2.0 unx     1705 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/grad.py
+-rw-r--r--  2.0 unx     2807 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/node_select.py
+-rw-r--r--  2.0 unx     1476 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/pred.py
+-rw-r--r--  2.0 unx      765 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/random.py
+-rw-r--r--  2.0 unx      644 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/__init__.py
+-rw-r--r--  2.0 unx     5543 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/factory.py
+-rw-r--r--  2.0 unx     1450 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/sgb_actor.py
+-rw-r--r--  2.0 unx      683 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/booster/__init__.py
+-rw-r--r--  2.0 unx     6635 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/booster/global_ordermap_booster.py
+-rw-r--r--  2.0 unx      983 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/__init__.py
+-rw-r--r--  2.0 unx     2495 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/component.py
+-rw-r--r--  2.0 unx     3295 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/logging.py
+-rw-r--r--  2.0 unx      780 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/__init__.py
+-rw-r--r--  2.0 unx     6214 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/bucket_sum_calculator.py
+-rw-r--r--  2.0 unx     6509 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/leaf_wise_bucket_sum_calculator.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/cache/__init__.py
+-rw-r--r--  2.0 unx     3644 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/cache/level_wise_cache.py
+-rw-r--r--  2.0 unx     1934 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/cache/node_bucket_sum_cache_internal.py
+-rw-r--r--  2.0 unx     3959 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/cache/node_wise_bucket_sum_cache.py
+-rw-r--r--  2.0 unx      666 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/data_preprocessor/__init__.py
+-rw-r--r--  2.0 unx     1307 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/data_preprocessor/data_preprocessor.py
+-rw-r--r--  2.0 unx      669 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/gradient_encryptor/__init__.py
+-rw-r--r--  2.0 unx     6108 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/gradient_encryptor/gradient_encryptor.py
+-rw-r--r--  2.0 unx      651 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/leaf_manager/__init__.py
+-rw-r--r--  2.0 unx     1567 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/leaf_manager/leaf_actor.py
+-rw-r--r--  2.0 unx     3184 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/leaf_manager/leaf_manager.py
+-rw-r--r--  2.0 unx      654 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/loss_computer/__init__.py
+-rw-r--r--  2.0 unx     6645 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/loss_computer/loss_computer.py
+-rw-r--r--  2.0 unx     3748 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/loss_computer/loss_computer_actor.py
+-rw-r--r--  2.0 unx      654 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/model_builder/__init__.py
+-rw-r--r--  2.0 unx     2724 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/model_builder/model_builder.py
+-rw-r--r--  2.0 unx      654 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/node_selector/__init__.py
+-rw-r--r--  2.0 unx     5952 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/node_selector/node_selector.py
+-rw-r--r--  2.0 unx      664 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/order_map_manager/__init__.py
+-rw-r--r--  2.0 unx     4002 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_actor.py
+-rw-r--r--  2.0 unx     2852 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_context.py
+-rw-r--r--  2.0 unx     5539 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_manager.py
+-rw-r--r--  2.0 unx      638 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/sampler/__init__.py
+-rw-r--r--  2.0 unx     2606 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/sampler/sample_actor.py
+-rw-r--r--  2.0 unx    10776 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/sampler/sampler.py
+-rw-r--r--  2.0 unx      641 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/shuffler/__init__.py
+-rw-r--r--  2.0 unx     4137 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/shuffler/shuffler.py
+-rw-r--r--  2.0 unx     2825 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/shuffler/shuffler_core.py
+-rw-r--r--  2.0 unx     2010 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/shuffler/worker_shuffler.py
+-rw-r--r--  2.0 unx      749 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/__init__.py
+-rw-r--r--  2.0 unx     3401 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_heap.py
+-rw-r--r--  2.0 unx     3409 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_manager.py
+-rw-r--r--  2.0 unx      651 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/split_finder/__init__.py
+-rw-r--r--  2.0 unx     3217 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/split_finder/leaf_wise_split_finder.py
+-rw-r--r--  2.0 unx     4173 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/split_finder/split_finder.py
+-rw-r--r--  2.0 unx      667 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/__init__.py
+-rw-r--r--  2.0 unx     4546 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/split_tree_actor.py
+-rw-r--r--  2.0 unx     6581 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/split_tree_builder.py
+-rw-r--r--  2.0 unx      226 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/tree_trainer/__init__.py
+-rw-r--r--  2.0 unx    14378 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/tree_trainer/leaf_wise_tree_trainer.py
+-rw-r--r--  2.0 unx    12632 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/tree_trainer/level_wise_tree_trainer.py
+-rw-r--r--  2.0 unx     1324 b- defN 23-Aug-07 06:04 secretflow/ml/boost/sgb_v/factory/components/tree_trainer/tree_trainer.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Aug-07 06:04 secretflow/ml/boost/ss_xgb_v/__init__.py
+-rw-r--r--  2.0 unx    19581 b- defN 23-Aug-07 06:04 secretflow/ml/boost/ss_xgb_v/model.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Aug-07 06:04 secretflow/ml/boost/ss_xgb_v/core/__init__.py
+-rw-r--r--  2.0 unx     8801 b- defN 23-Aug-07 06:04 secretflow/ml/boost/ss_xgb_v/core/node_split.py
+-rw-r--r--  2.0 unx     7747 b- defN 23-Aug-07 06:04 secretflow/ml/boost/ss_xgb_v/core/tree_worker.py
+-rw-r--r--  2.0 unx      980 b- defN 23-Aug-07 06:04 secretflow/ml/boost/ss_xgb_v/core/xgb_tree.py
+-rw-r--r--  2.0 unx     1013 b- defN 23-Aug-07 06:04 secretflow/ml/linear/__init__.py
+-rw-r--r--  2.0 unx    12354 b- defN 23-Aug-07 06:04 secretflow/ml/linear/fl_lr_mix.py
+-rw-r--r--  2.0 unx    19143 b- defN 23-Aug-07 06:04 secretflow/ml/linear/fl_lr_v.py
+-rw-r--r--  2.0 unx     1373 b- defN 23-Aug-07 06:04 secretflow/ml/linear/linear_model.py
+-rw-r--r--  2.0 unx      673 b- defN 23-Aug-07 06:04 secretflow/ml/linear/hess_sgd/__init__.py
+-rw-r--r--  2.0 unx    13354 b- defN 23-Aug-07 06:04 secretflow/ml/linear/hess_sgd/model.py
+-rw-r--r--  2.0 unx      639 b- defN 23-Aug-07 06:04 secretflow/ml/linear/ss_glm/__init__.py
+-rw-r--r--  2.0 unx    23754 b- defN 23-Aug-07 06:04 secretflow/ml/linear/ss_glm/model.py
+-rw-r--r--  2.0 unx      233 b- defN 23-Aug-07 06:04 secretflow/ml/linear/ss_glm/core/__init__.py
+-rw-r--r--  2.0 unx     5287 b- defN 23-Aug-07 06:04 secretflow/ml/linear/ss_glm/core/distribution.py
+-rw-r--r--  2.0 unx     2950 b- defN 23-Aug-07 06:04 secretflow/ml/linear/ss_glm/core/link.py
+-rw-r--r--  2.0 unx      653 b- defN 23-Aug-07 06:04 secretflow/ml/linear/ss_sgd/__init__.py
+-rw-r--r--  2.0 unx    21895 b- defN 23-Aug-07 06:04 secretflow/ml/linear/ss_sgd/model.py
+-rw-r--r--  2.0 unx      698 b- defN 23-Aug-07 06:04 secretflow/ml/nn/__init__.py
+-rw-r--r--  2.0 unx     8067 b- defN 23-Aug-07 06:04 secretflow/ml/nn/metrics.py
+-rw-r--r--  2.0 unx     3018 b- defN 23-Aug-07 06:04 secretflow/ml/nn/utils.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/applications/__init__.py
+-rw-r--r--  2.0 unx     5261 b- defN 23-Aug-07 06:04 secretflow/ml/nn/applications/sl_deep_fm.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/__init__.py
+-rw-r--r--  2.0 unx     1743 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/compress.py
+-rw-r--r--  2.0 unx    31183 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/fl_model.py
+-rw-r--r--  2.0 unx     2152 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/strategy_dispatcher.py
+-rw-r--r--  2.0 unx     3279 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/utils.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/tensorflow/__init__.py
+-rw-r--r--  2.0 unx    12678 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/tensorflow/fl_base.py
+-rw-r--r--  2.0 unx     4369 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/tensorflow/sampler.py
+-rw-r--r--  2.0 unx      904 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/tensorflow/strategy/__init__.py
+-rw-r--r--  2.0 unx     4071 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_avg_g.py
+-rw-r--r--  2.0 unx     4105 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_avg_u.py
+-rw-r--r--  2.0 unx     3834 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_avg_w.py
+-rw-r--r--  2.0 unx     4677 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_prox.py
+-rw-r--r--  2.0 unx     5579 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_scr.py
+-rw-r--r--  2.0 unx     5453 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/tensorflow/strategy/fed_stc.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/torch/__init__.py
+-rw-r--r--  2.0 unx    13393 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/torch/fl_base.py
+-rw-r--r--  2.0 unx     3605 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/torch/sampler.py
+-rw-r--r--  2.0 unx      904 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/torch/strategy/__init__.py
+-rw-r--r--  2.0 unx     3482 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/torch/strategy/fed_avg_g.py
+-rw-r--r--  2.0 unx     3324 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/torch/strategy/fed_avg_u.py
+-rw-r--r--  2.0 unx     3381 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/torch/strategy/fed_avg_w.py
+-rw-r--r--  2.0 unx     3838 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/torch/strategy/fed_prox.py
+-rw-r--r--  2.0 unx     5249 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/torch/strategy/fed_scr.py
+-rw-r--r--  2.0 unx     5258 b- defN 23-Aug-07 06:04 secretflow/ml/nn/fl/backend/torch/strategy/fed_stc.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/__init__.py
+-rw-r--r--  2.0 unx    34368 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/sl_model.py
+-rw-r--r--  2.0 unx     2395 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/strategy_dispatcher.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/agglayer/__init__.py
+-rw-r--r--  2.0 unx    19647 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/agglayer/agg_layer.py
+-rw-r--r--  2.0 unx     3450 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/agglayer/agg_method.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/attack/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/attack/torch/__init__.py
+-rw-r--r--  2.0 unx    14581 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/attack/torch/feature_inference_attack.py
+-rw-r--r--  2.0 unx    21096 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/attack/torch/label_inferece_attack.py
+-rw-r--r--  2.0 unx     2688 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/attack/torch/metrics.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/tensorflow/__init__.py
+-rw-r--r--  2.0 unx    30692 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/tensorflow/sl_base.py
+-rw-r--r--  2.0 unx     4487 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/tensorflow/utils.py
+-rw-r--r--  2.0 unx      820 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/tensorflow/strategy/__init__.py
+-rw-r--r--  2.0 unx     6768 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/tensorflow/strategy/pipeline.py
+-rw-r--r--  2.0 unx     6398 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/tensorflow/strategy/split_async.py
+-rw-r--r--  2.0 unx     5140 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/tensorflow/strategy/split_state_async.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/torch/__init__.py
+-rw-r--r--  2.0 unx     1131 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/torch/callback.py
+-rw-r--r--  2.0 unx    26221 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/torch/sl_base.py
+-rw-r--r--  2.0 unx      652 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/torch/strategy/__init__.py
+-rw-r--r--  2.0 unx     3749 b- defN 23-Aug-07 06:04 secretflow/ml/nn/sl/backend/torch/strategy/split_nn.py
+-rw-r--r--  2.0 unx      923 b- defN 23-Aug-07 06:04 secretflow/preprocessing/__init__.py
+-rw-r--r--  2.0 unx     1231 b- defN 23-Aug-07 06:04 secretflow/preprocessing/base.py
+-rw-r--r--  2.0 unx    11845 b- defN 23-Aug-07 06:04 secretflow/preprocessing/discretization.py
+-rw-r--r--  2.0 unx    13154 b- defN 23-Aug-07 06:04 secretflow/preprocessing/encoder.py
+-rw-r--r--  2.0 unx    14116 b- defN 23-Aug-07 06:04 secretflow/preprocessing/scaler.py
+-rw-r--r--  2.0 unx     5162 b- defN 23-Aug-07 06:04 secretflow/preprocessing/transformer.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/__init__.py
+-rw-r--r--  2.0 unx     7218 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/homo_binning.py
+-rw-r--r--  2.0 unx    11274 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/homo_binning_base.py
+-rw-r--r--  2.0 unx    12444 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/vert_woe_binning.py
+-rw-r--r--  2.0 unx    24804 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/vert_woe_binning_pyu.py
+-rw-r--r--  2.0 unx     3647 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/vert_woe_substitution.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/kernels/__init__.py
+-rw-r--r--  2.0 unx     2524 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/kernels/base_binning.py
+-rw-r--r--  2.0 unx     5281 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/kernels/chi_merge.py
+-rw-r--r--  2.0 unx     5291 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/kernels/quantile_binning.py
+-rw-r--r--  2.0 unx     6517 b- defN 23-Aug-07 06:04 secretflow/preprocessing/binning/kernels/quantile_summaries.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/protos/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/protos/component/__init__.py
+-rw-r--r--  2.0 unx    22918 b- defN 23-Aug-07 06:04 secretflow/protos/component/cluster_pb2.py
+-rw-r--r--  2.0 unx    35369 b- defN 23-Aug-07 06:04 secretflow/protos/component/comp_pb2.py
+-rw-r--r--  2.0 unx    20429 b- defN 23-Aug-07 06:04 secretflow/protos/component/data_pb2.py
+-rw-r--r--  2.0 unx     7299 b- defN 23-Aug-07 06:04 secretflow/protos/component/evaluation_pb2.py
+-rw-r--r--  2.0 unx    24311 b- defN 23-Aug-07 06:04 secretflow/protos/component/report_pb2.py
+-rw-r--r--  2.0 unx      941 b- defN 23-Aug-07 06:04 secretflow/security/__init__.py
+-rw-r--r--  2.0 unx    11579 b- defN 23-Aug-07 06:04 secretflow/security/diffie_hellman.py
+-rw-r--r--  2.0 unx      946 b- defN 23-Aug-07 06:04 secretflow/security/aggregation/__init__.py
+-rw-r--r--  2.0 unx     1039 b- defN 23-Aug-07 06:04 secretflow/security/aggregation/_utils.py
+-rw-r--r--  2.0 unx     1072 b- defN 23-Aug-07 06:04 secretflow/security/aggregation/aggregator.py
+-rw-r--r--  2.0 unx     4474 b- defN 23-Aug-07 06:04 secretflow/security/aggregation/plain_aggregator.py
+-rw-r--r--  2.0 unx    12148 b- defN 23-Aug-07 06:04 secretflow/security/aggregation/secure_aggregator.py
+-rw-r--r--  2.0 unx     5009 b- defN 23-Aug-07 06:04 secretflow/security/aggregation/sparse_plain_aggregator.py
+-rw-r--r--  2.0 unx     4179 b- defN 23-Aug-07 06:04 secretflow/security/aggregation/spu_aggregator.py
+-rw-r--r--  2.0 unx      858 b- defN 23-Aug-07 06:04 secretflow/security/compare/__init__.py
+-rw-r--r--  2.0 unx     1066 b- defN 23-Aug-07 06:04 secretflow/security/compare/comparator.py
+-rw-r--r--  2.0 unx     2220 b- defN 23-Aug-07 06:04 secretflow/security/compare/device_comparator.py
+-rw-r--r--  2.0 unx     1871 b- defN 23-Aug-07 06:04 secretflow/security/compare/plain_comparator.py
+-rw-r--r--  2.0 unx     1762 b- defN 23-Aug-07 06:04 secretflow/security/compare/spu_comparator.py
+-rw-r--r--  2.0 unx      948 b- defN 23-Aug-07 06:04 secretflow/security/privacy/__init__.py
+-rw-r--r--  2.0 unx  2208436 b- defN 23-Aug-07 06:07 secretflow/security/privacy/_lib.cpython-38-darwin.so
+-rw-r--r--  2.0 unx     3012 b- defN 23-Aug-07 06:04 secretflow/security/privacy/strategy.py
+-rw-r--r--  2.0 unx     1855 b- defN 23-Aug-07 06:04 secretflow/security/privacy/strategy_fl.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/security/privacy/accounting/__init__.py
+-rw-r--r--  2.0 unx     1814 b- defN 23-Aug-07 06:04 secretflow/security/privacy/accounting/budget_accountant.py
+-rw-r--r--  2.0 unx     2508 b- defN 23-Aug-07 06:04 secretflow/security/privacy/accounting/gdp_accountant.py
+-rw-r--r--  2.0 unx     3670 b- defN 23-Aug-07 06:04 secretflow/security/privacy/accounting/log_utils.py
+-rw-r--r--  2.0 unx     5702 b- defN 23-Aug-07 06:04 secretflow/security/privacy/accounting/rdp_accountant.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/security/privacy/mechanism/__init__.py
+-rw-r--r--  2.0 unx     2567 b- defN 23-Aug-07 06:04 secretflow/security/privacy/mechanism/label_dp.py
+-rw-r--r--  2.0 unx     5420 b- defN 23-Aug-07 06:04 secretflow/security/privacy/mechanism/mechanism_fl.py
+-rw-r--r--  2.0 unx      668 b- defN 23-Aug-07 06:04 secretflow/security/privacy/mechanism/tensorflow/__init__.py
+-rw-r--r--  2.0 unx     2750 b- defN 23-Aug-07 06:04 secretflow/security/privacy/mechanism/tensorflow/layers.py
+-rw-r--r--  2.0 unx      668 b- defN 23-Aug-07 06:04 secretflow/security/privacy/mechanism/torch/__init__.py
+-rw-r--r--  2.0 unx     2115 b- defN 23-Aug-07 06:04 secretflow/security/privacy/mechanism/torch/layers.py
+-rw-r--r--  2.0 unx     1195 b- defN 23-Aug-07 06:04 secretflow/stats/__init__.py
+-rw-r--r--  2.0 unx     4356 b- defN 23-Aug-07 06:04 secretflow/stats/biclassification_eval.py
+-rw-r--r--  2.0 unx     3440 b- defN 23-Aug-07 06:04 secretflow/stats/prediction_bias_eval.py
+-rw-r--r--  2.0 unx     2386 b- defN 23-Aug-07 06:04 secretflow/stats/psi_eval.py
+-rw-r--r--  2.0 unx     2995 b- defN 23-Aug-07 06:04 secretflow/stats/regression_eval.py
+-rw-r--r--  2.0 unx     4421 b- defN 23-Aug-07 06:04 secretflow/stats/score_card.py
+-rw-r--r--  2.0 unx     2421 b- defN 23-Aug-07 06:04 secretflow/stats/ss_pearsonr_v.py
+-rw-r--r--  2.0 unx     7265 b- defN 23-Aug-07 06:04 secretflow/stats/ss_pvalue_v.py
+-rw-r--r--  2.0 unx     3631 b- defN 23-Aug-07 06:04 secretflow/stats/ss_vif_v.py
+-rw-r--r--  2.0 unx     3450 b- defN 23-Aug-07 06:04 secretflow/stats/table_statistics.py
+-rw-r--r--  2.0 unx      750 b- defN 23-Aug-07 06:04 secretflow/stats/core/__init__.py
+-rw-r--r--  2.0 unx    21014 b- defN 23-Aug-07 06:04 secretflow/stats/core/biclassification_eval_core.py
+-rw-r--r--  2.0 unx     7200 b- defN 23-Aug-07 06:04 secretflow/stats/core/prediction_bias_core.py
+-rw-r--r--  2.0 unx     2802 b- defN 23-Aug-07 06:04 secretflow/stats/core/psi_core.py
+-rw-r--r--  2.0 unx     1776 b- defN 23-Aug-07 06:04 secretflow/stats/core/utils.py
+-rw-r--r--  2.0 unx      662 b- defN 23-Aug-07 06:04 secretflow/utils/__init__.py
+-rw-r--r--  2.0 unx    13370 b- defN 23-Aug-07 06:04 secretflow/utils/cloudpickle.py
+-rw-r--r--  2.0 unx      400 b- defN 23-Aug-07 06:04 secretflow/utils/communicate.py
+-rw-r--r--  2.0 unx    18161 b- defN 23-Aug-07 06:04 secretflow/utils/compressor.py
+-rw-r--r--  2.0 unx     1084 b- defN 23-Aug-07 06:04 secretflow/utils/errors.py
+-rw-r--r--  2.0 unx      870 b- defN 23-Aug-07 06:04 secretflow/utils/hash.py
+-rw-r--r--  2.0 unx      978 b- defN 23-Aug-07 06:04 secretflow/utils/io.py
+-rw-r--r--  2.0 unx      964 b- defN 23-Aug-07 06:04 secretflow/utils/logging.py
+-rw-r--r--  2.0 unx     2925 b- defN 23-Aug-07 06:04 secretflow/utils/ndarray_bigint.py
+-rw-r--r--  2.0 unx     2489 b- defN 23-Aug-07 06:04 secretflow/utils/ndarray_encoding.py
+-rw-r--r--  2.0 unx      402 b- defN 23-Aug-07 06:04 secretflow/utils/progress.py
+-rw-r--r--  2.0 unx      803 b- defN 23-Aug-07 06:04 secretflow/utils/random.py
+-rw-r--r--  2.0 unx     1330 b- defN 23-Aug-07 06:04 secretflow/utils/ray_compatibility.py
+-rw-r--r--  2.0 unx     3930 b- defN 23-Aug-07 06:04 secretflow/utils/sigmoid.py
+-rw-r--r--  2.0 unx     3308 b- defN 23-Aug-07 06:04 secretflow/utils/testing.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Aug-07 06:04 secretflow/utils/simulation/__init__.py
+-rw-r--r--  2.0 unx    30720 b- defN 23-Aug-07 06:04 secretflow/utils/simulation/datasets.py
+-rw-r--r--  2.0 unx     8941 b- defN 23-Aug-07 06:04 secretflow/utils/simulation/tf_gnn_model.py
+-rw-r--r--  2.0 unx      769 b- defN 23-Aug-07 06:04 secretflow/utils/simulation/data/__init__.py
+-rw-r--r--  2.0 unx     2548 b- defN 23-Aug-07 06:04 secretflow/utils/simulation/data/_utils.py
+-rw-r--r--  2.0 unx     5389 b- defN 23-Aug-07 06:04 secretflow/utils/simulation/data/dataframe.py
+-rw-r--r--  2.0 unx     4093 b- defN 23-Aug-07 06:04 secretflow/utils/simulation/data/ndarray.py
+-rw-r--r--  2.0 unx    11356 b- defN 23-Aug-07 06:07 secretflow_lite-1.1.0b0.dev0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3804 b- defN 23-Aug-07 06:07 secretflow_lite-1.1.0b0.dev0.dist-info/METADATA
+-rw-r--r--  2.0 unx      108 b- defN 23-Aug-07 06:07 secretflow_lite-1.1.0b0.dev0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       50 b- defN 23-Aug-07 06:07 secretflow_lite-1.1.0b0.dev0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       44 b- defN 23-Aug-07 06:07 secretflow_lite-1.1.0b0.dev0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    35062 b- defN 23-Aug-07 06:07 secretflow_lite-1.1.0b0.dev0.dist-info/RECORD
+339 files, 4106921 bytes uncompressed, 1590043 bytes compressed:  61.3%
```

## zipnote {}

```diff
@@ -297,18 +297,18 @@
 
 Filename: secretflow/ml/boost/sgb_v/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/model.py
 Comment: 
 
-Filename: secretflow/ml/boost/sgb_v/sgb.py
+Filename: secretflow/ml/boost/sgb_v/core/__init__.py
 Comment: 
 
-Filename: secretflow/ml/boost/sgb_v/core/__init__.py
+Filename: secretflow/ml/boost/sgb_v/core/params.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/core/cache/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/core/cache/level_cache.py
 Comment: 
@@ -318,29 +318,14 @@
 
 Filename: secretflow/ml/boost/sgb_v/core/distributed_tree/distributed_tree.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/core/distributed_tree/split_tree.py
 Comment: 
 
-Filename: secretflow/ml/boost/sgb_v/core/label_holder/__init__.py
-Comment: 
-
-Filename: secretflow/ml/boost/sgb_v/core/label_holder/label_holder.py
-Comment: 
-
-Filename: secretflow/ml/boost/sgb_v/core/preprocessing/__init__.py
-Comment: 
-
-Filename: secretflow/ml/boost/sgb_v/core/preprocessing/params.py
-Comment: 
-
-Filename: secretflow/ml/boost/sgb_v/core/preprocessing/preprocessing.py
-Comment: 
-
 Filename: secretflow/ml/boost/sgb_v/core/pure_numpy_ops/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/core/pure_numpy_ops/boost.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/core/pure_numpy_ops/bucket_sum.py
@@ -354,38 +339,20 @@
 
 Filename: secretflow/ml/boost/sgb_v/core/pure_numpy_ops/pred.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/core/pure_numpy_ops/random.py
 Comment: 
 
-Filename: secretflow/ml/boost/sgb_v/core/split_tree_trainer/__init__.py
-Comment: 
-
-Filename: secretflow/ml/boost/sgb_v/core/split_tree_trainer/order_map_context.py
-Comment: 
-
-Filename: secretflow/ml/boost/sgb_v/core/split_tree_trainer/shuffler.py
-Comment: 
-
-Filename: secretflow/ml/boost/sgb_v/core/split_tree_trainer/split_tree_trainer.py
-Comment: 
-
-Filename: secretflow/ml/boost/sgb_v/core/split_tree_trainer/splitter.py
-Comment: 
-
 Filename: secretflow/ml/boost/sgb_v/factory/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/factory.py
 Comment: 
 
-Filename: secretflow/ml/boost/sgb_v/factory/params.py
-Comment: 
-
 Filename: secretflow/ml/boost/sgb_v/factory/sgb_actor.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/booster/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/booster/global_ordermap_booster.py
@@ -444,14 +411,17 @@
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/loss_computer/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/loss_computer/loss_computer.py
 Comment: 
 
+Filename: secretflow/ml/boost/sgb_v/factory/components/loss_computer/loss_computer_actor.py
+Comment: 
+
 Filename: secretflow/ml/boost/sgb_v/factory/components/model_builder/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/model_builder/model_builder.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/node_selector/__init__.py
@@ -462,14 +432,17 @@
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/order_map_manager/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_actor.py
 Comment: 
 
+Filename: secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_context.py
+Comment: 
+
 Filename: secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_manager.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/sampler/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/sampler/sample_actor.py
@@ -480,14 +453,17 @@
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/shuffler/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/shuffler/shuffler.py
 Comment: 
 
+Filename: secretflow/ml/boost/sgb_v/factory/components/shuffler/shuffler_core.py
+Comment: 
+
 Filename: secretflow/ml/boost/sgb_v/factory/components/shuffler/worker_shuffler.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/__init__.py
 Comment: 
 
 Filename: secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_heap.py
@@ -690,14 +666,29 @@
 
 Filename: secretflow/ml/nn/sl/agglayer/agg_layer.py
 Comment: 
 
 Filename: secretflow/ml/nn/sl/agglayer/agg_method.py
 Comment: 
 
+Filename: secretflow/ml/nn/sl/attack/__init__.py
+Comment: 
+
+Filename: secretflow/ml/nn/sl/attack/torch/__init__.py
+Comment: 
+
+Filename: secretflow/ml/nn/sl/attack/torch/feature_inference_attack.py
+Comment: 
+
+Filename: secretflow/ml/nn/sl/attack/torch/label_inferece_attack.py
+Comment: 
+
+Filename: secretflow/ml/nn/sl/attack/torch/metrics.py
+Comment: 
+
 Filename: secretflow/ml/nn/sl/backend/__init__.py
 Comment: 
 
 Filename: secretflow/ml/nn/sl/backend/tensorflow/__init__.py
 Comment: 
 
 Filename: secretflow/ml/nn/sl/backend/tensorflow/sl_base.py
@@ -705,23 +696,29 @@
 
 Filename: secretflow/ml/nn/sl/backend/tensorflow/utils.py
 Comment: 
 
 Filename: secretflow/ml/nn/sl/backend/tensorflow/strategy/__init__.py
 Comment: 
 
+Filename: secretflow/ml/nn/sl/backend/tensorflow/strategy/pipeline.py
+Comment: 
+
 Filename: secretflow/ml/nn/sl/backend/tensorflow/strategy/split_async.py
 Comment: 
 
 Filename: secretflow/ml/nn/sl/backend/tensorflow/strategy/split_state_async.py
 Comment: 
 
 Filename: secretflow/ml/nn/sl/backend/torch/__init__.py
 Comment: 
 
+Filename: secretflow/ml/nn/sl/backend/torch/callback.py
+Comment: 
+
 Filename: secretflow/ml/nn/sl/backend/torch/sl_base.py
 Comment: 
 
 Filename: secretflow/ml/nn/sl/backend/torch/strategy/__init__.py
 Comment: 
 
 Filename: secretflow/ml/nn/sl/backend/torch/strategy/split_nn.py
@@ -840,15 +837,15 @@
 
 Filename: secretflow/security/compare/spu_comparator.py
 Comment: 
 
 Filename: secretflow/security/privacy/__init__.py
 Comment: 
 
-Filename: secretflow/security/privacy/_lib.cpython-38-x86_64-linux-gnu.so
+Filename: secretflow/security/privacy/_lib.cpython-38-darwin.so
 Comment: 
 
 Filename: secretflow/security/privacy/strategy.py
 Comment: 
 
 Filename: secretflow/security/privacy/strategy_fl.py
 Comment: 
@@ -960,14 +957,17 @@
 
 Filename: secretflow/utils/ndarray_bigint.py
 Comment: 
 
 Filename: secretflow/utils/ndarray_encoding.py
 Comment: 
 
+Filename: secretflow/utils/progress.py
+Comment: 
+
 Filename: secretflow/utils/random.py
 Comment: 
 
 Filename: secretflow/utils/ray_compatibility.py
 Comment: 
 
 Filename: secretflow/utils/sigmoid.py
@@ -993,26 +993,26 @@
 
 Filename: secretflow/utils/simulation/data/dataframe.py
 Comment: 
 
 Filename: secretflow/utils/simulation/data/ndarray.py
 Comment: 
 
-Filename: secretflow_lite-1.0.0b3.dist-info/LICENSE
+Filename: secretflow_lite-1.1.0b0.dev0.dist-info/LICENSE
 Comment: 
 
-Filename: secretflow_lite-1.0.0b3.dist-info/METADATA
+Filename: secretflow_lite-1.1.0b0.dev0.dist-info/METADATA
 Comment: 
 
-Filename: secretflow_lite-1.0.0b3.dist-info/WHEEL
+Filename: secretflow_lite-1.1.0b0.dev0.dist-info/WHEEL
 Comment: 
 
-Filename: secretflow_lite-1.0.0b3.dist-info/entry_points.txt
+Filename: secretflow_lite-1.1.0b0.dev0.dist-info/entry_points.txt
 Comment: 
 
-Filename: secretflow_lite-1.0.0b3.dist-info/top_level.txt
+Filename: secretflow_lite-1.1.0b0.dev0.dist-info/top_level.txt
 Comment: 
 
-Filename: secretflow_lite-1.0.0b3.dist-info/RECORD
+Filename: secretflow_lite-1.1.0b0.dev0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## secretflow/version.py

```diff
@@ -9,8 +9,8 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
-__version__ = "1.0.0b3"
+__version__ = "1.1.0b0.dev0"
```

## secretflow/component/component.py

```diff
@@ -33,16 +33,16 @@
     AttrType,
     ComponentDef,
     IoDef,
 )
 from secretflow.protos.component.evaluation_pb2 import NodeEvalParam, NodeEvalResult
 
 
-def clean_text(x: str) -> str:
-    return cleantext.clean(x, lower=False, no_line_breaks=True)
+def clean_text(x: str, no_line_breaks: bool = True) -> str:
+    return cleantext.clean(x.strip(), lower=False, no_line_breaks=no_line_breaks)
 
 
 class CompDeclError(Exception):
     ...
 
 
 class CompEvalError(Exception):
@@ -111,15 +111,15 @@
 
 
 class Component:
     def __init__(self, name: str, domain="", version="", desc="") -> None:
         self.name = name
         self.domain = domain
         self.version = version
-        self.desc = clean_text(desc)
+        self.desc = clean_text(desc, no_line_breaks=False)
 
         self.__definition = None
         self.__eval_callback = None
         self.__comp_attr_decls = []
         self.__input_io_decls = []
         self.__output_io_decls = []
         self.__argnames = set()
@@ -653,14 +653,15 @@
 
         init(
             address=config.private_config.ray_head_addr,
             num_cpus=32,
             log_to_driver=True,
             cluster_config=cluster_config,
             omp_num_threads=multiprocess.cpu_count(),
+            cross_silo_messages_max_size_in_bytes=1024**3,
         )
 
     def _check_storage(self, config: SFClusterConfig):
         # only local fs is supported at this moment.
         storage = config.private_config.storage_config
         if storage.type and storage.type != "local_fs":
             raise CompEvalError("only local_fs is supported.")
```

## secretflow/component/data_utils.py

```diff
@@ -70,25 +70,28 @@
 
 @enum.unique
 class DataSetFormatSupported(BaseEnum):
     CSV = "csv"
 
 
 SUPPORTED_VTABLE_DATA_TYPE = {
-    "i8": np.int8,
-    "i16": np.int16,
-    "i32": np.int32,
-    "i64": np.int64,
-    "u8": np.uint8,
-    "u16": np.uint16,
-    "u32": np.uint32,
-    "u64": np.uint64,
-    "f16": np.float16,
-    "f32": np.float32,
-    "f64": np.float64,
+    "int8": np.int8,
+    "int16": np.int16,
+    "int32": np.int32,
+    "int64": np.int64,
+    "uint8": np.uint8,
+    "uint16": np.uint16,
+    "uint32": np.uint32,
+    "uint64": np.uint64,
+    "float16": np.float16,
+    "float32": np.float32,
+    "float64": np.float64,
+    "bool": bool,
+    "int": int,
+    "float": float,
     "str": object,
 }
 
 REVERSE_DATA_TYPE_MAP = dict((v, k) for k, v in SUPPORTED_VTABLE_DATA_TYPE.items())
 
 
 def check_io_def(io_def: IoDef):
@@ -519,11 +522,11 @@
             else [],
             labels=(label_keys if label_keys is not None else []) + [pred_name],
             label_types=(
                 [REVERSE_DATA_TYPE_MAP[label_header[party][k]] for k in label_keys]
                 if label_keys is not None
                 else []
             )
-            + ["f32"],
+            + ["float"],
         ),
         num_lines=num_lines if num_lines is not None else -1,
     )
```

## secretflow/component/eval_param_reader.py

```diff
@@ -140,15 +140,16 @@
         self._instance_attrs = {}
         for path, attr in zip(
             list(self._instance.attr_paths), list(self._instance.attrs)
         ):
             if path in self._instance_attrs:
                 raise EvalParamError(f"attr {path} is duplicate in node def.")
 
-            self._instance_attrs[path] = attr
+            if not attr.is_na:
+                self._instance_attrs[path] = attr
 
         for attr in self._definition.attrs:
             if attr.type not in [
                 AttrType.AT_FLOAT,
                 AttrType.AT_FLOATS,
                 AttrType.AT_INT,
                 AttrType.AT_INTS,
```

## secretflow/component/feature/vert_woe_binning.py

```diff
@@ -105,15 +105,15 @@
     lower_bound_inclusive=False,
     upper_bound=1,
     upper_bound_inclusive=True,
 )
 vert_woe_binning_comp.io(
     io_type=IoType.INPUT,
     name="input_data",
-    desc="Input dataset for generating rule.",
+    desc="Input vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=[
         TableColParam(
             name="feature_selects",
             desc="which features should be binned.",
             col_min_cnt_inclusive=1,
         )
@@ -235,22 +235,22 @@
     desc="Vertical partitioning dataset to be substituted.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=None,
 )
 vert_woe_substitution_comp.io(
     io_type=IoType.INPUT,
     name="woe_rule",
-    desc="WOE substitution rule.",
+    desc="Input WOE substitution rule.",
     types=[DistDataType.WOE_RUNNING_RULE],
     col_params=None,
 )
 vert_woe_substitution_comp.io(
     io_type=IoType.OUTPUT,
     name="output_data",
-    desc="Output substituted dataset.",
+    desc="Output vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=None,
 )
 
 
 @vert_woe_substitution_comp.eval_fn
 def vert_woe_substitution_eval_fn(
@@ -281,19 +281,19 @@
         woe_rule[obj.device] = obj
 
     with ctx.tracer.trace_running():
         output_df = VertWOESubstitution().substitution(input_df, woe_rule)
 
     vt_wrapper = VerticalTableWrapper.from_dist_data(input_data, output_df.shape[0])
 
-    # modify types of feature_selects to f32
+    # modify types of feature_selects to float
     for v in vt_wrapper.schema_map.values():
         for i, f in enumerate(list(v.features)):
             if f in feature_selects:
-                v.feature_types[i] = 'f32'
+                v.feature_types[i] = 'float'
 
     return {
         "output_data": dump_vertical_table(
             ctx,
             output_df,
             output_data,
             vt_wrapper,
```

## secretflow/component/ml/boost/sgb/sgb.py

```diff
@@ -36,17 +36,17 @@
 sgb_train_comp = Component(
     "sgb_train",
     domain="ml.train",
     version="0.0.1",
     desc="""Provides both classification and regression tree boosting (also known as GBDT, GBM)
     for vertical split dataset setting by using secure boost.
 
-    SGB is short for SecureBoost. Compared to its safer counterpart SS-XGB, SecureBoost focused on protecting label holder.
+    - SGB is short for SecureBoost. Compared to its safer counterpart SS-XGB, SecureBoost focused on protecting label holder.
 
-    Check https://arxiv.org/abs/1901.08755.
+    - Check https://arxiv.org/abs/1901.08755.
     """,
 )
 sgb_train_comp.int_attr(
     name="num_boost_round",
     desc="Number of boosting iterations.",
     is_list=False,
     is_optional=True,
@@ -106,25 +106,14 @@
     default_value=0.1,
     lower_bound=0,
     upper_bound=10000,
     lower_bound_inclusive=True,
     upper_bound_inclusive=True,
 )
 sgb_train_comp.float_attr(
-    name="subsample",
-    desc="Subsample ratio of the training instances.",
-    is_list=False,
-    is_optional=True,
-    default_value=1,
-    lower_bound=0,
-    upper_bound=1,
-    lower_bound_inclusive=False,
-    upper_bound_inclusive=True,
-)
-sgb_train_comp.float_attr(
     name="colsample_by_tree",
     desc="Subsample ratio of columns when constructing each tree.",
     is_list=False,
     is_optional=True,
     default_value=1,
     lower_bound=0,
     upper_bound=1,
@@ -156,32 +145,149 @@
     desc="Pseudorandom number generator seed.",
     is_list=False,
     is_optional=True,
     default_value=42,
     lower_bound=0,
     lower_bound_inclusive=True,
 )
+
 sgb_train_comp.int_attr(
     name="fixed_point_parameter",
     desc="""Any floating point number encoded by heu,
             will multiply a scale and take the round,
             scale = 2 ** fixed_point_parameter.
             larger value may mean more numerical accuracy,
             but too large will lead to overflow problem.""",
     is_list=False,
     is_optional=True,
     default_value=20,
+    lower_bound=1,
+    upper_bound=100,
+    lower_bound_inclusive=True,
+    upper_bound_inclusive=True,
+)
+sgb_train_comp.bool_attr(
+    name="first_tree_with_label_holder_feature",
+    desc="Whether to train the first tree with label holder's own features.",
+    is_list=False,
+    is_optional=True,
+    default_value=False,
+)
+
+sgb_train_comp.bool_attr(
+    name="batch_encoding_enabled",
+    desc="If use batch encoding optimization.",
+    is_list=False,
+    is_optional=True,
+    default_value=True,
+)
+sgb_train_comp.bool_attr(
+    name="enable_quantization",
+    desc="Whether enable quantization of g and h.",
+    is_list=False,
+    is_optional=True,
+    default_value=False,
+)
+sgb_train_comp.float_attr(
+    name="quantization_scale",
+    desc="Scale the sum of g to the specified value.",
+    is_list=False,
+    is_optional=True,
+    default_value=10000.0,
+    lower_bound=0,
+    upper_bound=10000000.0,
+    lower_bound_inclusive=True,
+    upper_bound_inclusive=True,
+)
+
+sgb_train_comp.int_attr(
+    name="max_leaf",
+    desc="Maximum leaf of a tree. Only effective if train leaf wise.",
+    is_list=False,
+    is_optional=True,
+    default_value=15,
+    lower_bound=1,
+    upper_bound=2**15,
+    lower_bound_inclusive=True,
+    upper_bound_inclusive=True,
+)
+
+sgb_train_comp.float_attr(
+    name="rowsample_by_tree",
+    desc="Row sub sample ratio of the training instances.",
+    is_list=False,
+    is_optional=True,
+    default_value=1,
+    lower_bound=0,
+    upper_bound=1,
+    lower_bound_inclusive=False,
+    upper_bound_inclusive=True,
+)
+
+sgb_train_comp.bool_attr(
+    name="enable_goss",
+    desc="Whether to enable GOSS.",
+    is_list=False,
+    is_optional=True,
+    default_value=False,
+)
+sgb_train_comp.float_attr(
+    name="top_rate",
+    desc="GOSS-specific parameter. The fraction of large gradients to sample.",
+    is_list=False,
+    is_optional=True,
+    default_value=0.3,
     lower_bound=0,
+    upper_bound=1,
+    lower_bound_inclusive=False,
+    upper_bound_inclusive=True,
+)
+sgb_train_comp.float_attr(
+    name="bottom_rate",
+    desc="GOSS-specific parameter. The fraction of small gradients to sample.",
+    is_list=False,
+    is_optional=True,
+    default_value=0.5,
+    lower_bound=0,
+    upper_bound=1,
+    lower_bound_inclusive=False,
+    upper_bound_inclusive=True,
+)
+sgb_train_comp.float_attr(
+    name="early_stop_criterion_g_abs_sum",
+    desc="If sum(abs(g)) is lower than or equal to this threshold, training will stop.",
+    is_list=False,
+    is_optional=True,
+    default_value=0.0,
+    lower_bound=0.0,
     lower_bound_inclusive=True,
 )
+sgb_train_comp.float_attr(
+    name="early_stop_criterion_g_abs_sum_change_ratio",
+    desc="If absolute g sum change ratio is lower than or equal to this threshold, training will stop.",
+    is_list=False,
+    is_optional=True,
+    default_value=0.0,
+    lower_bound=0,
+    upper_bound=1,
+    lower_bound_inclusive=True,
+    upper_bound_inclusive=True,
+)
+sgb_train_comp.str_attr(
+    name="tree_growing_method",
+    desc="How to grow tree?",
+    is_list=False,
+    is_optional=True,
+    default_value="level",
+)
 
 sgb_train_comp.io(
     io_type=IoType.INPUT,
     name="train_dataset",
-    desc="Input train dataset.",
+    desc="Input vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=None,
 )
 sgb_train_comp.io(
     io_type=IoType.OUTPUT,
     name="output_model",
     desc="Output model.",
@@ -189,30 +295,42 @@
 )
 
 # current version 0.1
 MODEL_MAX_MAJOR_VERSION = 0
 MODEL_MAX_MINOR_VERSION = 1
 
 
+# audit path is not supported in this form yet.
 @sgb_train_comp.eval_fn
 def sgb_train_eval_fn(
     *,
     ctx,
     num_boost_round,
     max_depth,
     learning_rate,
     objective,
     reg_lambda,
     gamma,
-    subsample,
+    rowsample_by_tree,
     colsample_by_tree,
+    bottom_rate,
+    top_rate,
+    max_leaf,
+    quantization_scale,
     sketch_eps,
     base_score,
     seed,
     fixed_point_parameter,
+    enable_goss,
+    enable_quantization,
+    batch_encoding_enabled,
+    early_stop_criterion_g_abs_sum_change_ratio,
+    early_stop_criterion_g_abs_sum,
+    tree_growing_method,
+    first_tree_with_label_holder_feature,
     train_dataset,
     output_model,
 ):
     y = load_table(ctx, train_dataset, load_labels=True)
     x = load_table(ctx, train_dataset, load_features=True)
 
     assert ctx.heu_config is not None, "need heu config in SFClusterDesc"
@@ -230,26 +348,37 @@
     }
     heu = HEU((heu_config), spu.spu_pb2.FM64)
 
     with ctx.tracer.trace_running():
         sgb = Sgb(heu)
         model = sgb.train(
             params={
-                "num_boost_round": num_boost_round,
-                "max_depth": max_depth,
-                "learning_rate": learning_rate,
-                "objective": objective,
-                "reg_lambda": reg_lambda,
-                "gamma": gamma,
-                "subsample": subsample,
-                "colsample_by_tree": colsample_by_tree,
-                "sketch_eps": sketch_eps,
-                "base_score": base_score,
-                "seed": seed,
-                "fixed_point_parameter": fixed_point_parameter,
+                'num_boost_round': num_boost_round,
+                'max_depth': max_depth,
+                'learning_rate': learning_rate,
+                'objective': objective,
+                'reg_lambda': reg_lambda,
+                'gamma': gamma,
+                'rowsample_by_tree': rowsample_by_tree,
+                'colsample_by_tree': colsample_by_tree,
+                'bottom_rate': bottom_rate,
+                'top_rate': top_rate,
+                'max_leaf': max_leaf,
+                'quantization_scale': quantization_scale,
+                'sketch_eps': sketch_eps,
+                'base_score': base_score,
+                'seed': seed,
+                'fixed_point_parameter': fixed_point_parameter,
+                'enable_goss': enable_goss,
+                'enable_quantization': enable_quantization,
+                'batch_encoding_enabled': batch_encoding_enabled,
+                'early_stop_criterion_g_abs_sum_change_ratio': early_stop_criterion_g_abs_sum_change_ratio,
+                'early_stop_criterion_g_abs_sum': early_stop_criterion_g_abs_sum,
+                'tree_growing_method': tree_growing_method,
+                'first_tree_with_label_holder_feature': first_tree_with_label_holder_feature,
             },
             dtrain=x,
             label=y,
         )
 
     m_dict = model.to_dict()
     leaf_weights = m_dict.pop("leaf_weights")
@@ -315,22 +444,22 @@
 )
 sgb_predict_comp.io(
     io_type=IoType.INPUT, name="model", desc="model", types=["sf.model.sgb"]
 )
 sgb_predict_comp.io(
     io_type=IoType.INPUT,
     name="feature_dataset",
-    desc="Input feature dataset.",
+    desc="Input vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=None,
 )
 sgb_predict_comp.io(
     io_type=IoType.OUTPUT,
     name="pred",
-    desc="Output prediction。",
+    desc="Output prediction.",
     types=[DistDataType.INDIVIDUAL_TABLE],
     col_params=None,
 )
 
 
 def load_sgb_model(ctx, pyus, model) -> SgbModel:
     model_objs, model_meta_str = model_loads(
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## secretflow/component/ml/boost/ss_xgb/ss_xgb.py

```diff
@@ -34,16 +34,16 @@
 ss_xgb_train_comp = Component(
     "ss_xgb_train",
     domain="ml.train",
     version="0.0.1",
     desc="""This method provides both classification and regression tree boosting (also known as GBDT, GBM)
     for vertical partitioning dataset setting by using secret sharing.
 
-    SS-XGB is short for secret sharing XGB.
-    More details: https://arxiv.org/pdf/2005.08479.pdf
+    - SS-XGB is short for secret sharing XGB.
+    - More details: https://arxiv.org/pdf/2005.08479.pdf
     """,
 )
 ss_xgb_train_comp.int_attr(
     name="num_boost_round",
     desc="Number of boosting iterations.",
     is_list=False,
     is_optional=True,
@@ -146,15 +146,15 @@
     lower_bound=0,
     lower_bound_inclusive=True,
 )
 
 ss_xgb_train_comp.io(
     io_type=IoType.INPUT,
     name="train_dataset",
-    desc="Train dataset",
+    desc="Input vertical table.",
     types=["sf.table.vertical_table"],
     col_params=None,
 )
 ss_xgb_train_comp.io(
     io_type=IoType.OUTPUT,
     name="output_model",
     desc="Output model.",
@@ -282,15 +282,15 @@
     name="model",
     desc="Input model.",
     types=[DistDataType.SS_XGB_MODEL],
 )
 ss_xgb_predict_comp.io(
     io_type=IoType.INPUT,
     name="feature_dataset",
-    desc="Input features.",
+    desc="Input vertical table.",
     types=["sf.table.vertical_table"],
     col_params=None,
 )
 ss_xgb_predict_comp.io(
     io_type=IoType.OUTPUT,
     name="pred",
     desc="Output prediction.",
```

## secretflow/component/ml/eval/biclassification_eval.py

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from secretflow.component.component import Component, IoType, TableColParam
 from secretflow.component.data_utils import DistDataType, load_table
 from secretflow.device.driver import reveal
-from secretflow.protos.component.comp_pb2 import Attribute, AttrType
+from secretflow.protos.component.comp_pb2 import Attribute
 from secretflow.protos.component.data_pb2 import DistData
 from secretflow.protos.component.report_pb2 import Descriptions, Div, Report, Tab, Table
 from secretflow.stats.biclassification_eval import BiClassificationEval
 
 biclassification_eval_comp = Component(
     name="biclassification_eval",
     domain="ml.eval",
@@ -57,29 +57,29 @@
     lower_bound=5,
     lower_bound_inclusive=True,
 )
 
 biclassification_eval_comp.io(
     io_type=IoType.INPUT,
     name="labels",
-    desc="labels",
+    desc="Input table with labels",
     types=[DistDataType.VERTICAL_TABLE, DistDataType.INDIVIDUAL_TABLE],
     col_params=[
         TableColParam(
             name="col",
             desc="The column name to use in the dataset. If not provided, the label of dataset will be used by default.",
             col_max_cnt_inclusive=1,
         )
     ],
 )
 
 biclassification_eval_comp.io(
     io_type=IoType.INPUT,
     name="predictions",
-    desc="predictions",
+    desc="Input table with predictions",
     types=[DistDataType.VERTICAL_TABLE, DistDataType.INDIVIDUAL_TABLE],
     col_params=[
         TableColParam(
             name="col",
             desc="The column name to use in the dataset. If not provided, the label of dataset will be used by default.",
             col_max_cnt_inclusive=1,
         )
@@ -142,79 +142,79 @@
     )
 
     def get_div_from_eq_bin_report(equal_bin_reports):
         headers, rows = [], []
         headers = [
             Table.HeaderItem(
                 name="start_value",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="end_value",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="positive",
-                type=AttrType.AT_INT,
+                type="int",
             ),
             Table.HeaderItem(
                 name="negative",
-                type=AttrType.AT_INT,
+                type="int",
             ),
             Table.HeaderItem(
                 name="total",
-                type=AttrType.AT_INT,
+                type="int",
             ),
             Table.HeaderItem(
                 name="precision",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="recall",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="false_positive_rate",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="f1_score",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="lift",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="predicted_positive_ratio",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="predicted_negative_ratio",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="cumulative_percent_of_positive",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="cumulative_percent_of_negative",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="total_cumulative_percent",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="ks",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="avg_score",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
         ]
         for idx, bin_report in enumerate(equal_bin_reports):
             rows.append(
                 Table.Row(
                     name=f'bin_{idx}',
                     items=[
@@ -254,27 +254,27 @@
             ],
         )
 
     def make_head_report_div(head_report):
         headers = [
             Table.HeaderItem(
                 name="threshold",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="FPR(False Positive Rate)",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="precision",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="recall",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
         ]
         rows = []
         for idx, report in enumerate(head_report):
             rows.append(
                 Table.Row(
                     name=f"case_{idx}",
@@ -318,56 +318,56 @@
                                 type="descriptions",
                                 descriptions=Descriptions(
                                     name="",
                                     desc="",
                                     items=[
                                         Descriptions.Item(
                                             name="total_samples",
-                                            type=AttrType.AT_INT,
+                                            type="int",
                                             value=Attribute(
                                                 i64=int(
                                                     reports.summary_report.total_samples
                                                 )
                                             ),
                                         ),
                                         Descriptions.Item(
                                             name="positive_samples",
-                                            type=AttrType.AT_INT,
+                                            type="int",
                                             value=Attribute(
                                                 i64=int(
                                                     reports.summary_report.positive_samples
                                                 )
                                             ),
                                         ),
                                         Descriptions.Item(
                                             name="negative_samples",
-                                            type=AttrType.AT_INT,
+                                            type="int",
                                             value=Attribute(
                                                 i64=int(
                                                     reports.summary_report.negative_samples
                                                 )
                                             ),
                                         ),
                                         Descriptions.Item(
                                             name="auc",
-                                            type=AttrType.AT_FLOAT,
+                                            type="float",
                                             value=Attribute(
                                                 f=reports.summary_report.auc
                                             ),
                                         ),
                                         Descriptions.Item(
                                             name="ks",
-                                            type=AttrType.AT_FLOAT,
+                                            type="float",
                                             value=Attribute(
                                                 f=reports.summary_report.ks
                                             ),
                                         ),
                                         Descriptions.Item(
                                             name="f1_score",
-                                            type=AttrType.AT_FLOAT,
+                                            type="float",
                                             value=Attribute(
                                                 f=reports.summary_report.f1_score
                                             ),
                                         ),
                                     ],
                                 ),
                             ),
```

## secretflow/component/ml/eval/prediction_bias_eval.py

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from secretflow.component.component import Component, IoType, TableColParam
 from secretflow.component.data_utils import DistDataType, load_table
 from secretflow.device.driver import reveal
-from secretflow.protos.component.comp_pb2 import Attribute, AttrType
+from secretflow.protos.component.comp_pb2 import Attribute
 from secretflow.protos.component.data_pb2 import DistData
 from secretflow.protos.component.report_pb2 import Div, Report, Tab, Table
 from secretflow.stats.core.prediction_bias_core import PredictionBiasReport
 from secretflow.stats.prediction_bias_eval import prediction_bias_eval
 
 prediction_bias_comp = Component(
     "prediction_bias_eval",
@@ -56,29 +56,29 @@
     default_value="equal_width",
     allowed_values=["equal_width", "equal_frequency"],
 )
 
 prediction_bias_comp.io(
     io_type=IoType.INPUT,
     name="labels",
-    desc="labels",
+    desc="Input table with labels.",
     types=[DistDataType.VERTICAL_TABLE, DistDataType.INDIVIDUAL_TABLE],
     col_params=[
         TableColParam(
             name="col",
             desc="The column name to use in the dataset. If not provided, the label of dataset will be used by default.",
             col_max_cnt_inclusive=1,
         )
     ],
 )
 
 prediction_bias_comp.io(
     io_type=IoType.INPUT,
     name="predictions",
-    desc="predictions",
+    desc="Input table with predictions.",
     types=[DistDataType.VERTICAL_TABLE, DistDataType.INDIVIDUAL_TABLE],
     col_params=[
         TableColParam(
             name="col",
             desc="The column name to use in the dataset. If not provided, the label of dataset will be used by default.",
             col_max_cnt_inclusive=1,
         )
@@ -94,51 +94,49 @@
 
 
 def dump_report(name, sys_info, report: PredictionBiasReport) -> DistData:
     table = Table(
         name="Prediction Bias Table",
         desc="Calculate prediction bias, ie. average of predictions - average of labels.",
         headers=[
-            Table.HeaderItem(
-                name="interval", desc="prediction interval", type=AttrType.AT_STRING
-            ),
+            Table.HeaderItem(name="interval", desc="prediction interval", type="str"),
             Table.HeaderItem(
                 name="left_endpoint",
                 desc="left endpoint of interval",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="left_closed",
                 desc="indicate if left endpoint of interval is closed",
-                type=AttrType.AT_BOOL,
+                type="bool",
             ),
             Table.HeaderItem(
                 name="right_endpoint",
                 desc="right endpoint of interval",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="right_closed",
                 desc="indicate if right endpoint of interval is closed",
-                type=AttrType.AT_BOOL,
+                type="bool",
             ),
             Table.HeaderItem(
                 name="avg_prediction",
                 desc="average prediction of interval",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="avg_label",
                 desc="average label of interval",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
             Table.HeaderItem(
                 name="bias",
                 desc="prediction bias of interval",
-                type=AttrType.AT_FLOAT,
+                type="float",
             ),
         ],
     )
 
     def gen_interval_str(left_endpoint, left_closed, right_endpoint, right_closed):
         return f"{'[' if left_closed else '('}{left_endpoint}, {right_endpoint}{']' if right_closed else ')'}"
```

## secretflow/component/ml/eval/ss_pvalue.py

```diff
@@ -14,15 +14,15 @@
 
 import numpy as np
 
 from secretflow.component.component import CompEvalError, Component, IoType
 from secretflow.component.data_utils import DistDataType, load_table
 from secretflow.component.ml.linear.ss_sgd import load_ss_sgd_model
 from secretflow.device.device.spu import SPU
-from secretflow.protos.component.comp_pb2 import Attribute, AttrType
+from secretflow.protos.component.comp_pb2 import Attribute
 from secretflow.protos.component.data_pb2 import DistData
 from secretflow.protos.component.report_pb2 import Descriptions, Div, Report, Tab
 from secretflow.stats.ss_pvalue_v import PValue
 
 ss_pvalue_comp = Component(
     "ss_pvalue",
     domain="ml.eval",
@@ -38,21 +38,21 @@
     name="model",
     desc="Input model.",
     types=[DistDataType.SS_SGD_MODEL],
 )
 ss_pvalue_comp.io(
     io_type=IoType.INPUT,
     name="input_data",
-    desc="Input dataset.",
+    desc="Input vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
 )
 ss_pvalue_comp.io(
     io_type=IoType.OUTPUT,
     name="report",
-    desc="Output report.",
+    desc="Output P-Value report.",
     types=[DistDataType.REPORT],
 )
 
 
 @ss_pvalue_comp.eval_fn
 def ss_pearsonr_eval_fn(
     *,
@@ -81,23 +81,23 @@
 
     assert pv.shape[0] == len(feature_names) + 1  # last one is bias
 
     r_desc = Descriptions(
         items=[
             Descriptions.Item(
                 name=f'feature/{feature_names[i]}',
-                type=AttrType.AT_FLOAT,
+                type="float",
                 value=Attribute(f=pv[i]),
             )
             for i in range(len(feature_names))
         ]
         + [
             Descriptions.Item(
                 name="bias",
-                type=AttrType.AT_FLOAT,
+                type="float",
                 value=Attribute(f=pv[len(feature_names)]),
             )
         ],
     )
 
     report_mate = Report(
         name="pvalue",
```

## secretflow/component/ml/linear/ss_sgd.py

```diff
@@ -34,15 +34,16 @@
 
 ss_sgd_train_comp = Component(
     "ss_sgd_train",
     domain="ml.train",
     version="0.0.1",
     desc="""Train both linear and logistic regression
     linear models for vertical partitioning dataset with mini batch SGD training solver by using secret sharing.
-    SS-SGD is short for secret sharing SGD training.
+
+    - SS-SGD is short for secret sharing SGD training.
     """,
 )
 ss_sgd_train_comp.int_attr(
     name="epochs",
     desc="The number of complete pass through the training data.",
     is_list=False,
     is_optional=True,
@@ -113,22 +114,22 @@
     default_value=0.001,
     lower_bound=0,
     lower_bound_inclusive=False,
 )
 ss_sgd_train_comp.io(
     io_type=IoType.INPUT,
     name="train_dataset",
-    desc="Input train dataset.",
+    desc="Input vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=None,
 )
 ss_sgd_train_comp.io(
     io_type=IoType.OUTPUT,
     name="output_model",
-    desc="Output model",
+    desc="Output model.",
     types=[DistDataType.SS_SGD_MODEL],
 )
 
 # current version 0.1
 MODEL_MAX_MAJOR_VERSION = 0
 MODEL_MAX_MINOR_VERSION = 1
 
@@ -251,15 +252,15 @@
     name="model",
     desc="Input model.",
     types=[DistDataType.SS_SGD_MODEL],
 )
 ss_sgd_predict_comp.io(
     io_type=IoType.INPUT,
     name="feature_dataset",
-    desc="Input feature dataset.",
+    desc="Input vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=None,
 )
 ss_sgd_predict_comp.io(
     io_type=IoType.OUTPUT,
     name="pred",
     desc="Output prediction.",
```

## secretflow/component/preprocessing/feature_filter.py

```diff
@@ -25,23 +25,23 @@
     version="0.0.1",
     desc="Drop features from the dataset.",
 )
 
 feature_filter_comp.io(
     io_type=IoType.INPUT,
     name="in_ds",
-    desc="Input dataset.",
+    desc="Input vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=[TableColParam(name="drop_features", desc="Features to drop.")],
 )
 
 feature_filter_comp.io(
     io_type=IoType.OUTPUT,
     name="out_ds",
-    desc="Output dataset with filtered features.",
+    desc="Output vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=None,
 )
 
 
 @feature_filter_comp.eval_fn
 def feature_filter_eval_fn(*, ctx, in_ds, in_ds_drop_features, out_ds):
```

## secretflow/component/preprocessing/psi.py

```diff
@@ -35,15 +35,15 @@
     VerticalTable,
 )
 
 psi_comp = Component(
     "psi",
     domain="preprocessing",
     version="0.0.1",
-    desc="Balanced PSI between two parties.",
+    desc="PSI between two parties.",
 )
 psi_comp.str_attr(
     name="protocol",
     desc="PSI protocol.",
     is_list=False,
     is_optional=True,
     default_value="ECDH_PSI_2PC",
@@ -65,39 +65,39 @@
     is_optional=True,
     default_value="CURVE_FOURQ",
     allowed_values=["CURVE_25519", "CURVE_FOURQ", "CURVE_SM2", "CURVE_SECP256K1"],
 )
 psi_comp.io(
     io_type=IoType.INPUT,
     name="receiver_input",
-    desc="Input for receiver",
+    desc="Individual table for receiver",
     types=[DistDataType.INDIVIDUAL_TABLE],
     col_params=[
         TableColParam(
             name="key",
             desc="Column(s) used to join. If not provided, ids of the dataset will be used.",
         )
     ],
 )
 psi_comp.io(
     io_type=IoType.INPUT,
     name="sender_input",
-    desc="Input for sender",
+    desc="Individual table for sender",
     types=[DistDataType.INDIVIDUAL_TABLE],
     col_params=[
         TableColParam(
             name="key",
             desc="Column(s) used to join. If not provided, ids of the dataset will be used.",
         )
     ],
 )
 psi_comp.io(
     io_type=IoType.OUTPUT,
     name="psi_output",
-    desc="Output",
+    desc="Output vertical table",
     types=[DistDataType.VERTICAL_TABLE],
 )
 
 
 # We would respect user-specified ids even ids are set in TableSchema.
 def modify_schema(x: DistData, keys: List[str]) -> DistData:
     new_x = DistData()
```

## secretflow/component/preprocessing/train_test_split.py

```diff
@@ -23,15 +23,16 @@
 from secretflow.data.split import train_test_split as train_test_split_fn
 
 train_test_split_comp = Component(
     "train_test_split",
     domain="preprocessing",
     version="0.0.1",
     desc="""Split datasets into random train and test subsets.
-    Please check: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
+
+    - Please check: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
     """,
 )
 
 
 train_test_split_comp.float_attr(
     name="train_size",
     desc="Proportion of the dataset to include in the train subset.",
@@ -71,15 +72,15 @@
     is_list=False,
     is_optional=True,
     default_value=True,
 )
 train_test_split_comp.io(
     io_type=IoType.INPUT,
     name="input_data",
-    desc="Input dataset.",
+    desc="Input vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=None,
 )
 train_test_split_comp.io(
     io_type=IoType.OUTPUT,
     name="train",
     desc="Output train dataset.",
```

## secretflow/component/stats/ss_pearsonr.py

```diff
@@ -18,46 +18,45 @@
     CompEvalError,
     Component,
     IoType,
     TableColParam,
 )
 from secretflow.component.data_utils import DistDataType, load_table
 from secretflow.device.device.spu import SPU
-from secretflow.protos.component.comp_pb2 import Attribute, AttrType
+from secretflow.protos.component.comp_pb2 import Attribute
 from secretflow.protos.component.data_pb2 import DistData
 from secretflow.protos.component.report_pb2 import Div, Report, Tab, Table
 from secretflow.stats.ss_pearsonr_v import PearsonR
 
 ss_pearsonr_comp = Component(
     "ss_pearsonr",
     domain="stats",
     version="0.0.1",
     desc="""Calculate Pearson's product-moment correlation coefficient for vertical partitioning dataset
     by using secret sharing.
 
-    For large dataset(large than 10w samples & 200 features),
-    recommend to use [Ring size: 128, Fxp: 40] options for SPU device.
+    - For large dataset(large than 10w samples & 200 features), recommend to use [Ring size: 128, Fxp: 40] options for SPU device.
     """,
 )
 ss_pearsonr_comp.io(
     io_type=IoType.INPUT,
     name="input_data",
-    desc="Input dataset.",
+    desc="Input vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=[
         TableColParam(
             name="feature_selects",
             desc="Specify which features to calculate correlation coefficient with. If empty, all features will be used",
         )
     ],
 )
 ss_pearsonr_comp.io(
     io_type=IoType.OUTPUT,
     name="report",
-    desc="Output report.",
+    desc="Output Pearson's product-moment correlation coefficient report.",
     types=[DistDataType.REPORT],
 )
 
 
 @ss_pearsonr_comp.eval_fn
 def ss_pearsonr_eval_fn(
     *,
@@ -90,16 +89,15 @@
 
     feature_names = x.columns
 
     assert pr.shape[0] == len(feature_names) and pr.shape[1] == len(feature_names)
 
     r_table = Table(
         headers=[
-            Table.HeaderItem(name=f, desc="", type=AttrType.AT_FLOAT)
-            for f in feature_names
+            Table.HeaderItem(name=f, desc="", type="float") for f in feature_names
         ],
         rows=[
             Table.Row(
                 name=feature_names[r], desc="", items=[Attribute(f=c) for c in pr[r]]
             )
             for r in range(pr.shape[0])
         ],
```

## secretflow/component/stats/ss_vif.py

```diff
@@ -18,46 +18,45 @@
     CompEvalError,
     Component,
     IoType,
     TableColParam,
 )
 from secretflow.component.data_utils import DistDataType, load_table
 from secretflow.device.device.spu import SPU
-from secretflow.protos.component.comp_pb2 import Attribute, AttrType
+from secretflow.protos.component.comp_pb2 import Attribute
 from secretflow.protos.component.data_pb2 import DistData
 from secretflow.protos.component.report_pb2 import Descriptions, Div, Report, Tab
 from secretflow.stats.ss_vif_v import VIF
 
 ss_vif_comp = Component(
     "ss_vif",
     domain="stats",
     version="0.0.1",
     desc="""Calculate Variance Inflation Factor(VIF) for vertical partitioning dataset
     by using secret sharing.
 
-    For large dataset(large than 10w samples & 200 features),
-    recommend to use [Ring size: 128, Fxp: 40] options for SPU device.
+    - For large dataset(large than 10w samples & 200 features), recommend to use [Ring size: 128, Fxp: 40] options for SPU device.
     """,
 )
 ss_vif_comp.io(
     io_type=IoType.INPUT,
     name="input_data",
-    desc="Input dataset.",
+    desc="Input vertical table.",
     types=[DistDataType.VERTICAL_TABLE],
     col_params=[
         TableColParam(
             name="feature_selects",
             desc="Specify which features to calculate VIF with. If empty, all features will be used.",
         )
     ],
 )
 ss_vif_comp.io(
     io_type=IoType.OUTPUT,
     name="report",
-    desc="Output report.",
+    desc="Output Variance Inflation Factor(VIF) report.",
     types=[DistDataType.REPORT],
 )
 
 
 @ss_vif_comp.eval_fn
 def ss_vif_eval_fn(
     *,
@@ -90,15 +89,15 @@
     feature_names = x.columns
 
     assert vif.shape[0] == len(feature_names)
 
     r_desc = Descriptions(
         items=[
             Descriptions.Item(
-                name=feature_names[i], type=AttrType.AT_FLOAT, value=Attribute(f=vif[i])
+                name=feature_names[i], type="float", value=Attribute(f=vif[i])
             )
             for i in range(vif.shape[0])
         ],
     )
 
     report_mate = Report(
         name="vif",
```

## secretflow/component/stats/table_statistics.py

```diff
@@ -12,55 +12,78 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import pandas as pd
 
 from secretflow.component.component import Component, IoType
 from secretflow.component.data_utils import DistDataType, load_table
-from secretflow.protos.component.comp_pb2 import Attribute, AttrType
+from secretflow.protos.component.comp_pb2 import Attribute
 from secretflow.protos.component.data_pb2 import DistData
 from secretflow.protos.component.report_pb2 import Div, Report, Tab, Table
 from secretflow.stats.table_statistics import table_statistics
 
 table_statistics_comp = Component(
     name="table_statistics",
     domain="stats",
     version="0.0.1",
     desc="""Get a table of statistics,
-    including each column's datatype, total_count, count, count_na, min, max,
-    var, std, sem, skewness, kurtosis, q1, q2, q3, moment_2, moment_3, moment_4,
-    central_moment_2, central_moment_3, central_moment_4, sum, sum_2, sum_3 and sum_4.
-
-    moment_2 means E[X^2].
-    central_moment_2 means E[(X - mean(X))^2].
-    sum_2 means sum(X^2).
+    including each column's
+
+    1. datatype
+    2. total_count
+    3. count
+    4. count_na
+    5. min
+    6. max
+    7. var
+    8. std
+    9. sem
+    10. skewness
+    11. kurtosis
+    12. q1
+    13. q2
+    14. q3
+    15. moment_2
+    16. moment_3
+    17. moment_4
+    18. central_moment_2
+    19. central_moment_3
+    20. central_moment_4
+    21. sum
+    22. sum_2
+    23. sum_3
+    24. sum_4
+
+    - moment_2 means E[X^2].
+    - central_moment_2 means E[(X - mean(X))^2].
+    - sum_2 means sum(X^2).
     """,
 )
 
 
 table_statistics_comp.io(
     io_type=IoType.INPUT,
     name="input_data",
-    desc="Input data.",
+    desc="Input table.",
     types=[DistDataType.VERTICAL_TABLE, DistDataType.INDIVIDUAL_TABLE],
     col_params=None,
 )
 table_statistics_comp.io(
     io_type=IoType.OUTPUT,
     name="report",
-    desc="Output report.",
+    desc="Output table statistics report.",
     types=[DistDataType.REPORT],
     col_params=None,
 )
 
 
 def gen_table_statistic_report(df: pd.DataFrame) -> Report:
     headers, rows = [], []
     for k in df.columns:
-        headers.append(Table.HeaderItem(name=k, desc="", type=AttrType.AT_STRING))
+        headers.append(Table.HeaderItem(name=k, desc="", type="str"))
 
     for index, df_row in df.iterrows():
         rows.append(
             Table.Row(
                 name=index, items=[Attribute(s=str(df_row[k])) for k in df.columns]
             )
         )
```

## secretflow/device/driver.py

```diff
@@ -127,30 +127,34 @@
         @wraps(func_or_object)
         def wrapper(*arg, **kwargs):
             return reveal(func_or_object(*arg, **kwargs))
 
         return wrapper
     all_object_refs = []
     flatten_val, tree = jax.tree_util.tree_flatten(func_or_object)
+    all_spu_chunks_count = []
+    spu_chunks_idx = 0
 
     for x in flatten_val:
         if isinstance(x, PYUObject):
             all_object_refs.append(x.data)
         elif isinstance(x, HEUObject):
             if x.is_plain:
                 ref = x.device.get_participant(x.location).decode.remote(x.data)
             else:
                 ref = x.device.sk_keeper.decrypt_and_decode.remote(x.data, heu_encoder)
             all_object_refs.append(ref)
         elif isinstance(x, SPUObject):
             assert isinstance(
                 x.shares_name[0], (ray.ObjectRef, fed.FedObject)
             ), f"shares_name in spu obj should be ObjectRef or FedObject, but got {type(x.shares_name[0])} "
-            all_object_refs.append(x.meta)
-            all_object_refs.extend(x.device.outfeed_shares(x.shares_name))
+            info, shares_chunk = x.device.outfeed_shares(x.shares_name)
+            all_spu_chunks_count.append(len(shares_chunk))
+            all_object_refs.append(info)
+            all_object_refs.extend([s for s in shares_chunk])
         elif isinstance(x, TEEUObject):
             all_object_refs.append(x.data)
             logging.debug(f'Getting teeu data from TEEU {x.device.party}.')
 
     cur_idx = 0
     all_object = sfd.get(all_object_refs)
 
@@ -158,20 +162,22 @@
     for x in flatten_val:
         if isinstance(x, (PYUObject, HEUObject, TEEUObject)):
             new_flatten_val.append(all_object[cur_idx])
             cur_idx += 1
 
         elif isinstance(x, SPUObject):
             io = SPUIO(x.device.conf, x.device.world_size)
-            meta = all_object[cur_idx]
-            shares = [all_object[cur_idx + i + 1] for i in range(x.device.world_size)]
-            new_idx = cur_idx + x.device.world_size + 1
+            io_info = all_object[cur_idx]
+            cur_idx += 1
+            chunks_count = all_spu_chunks_count[spu_chunks_idx]
+            spu_chunks_idx += 1
+            shares_chunk = all_object[cur_idx : cur_idx + chunks_count]
+            cur_idx += chunks_count
 
-            new_flatten_val.append(io.reconstruct(shares, meta))
-            cur_idx = new_idx
+            new_flatten_val.append(io.reconstruct(shares_chunk, io_info))
         else:
             new_flatten_val.append(x)
 
     return jax.tree_util.tree_unflatten(tree, new_flatten_val)
 
 
 def wait(objects: Any):
```

## secretflow/device/device/heu.py

```diff
@@ -11,29 +11,29 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
 from functools import reduce
 from pathlib import Path
-from typing import Union, List
+from typing import List, Union
 
 import cloudpickle as pickle
 import jax.tree_util
 import numpy as np
 import ray
 import spu
 from heu import numpy as hnp
 from heu import phe
 
 import secretflow.distributed as sfd
 from secretflow.utils.errors import PartyNotFoundError
 
 from .base import Device, DeviceType
-from .spu import SPUValueMeta
+from .spu import SPUIOInfo, SPUValueMeta
 from .type_traits import (
     heu_datatype_to_numpy,
     heu_datatype_to_spu,
     spu_fxp_precision,
     spu_fxp_size,
 )
 
@@ -351,15 +351,17 @@
     def dump_pk(self, path):
         """Dump public key to the specified file."""
         pk = self.hekit.public_key()
         Path(path).parent.mkdir(parents=True, exist_ok=True)
         with open(path, "wb") as f:
             pickle.dump(pk, f)
 
-    def decrypt(self, data) -> Union[phe.Plaintext, List[phe.Plaintext], hnp.PlaintextArray]:
+    def decrypt(
+        self, data
+    ) -> Union[phe.Plaintext, List[phe.Plaintext], hnp.PlaintextArray]:
         """Decrypt data: ciphertext -> plaintext"""
         if isinstance(data, list):
             return [self.decrypt(d) for d in data]
 
         if isinstance(data, hnp.CiphertextArray):
             return self.hekit.decryptor().decrypt(data)
 
@@ -381,21 +383,21 @@
         self, data_with_mask: hnp.CiphertextArray, spu_field_type
     ):
         """H2A: Decrypt the masked data array"""
         # decrypt without decode
         data_with_mask = self.decrypt(data_with_mask)
         byte_content = data_with_mask.to_bytes(spu_fxp_size(spu_field_type), 'little')
         # ValueProto: see spu.proto in SPU repo for details.
-        proto = spu.ValueProto()
-        proto.visibility = spu.Visibility.VIS_SECRET
-        proto.data_type = heu_datatype_to_spu(self.cleartext_type)
-        proto.storage_type = f"semi2k.AShr<{spu.FieldType.Name(spu_field_type)}>"
-        proto.shape.dims.extend(data_with_mask.shape)
-        proto.content = byte_content
-        return proto.SerializeToString()
+
+        # TODO: support chunk
+        chunk = spu.spu_pb2.ValueChunkProto()
+        chunk.content = byte_content
+        chunk.chunk_offset = 0
+        chunk.total_bytes = len(chunk.content)
+        return chunk.SerializeToString()
 
 
 class HEUEvaluator(HEUActor):
     def __init__(
         self, heu_id, party: str, config, pk, cleartext_type: np.dtype, encoder
     ):
         self.config = config
@@ -463,23 +465,29 @@
         masks = [hnp.random.randint(-bound, bound, data.shape)]
         data_with_mask: hnp.CiphertextArray = data
         for m in masks:
             data_with_mask = self.evaluator.sub(data_with_mask, m)
 
         # convert mask to ValueProto
         # ValueProto: see spu.proto in SPU repo for details.
-        masks_value = []
+        shares_chunk = []
         for mask in masks:
-            proto = spu.ValueProto()
-            proto.visibility = spu.Visibility.VIS_SECRET
-            proto.data_type = heu_datatype_to_spu(self.cleartext_type)
-            proto.storage_type = f"semi2k.AShr<{spu.FieldType.Name(spu_field_type)}>"
-            proto.shape.dims.extend(tuple(mask.shape))
-            proto.content = mask.to_bytes(spu_fxp_size(spu_field_type), 'little')
-            masks_value.append(proto.SerializeToString())
+            # TODO: support chunk
+            chunk = spu.spu_pb2.ValueChunkProto()
+            chunk.content = mask.to_bytes(spu_fxp_size(spu_field_type), 'little')
+            chunk.chunk_offset = 0
+            chunk.total_bytes = len(chunk.content)
+            shares_chunk.append(chunk.SerializeToString())
+
+            meta = spu.spu_pb2.ValueMetaProto()
+            meta.visibility = spu.Visibility.VIS_SECRET
+            meta.data_type = heu_datatype_to_spu(self.cleartext_type)
+            meta.storage_type = f"semi2k.AShr<{spu.FieldType.Name(spu_field_type)}>"
+            meta.shape.dims.extend(tuple(mask.shape))
+            io_info = SPUIOInfo(0, 1, meta.SerializeToString())
 
         value_meta = SPUValueMeta(
             data.shape,
             heu_datatype_to_numpy(self.cleartext_type),
             spu.Visibility.VIS_SECRET,
             spu_protocol,
             spu_field_type,
@@ -487,15 +495,16 @@
         )
 
         # Because Flake8 is very stupid, so we return a list instead of a tuple
         # If we return a tuple, Flake8 will say there is a syntax error. (・◇・)
         return [
             value_meta,
             data_with_mask,
-            *masks_value,
+            io_info,
+            *shares_chunk,
         ]
 
 
 class HEU(Device):
     """Homomorphic encryption device
 
     HEU is a virtual device, and each HEU instance consists of multiple parties. Since HE is an
```

## secretflow/device/device/spu.py

```diff
@@ -9,14 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import functools
+import itertools
 import json
 import logging
 import os
 import struct
 import subprocess
 import sys
 import tempfile
@@ -40,14 +41,15 @@
 from heu import phe
 from spu import pir, psi, spu_pb2
 from spu.utils.distributed import dtype_spu_to_np, shape_spu_to_np
 
 import secretflow.distributed as sfd
 from secretflow.utils.errors import InvalidArgumentError
 from secretflow.utils.ndarray_bigint import BigintNdArray
+from secretflow.utils.progress import ProgressData
 
 from .base import Device, DeviceObject, DeviceType
 from .pyu import PYUObject
 from .register import dispatch
 from .type_traits import spu_datatype_to_heu, spu_fxp_size
 
 _LINK_DESC_NAMES = [
@@ -115,24 +117,42 @@
     return np.asarray(jnp.asarray(data))
 
 
 @dataclass
 class SPUValueMeta:
     """The metadata of an SPU value, which is a Numpy array or equivalent."""
 
+    # duck type for jax compile
     shape: Sequence[int]
     dtype: np.dtype
+    # used in _spu_compile
     vtype: spu.Visibility
 
     # the following meta ensures SPU object could be consumed by SPU device.
     protocol: spu_pb2.ProtocolKind
     field: spu_pb2.FieldType
     fxp_fraction_bits: int
 
 
+@dataclass
+class SPUIOInfo:
+    """Used in SPU IO"""
+
+    # for complex py-object that can be flattened into multiply numpy values.
+    # this index indicate which chunks belong to which value.
+    start_chunk_index: int
+    end_chunk_index: int
+
+    # spu.libspu.Share.meta, ValueMetaProto
+    # The main difference from SPUValueMeta is that SPUValueMeta is used to compile py function
+    # and the spu runtime does not perceive SPUValueMeta
+    # ValueMetaProto is used for SPU IO, py runtime does not perceive the content, so keep in binary form
+    meta: bytes
+
+
 class SPUObject(DeviceObject):
     def __init__(
         self,
         device: Device,
         meta: Union[ray.ObjectRef, fed.FedObject],
         shares_name: Sequence[Union[ray.ObjectRef, fed.FedObject]],
     ):
@@ -146,16 +166,16 @@
 
         2. If referred Python object is {'a': 1, 'b': [3, np.array(...)]}
         The meta would be referred to something like {'a': SPUValueMeta1,
         'b': [SPUValueMeta2, SPUValueMeta3]}
         Each element of shares would be referred to something like
         {'a': share1, 'b': [share2, share3]}
 
-        3. shares is a list of ObjectRef to share slices while these share
-        slices are not necessarily located at SPU device. The data transfer
+        3. shares is a list of ObjectRef to share chunks while these share
+        chunks are not necessarily located at SPU device. The data transfer
         would only happen when SPU device consumes SPU objects.
 
         Args:
             meta: Union[ray.ObjectRef, fed.FedObject]: Ref to the metadata.
             shares_name: Sequence[Union[ray.ObjectRef, fed.FedObject]]: names of shares of data in each SPU node.
         """
         super().__init__(device)
@@ -183,78 +203,124 @@
             runtime_config (RuntimeConfig): runtime_config of SPU device.
             world_size (int): world_size of SPU device.
         """
         self.runtime_config = runtime_config
         self.world_size = world_size
         self.io = spu.Io(self.world_size, self.runtime_config)
 
-    def make_shares(self, data: Any, vtype: spu.Visibility) -> Tuple[Any, List[Any]]:
+    def get_shares_chunk_count(self, data: Any, vtype: spu.Visibility) -> int:
+        flatten_value, _ = jax.tree_util.tree_flatten(data)
+        count = 0
+        for val in flatten_value:
+            val = _plaintext_to_numpy(val)
+            count += self.io.get_share_chunk_count(val, vtype)
+
+        return count
+
+    def make_shares(
+        self, data: Any, vtype: spu.Visibility
+    ) -> Tuple[Any, Any, List[bytes]]:
         """Convert a Python object to meta and shares of an SPUObject.
 
         Args:
             data (Any): Any Python object.
             vtype (Visibility): Visibility
 
         Returns:
-            Tuple[Any, List[Any]]: meta and shares of an SPUObject
+            Tuple[Any, Any, *List[bytes]]: meta and share chunks of an SPUObject
+            TODO: return typing in function definition is not correct,
+                  */typing.Unpack support is add in py311, not support in py38
         """
         flatten_value, tree = jax.tree_util.tree_flatten(data)
-        flatten_shares = []
+        flatten_shares_chunk = [[] for _ in range(self.world_size)]
         flatten_meta = []
+        flatten_io_info = []
 
         if len(flatten_value) == 0:
-            return data, *[[] for _ in range(self.world_size)]  # noqa e999
+            return data, data
 
         for val in flatten_value:
             val = _plaintext_to_numpy(val)
+            shares_chunk = self.io.make_shares(val, vtype)
+            assert (
+                len(shares_chunk) == self.world_size
+            ), f"{len(shares_chunk)} != {self.world_size}"
+            assert (
+                len(set([len(s.share_chunks) for s in shares_chunk])) == 1
+            ), "count of share_chunks miss match, all shares from one val should has same count."
             flatten_meta.append(
                 SPUValueMeta(
                     val.shape,
                     val.dtype,
                     vtype,
                     self.runtime_config.protocol,
                     self.runtime_config.field,
                     self.runtime_config.fxp_fraction_bits,
                 )
             )
-            flatten_shares.append(self.io.make_shares(val, vtype))
+            flatten_io_info.append(
+                SPUIOInfo(
+                    len(flatten_shares_chunk[0]),
+                    len(flatten_shares_chunk[0]) + len(shares_chunk[0].share_chunks),
+                    shares_chunk[0].meta,
+                )
+            )
+            for w in range(self.world_size):
+                flatten_shares_chunk[w].extend(shares_chunk[w].share_chunks)
 
-        return jax.tree_util.tree_unflatten(tree, flatten_meta), *[  # noqa e999
-            jax.tree_util.tree_unflatten(tree, list(shares))
-            for shares in list(zip(*flatten_shares))
-        ]
+        return (
+            jax.tree_util.tree_unflatten(tree, flatten_meta),
+            jax.tree_util.tree_unflatten(tree, flatten_io_info),
+            *(itertools.chain.from_iterable(flatten_shares_chunk)),
+        )
 
-    def reconstruct(self, shares: List[Any], meta: Any = None) -> Any:
+    def reconstruct(
+        self, shares_chunk: List[bytes], io_info: Any, meta: Any = None
+    ) -> Any:
         """Convert shares of an SPUObject to the origin Python object.
 
         Args:
             shares (List[Any]): Shares of an SPUObject
             meta (Any): Meta of an SPUObject. If not provided, sanity check would be skipped.
 
         Returns:
             Any: the origin Python object.
         """
-        assert len(shares) == self.world_size
+        assert (
+            len(shares_chunk) % self.world_size == 0
+        ), f"{len(shares_chunk)} % {self.world_size}"
+        flatten_info, flatten_tree = jax.tree_util.tree_flatten(io_info)
         if meta:
             flatten_metas, _ = jax.tree_util.tree_flatten(meta)
+            assert len(flatten_metas) == len(
+                flatten_info
+            ), f"{len(flatten_metas)} != {len(flatten_info)}"
             for m in flatten_metas:
                 assert m.protocol == self.runtime_config.protocol
                 assert m.field == self.runtime_config.field
                 assert m.fxp_fraction_bits == self.runtime_config.fxp_fraction_bits
-        _, tree = jax.tree_util.tree_flatten(shares[0])
-        flatten_shares = []
-        for share in shares:
-            flatten_share, _ = jax.tree_util.tree_flatten(share)
-            flatten_shares.append(flatten_share)
 
-        flatten_value = [
-            self.io.reconstruct(list(shares)) for shares in list(zip(*flatten_shares))
+        chunks_count_pre_party = int(len(shares_chunk) / self.world_size)
+        chunks_pre_party = [
+            shares_chunk[i * chunks_count_pre_party : (i + 1) * chunks_count_pre_party]
+            for i in range(self.world_size)
         ]
+        flatten_value = []
+        for info in flatten_info:
+            shares = []
+            for i in range(self.world_size):
+                share = spu.libspu.Share()
+                share.meta = info.meta
+                share.share_chunks = chunks_pre_party[i][
+                    info.start_chunk_index : info.end_chunk_index
+                ]
+                shares.append(share)
+            flatten_value.append(self.io.reconstruct(shares))
 
-        return jax.tree_util.tree_unflatten(tree, flatten_value)
+        return jax.tree_util.tree_unflatten(flatten_tree, flatten_value)
 
 
 @unique
 class SPUCompilerNumReturnsPolicy(Enum):
     """Tell SPU device how to decide num of returns of called function."""
 
     FROM_COMPILER = 'from_compiler'
@@ -319,43 +385,73 @@
     def __repr__(self):
         return f"SPURuntime(device_id={self.id}, party={self.party})"
 
     def get_new_share_name(self) -> str:
         self.share_seq_id += 1
         return f"{self.share_seq_id}"
 
-    def infeed_share(self, val: Any) -> Any:
-        flatten_val, flatten_tree = jax.tree_util.tree_flatten(val)
+    def infeed_share(self, io_info: Any, *shares_chunk: List[bytes]) -> Any:
+        flatten_io_info, flatten_tree = jax.tree_util.tree_flatten(io_info)
         shares_name = []
-        for share in flatten_val:
+        for io_info in flatten_io_info:
+            share = spu.libspu.Share()
+            share.meta = io_info.meta
+            share.share_chunks = shares_chunk[
+                io_info.start_chunk_index : io_info.end_chunk_index
+            ]
+
             name = self.get_new_share_name()
             self.runtime.set_var(name, share)
             shares_name.append(name)
 
         return jax.tree_util.tree_unflatten(flatten_tree, shares_name)
 
-    def outfeed_share(self, val: Any) -> Any:
+    def outfeed_share(self, val: Any) -> Tuple[Any, List[bytes]]:
         flatten_names, flatten_tree = jax.tree_util.tree_flatten(val)
-        shares = []
+        shares_chunk = []
+        flatten_io_info = []
+
+        if len(flatten_names) == 0:
+            return val
+
         for name in flatten_names:
-            shares.append(self.runtime.get_var(name))
+            var = self.runtime.get_var(name)
+            flatten_io_info.append(
+                SPUIOInfo(
+                    len(shares_chunk),
+                    len(shares_chunk) + len(var.share_chunks),
+                    var.meta,
+                )
+            )
+            shares_chunk.extend(var.share_chunks)
 
-        return jax.tree_util.tree_unflatten(flatten_tree, shares)
+        return (
+            jax.tree_util.tree_unflatten(flatten_tree, flatten_io_info),
+            *shares_chunk,
+        )
+
+    def outfeed_shares_chunk_count(self, val: Any) -> int:
+        flatten_names, _ = jax.tree_util.tree_flatten(val)
+        chunk_count = 0
+        for name in flatten_names:
+            chunk_count += self.runtime.get_var_chunk_count(name)
+
+        return chunk_count
 
     def del_share(self, val: Any):
         flatten_names, _ = jax.tree_util.tree_flatten(val)
         for name in flatten_names:
             assert isinstance(name, str)
             self.runtime.del_var(name)
 
     def dump(self, meta: Any, val: Any, path: str):
         flatten_names, _ = jax.tree_util.tree_flatten(val)
         shares = []
         for name in flatten_names:
-            shares.append(self.runtime.get_var(name).decode("latin1"))
+            shares.append(self.runtime.get_var(name))
 
         import cloudpickle as pickle
         from pathlib import Path
 
         # create parent folders.
         file = Path(path)
         file.parent.mkdir(parents=True, exist_ok=True)
@@ -371,15 +467,15 @@
 
         meta = record['meta']
         shares = record['shares']
 
         shares_name = []
         for share in shares:
             name = self.get_new_share_name()
-            self.runtime.set_var(name, share.encode("latin1"))
+            self.runtime.set_var(name, share)
             shares_name.append(name)
 
         _, flatten_tree = jax.tree_util.tree_flatten(meta)
 
         return meta, jax.tree_util.tree_unflatten(flatten_tree, shares_name)
 
     def run(
@@ -437,61 +533,87 @@
         elif num_returns_policy == SPUCompilerNumReturnsPolicy.FROM_USER:
             _, out_tree = jax.tree_util.tree_flatten(out_shape)
             single_meta, single_share = jax.tree_util.tree_unflatten(
                 out_tree, metadata
             ), jax.tree_util.tree_unflatten(out_tree, output_names)
 
             if hasattr(single_meta, '__iter__'):
-                return *(single_meta), *(single_share)
+                return (*(single_meta), *(single_share))
             else:
                 return single_meta, single_share
         else:
             raise ValueError('unsupported SPUCompilerNumReturnsPolicy.')
 
-    def a2h(self, value, exp_heu_data_type: str, schema):
+    def a2h(self, io_info, exp_heu_data_type: str, schema, *chunks):
         """Convert SPUObject to HEUObject.
 
         Args:
             tree (PyTreeLeaf): SPUObject meta info.
 
             exp_heu_data_type (str): HEU data type.
 
         Returns:
             np.ndarray: Array of `phe.Plaintext`.
         """
+        assert isinstance(io_info, SPUIOInfo), "not support pytree for now"
+        spu_meta = spu_pb2.ValueMetaProto()
+        spu_meta.ParseFromString(io_info.meta)
+        assert io_info.start_chunk_index == 0, "not support pytree for now"
+        assert io_info.end_chunk_index == len(chunks), "not support pytree for now"
 
-        def _bytes_to_pb(msg: bytes) -> spu_pb2.ValueProto:
-            ret = spu_pb2.ValueProto()
-            ret.ParseFromString(msg)
-            return ret
-
-        # TODO: avoid der ValueProto in python
-        value = _bytes_to_pb(value)
         expect_st = f"semi2k.AShr<{spu.spu_pb2.FieldType.Name(self.conf.field)}>"
         assert (
-            value.storage_type == expect_st
-        ), f"Unsupported storage type {value.storage_type}, expected {expect_st}"
+            spu_meta.storage_type == expect_st
+        ), f"Unsupported storage type {spu_meta.storage_type}, expected {expect_st}"
 
-        assert spu_datatype_to_heu(value.data_type) == exp_heu_data_type, (
-            f"You cannot feed {value.data_type} into this HEU since it only "
+        assert spu_datatype_to_heu(spu_meta.data_type) == exp_heu_data_type, (
+            f"You cannot feed {spu_meta.data_type} into this HEU since it only "
             f"supports {exp_heu_data_type}, if you want to change data type of HEU, "
             f"please modify the initial configuration of HEU."
         )
 
         size = spu_fxp_size(self.conf.field)
+
+        def _bytes_to_pb(chunk_idx: int) -> spu_pb2.ValueChunkProto:
+            ret = spu_pb2.ValueChunkProto()
+            ret.ParseFromString(chunks[chunk_idx])
+            return ret
+
+        def _get_int_bytes() -> bytes:
+            if len(chunks) == 0:
+                return
+            chunk_idx = 0
+            chunk_pb = _bytes_to_pb(chunk_idx)
+            chunk_idx += 1
+            assert (
+                chunk_pb.total_bytes % size == 0
+            ), f"share size {chunk_pb.total_bytes} need align to {size}"
+            total_pos = 0
+            chunk_pos = 0
+            int_bytes = b""
+            while total_pos < chunk_pb.total_bytes:
+                except_len = size - len(int_bytes)
+                read_len = min(except_len, len(chunk_pb.content) - chunk_pos)
+                int_bytes += chunk_pb.content[chunk_pos : chunk_pos + read_len]
+                total_pos += read_len
+                chunk_pos += read_len
+                if len(int_bytes) == size:
+                    yield int_bytes
+                    int_bytes = b""
+                if chunk_pos == len(chunk_pb.content) and chunk_idx < len(chunks):
+                    chunk_pb = _bytes_to_pb(chunk_idx)
+                    chunk_idx += 1
+                    chunk_pos = 0
+
         value = BigintNdArray(
             [
-                int.from_bytes(
-                    value.content[i * size : (i + 1) * size],
-                    sys.byteorder,
-                    signed=True,
-                )
-                for i in range(len(value.content) // size)
+                int.from_bytes(int_b, sys.byteorder, signed=True)
+                for int_b in _get_int_bytes()
             ],
-            value.shape.dims,
+            spu_meta.shape.dims,
         )
 
         return value.to_hnp(encoder=phe.BigintEncoder(schema))
 
     def psi_df(
         self,
         key: Union[str, List[str]],
@@ -503,14 +625,16 @@
         broadcast_result=True,
         bucket_size=1 << 20,
         curve_type="CURVE_25519",
         preprocess_path=None,
         ecdh_secret_key_path=None,
         dppsi_bob_sub_sampling=0.9,
         dppsi_epsilon=3,
+        progress_callbacks: Callable[[str, ProgressData], None] = None,
+        callbacks_interval_ms: int = 5 * 1000,
         ic_mode: bool = False,
     ):
         """Private set intersection with DataFrame.
 
         Args:
             key (str, List[str]): Column(s) used to join.
             data (pd.DataFrame): DataFrame to be joined.
@@ -522,14 +646,16 @@
             bucket_size (int): Specified the hash bucket size used in psi. Larger values consume more memory.
             curve_type (str): curve for ecdh psi
             preprocess_path (str): preprocess file path for unbalanced psi.
             ecdh_secret_key_path (str): ecdh_oprf secretkey file path, binary format, 32B.
             dppsi_bob_sub_sampling (double): bob subsampling bernoulli_distribution
                 probability of dp psi
             dppsi_epsilon (double): epsilon of dp psi
+            progress_callbacks (Callable[[str, ProgressData], None]): Callbacks for update progress.
+            callbacks_interval_ms (int): The interval at which the callbacks is called
             ic_mode (bool): Whether to run psi in interconnection mode
 
         Returns:
             pd.DataFrame or None: joined DataFrame.
         """
         # save key dataframe to temp file for streaming psi
         with tempfile.TemporaryDirectory() as data_dir:
@@ -550,14 +676,16 @@
                 broadcast_result,
                 bucket_size,
                 curve_type,
                 preprocess_path,
                 ecdh_secret_key_path,
                 dppsi_bob_sub_sampling,
                 dppsi_epsilon,
+                progress_callbacks,
+                callbacks_interval_ms,
                 ic_mode,
             )
 
             if report['intersection_count'] == -1:
                 # can not get result, return None
                 return None
             else:
@@ -576,14 +704,16 @@
         broadcast_result=True,
         bucket_size=1 << 20,
         curve_type="CURVE_25519",
         preprocess_path=None,
         ecdh_secret_key_path=None,
         dppsi_bob_sub_sampling=0.9,
         dppsi_epsilon=3,
+        progress_callbacks: Callable[[str, ProgressData], None] = None,
+        callbacks_interval_ms: int = 5 * 1000,
         ic_mode: bool = False,
     ):
         """Private set intersection with csv file.
 
         Examples:
             >>> spu = sf.SPU(utils.cluster_def)
             >>> alice = sf.PYU('alice'), sf.PYU('bob')
@@ -610,14 +740,16 @@
             broadcast_result (bool): Whether to broadcast joined data to all parties.
             bucket_size (int): Specified the hash bucket size used in psi.
                 Larger values consume more memory.
             curve_type (str): curve for ecdh psi
             dppsi_bob_sub_sampling (double): bob subsampling bernoulli_distribution
                 probability of dp psi
             dppsi_epsilon (double): epsilon of dp psi
+            progress_callbacks (Callable[[str, ProgressData], None]): Callbacks for update progress
+            callbacks_interval_ms (int): The interval at which the callbacks is called
             ic_mode (bool): Whether to run psi in interconnection mode
 
         Returns:
             Dict: PSI report output by SPU.
         """
         if isinstance(key, str):
             key = [key]
@@ -634,14 +766,33 @@
         receiver_rank = -1
         for i, node in enumerate(self.cluster_def['nodes']):
             if node['party'] == receiver:
                 receiver_rank = i
                 break
         assert receiver_rank >= 0, f'invalid receiver {receiver}'
 
+        # define callbacks
+        callbacks_func = None
+
+        def psi_callbacks(p_data: psi.ProgressData):
+            if progress_callbacks:
+                progress_callbacks(
+                    party,
+                    ProgressData(
+                        p_data.total,
+                        p_data.finished,
+                        p_data.running,
+                        p_data.percentage,
+                        p_data.description,
+                    ),
+                )
+
+        if progress_callbacks:
+            callbacks_func = psi_callbacks
+
         config = psi.BucketPsiConfig(
             psi_type=psi.PsiType.Value(protocol),
             broadcast_result=broadcast_result,
             receiver_rank=receiver_rank,
             input_params=psi.InputParams(
                 path=input_path, select_fields=key, precheck=precheck_input
             ),
@@ -697,15 +848,17 @@
                 assert isinstance(
                     ecdh_secret_key_path, str
                 ), f'invalid ecdh_secret_key for {protocol}'
                 config.ecdh_secret_key_path = ecdh_secret_key_path
             else:
                 config.preprocess_path = preprocess_path
 
-        report = psi.bucket_psi(self.link, config, ic_mode)
+        report = psi.bucket_psi(
+            self.link, config, callbacks_func, callbacks_interval_ms, ic_mode
+        )
 
         return {
             'party': party,
             'original_count': report.original_count,
             'intersection_count': report.intersection_count,
         }
 
@@ -714,14 +867,16 @@
         key: Union[str, List[str]],
         data: pd.DataFrame,
         receiver: str,
         join_party: str,
         protocol='KKRT_PSI_2PC',
         bucket_size=1 << 20,
         curve_type="CURVE_25519",
+        progress_callbacks: Callable[[str, ProgressData], None] = None,
+        callbacks_interval_ms: int = 5 * 1000,
         ic_mode: bool = False,
     ):
         """Private set intersection with DataFrame.
 
         Examples:
             >>> spu = sf.SPU(utils.cluster_def)
             >>> spu.psi_join_df(['c1', 'c2'], [df_alice, df_bob], 'alice', 'alice')
@@ -730,14 +885,16 @@
             key (str, List[str]): Column(s) used to join.
             data (pd.DataFrame): DataFrame to be joined.
             receiver (str): Which party can get joined data, others will get None.
             join_party (str): party joined data
             protocol (str): PSI protocol, See spu.psi.PsiType.
             bucket_size (int): Specified the hash bucket size used in psi. Larger values consume more memory.
             curve_type (str): curve for ecdh psi
+            progress_callbacks (Callable[[str, ProgressData], None]): Callbacks for update progress
+            callbacks_interval_ms (int): The interval at which the callbacks is called
             ic_mode (bool): Whether to run psi in interconnection mode
 
         Returns:
             pd.DataFrame or None: joined DataFrame.
         """
         # save key dataframe to temp file for streaming psi
         with tempfile.TemporaryDirectory() as data_dir:
@@ -752,14 +909,16 @@
                 input_path,
                 output_path,
                 receiver,
                 join_party,
                 protocol,
                 bucket_size,
                 curve_type,
+                progress_callbacks,
+                callbacks_interval_ms,
                 ic_mode,
             )
 
             if report['intersection_count'] == -1:
                 # can not get result, return None
                 return None
             else:
@@ -772,14 +931,16 @@
         input_path: str,
         output_path: str,
         receiver: str,
         join_party: str,
         protocol='KKRT_PSI_2PC',
         bucket_size=1 << 20,
         curve_type="CURVE_25519",
+        progress_callbacks: Callable[[str, ProgressData], None] = None,
+        callbacks_interval_ms: int = 5 * 1000,
         ic_mode: bool = False,
     ):
         """Private set intersection with csv file.
 
         Examples:
             >>> spu = sf.SPU(utils.cluster_def)
             >>> alice = sf.PYU('alice'), sf.PYU('bob')
@@ -794,14 +955,16 @@
             output_path: Joined csv file, comma separated and contains header.
                 Use an absolute path.
             receiver (str): Which party can get joined data. Others won't generate output file and `intersection_count` get `-1`
             join_party (str): party joined data
             protocol (str): PSI protocol.
             bucket_size (int): Specified the hash bucket size used in psi. Larger values consume more memory.
             curve_type (str): curve for ecdh psi
+            progress_callbacks (Callable[[str, ProgressData], None]): Callbacks for update progress
+            callbacks_interval_ms (int): The interval at which the callbacks is called
             ic_mode (bool): Whether to run psi in interconnection mode
 
         Returns:
             Dict: PSI report output by SPU.
         """
         assert (
             (protocol == "ECDH_PSI_2PC")
@@ -835,27 +998,51 @@
         logging.warning(
             f"origin_table size:{origin_table.shape[0]},drop_duplicates size:{table_nodup.shape[0]}"
         )
 
         # free table_nodup dataframe
         del table_nodup
 
+        # define callbacks
+        callbacks_func = None
+        party = self.cluster_def['nodes'][self.rank]['party']
+        total = 1
+
+        def psi_callbacks(psi_progress: psi.ProgressData):
+            # deal progress
+            nonlocal total
+            total = psi_progress.total + 1
+            p_data = ProgressData(
+                total,
+                psi_progress.finished,
+                psi_progress.running,
+                int(psi_progress.percentage * psi_progress.total / total),
+                psi_progress.description,
+            )
+            if progress_callbacks:
+                progress_callbacks(party, p_data)
+
+        if progress_callbacks:
+            callbacks_func = psi_callbacks
+
         # psi join case, need sort and broadcast set True
         config = psi.BucketPsiConfig(
             psi_type=psi.PsiType.Value(protocol),
             broadcast_result=True,
             receiver_rank=receiver_rank,
             input_params=psi.InputParams(
                 path=input_path1, select_fields=key, precheck=False
             ),
             output_params=psi.OutputParams(path=output_psi, need_sort=True),
             curve_type=curve_type,
             bucket_size=bucket_size,
         )
-        report = psi.bucket_psi(self.link, config, ic_mode)
+        report = psi.bucket_psi(
+            self.link, config, callbacks_func, callbacks_interval_ms, ic_mode
+        )
 
         df_psi_out = pd.read_csv(output_psi)
 
         join_rank = -1
         for i, node in enumerate(self.cluster_def['nodes']):
             if node['party'] == join_party:
                 join_rank = i
@@ -976,15 +1163,16 @@
         assert (
             sp_ret.returncode == 0
         ), f"sort cmd failed, return {sp_ret.returncode}, expected 0"
 
         # delete tmp data dir
         data_dir.cleanup()
 
-        party = self.cluster_def['nodes'][self.rank]['party']
+        if progress_callbacks:
+            progress_callbacks(party, ProgressData(total, total, 0, 100, "Join, 100%"))
 
         return {
             'party': party,
             'original_count': origin_table.shape[0],
             'intersection_count': report.intersection_count,
             'join_count': join_count,
         }
@@ -1378,15 +1566,15 @@
             meta_kwargs,
             input_name,
             input_vis,
             lambda output_flat: [
                 _generate_output_uuid() for _ in range(len(output_flat))
             ],
         )
-    except Exception as error:
+    except Exception:
         raise ray.exceptions.WorkerCrashedError()
 
     return executable, output_tree
 
 
 class SPU(Device):
     def __init__(
@@ -1527,21 +1715,19 @@
     def _place_arguments(self, *args, **kwargs):
         def place(obj):
             if isinstance(obj, DeviceObject):
                 return obj.to(self)
             else:
                 # if obj is not a DeviceObject, it should be a plaintext from
                 # host program, so it's safe to mark it as VIS_PUBLIC.
-                meta, *refs = self.io.make_shares(obj, spu.Visibility.VIS_PUBLIC)
-
-                shares_name = []
-                for i, actor in enumerate(self.actors.values()):
-                    shares_name.append(actor.infeed_share.remote(refs[i]))
+                meta, io_info, *refs = self.io.make_shares(
+                    obj, spu.Visibility.VIS_PUBLIC
+                )
 
-                return SPUObject(self, meta, shares_name)
+                return SPUObject(self, meta, self.infeed_shares(io_info, refs))
 
         return jax.tree_util.tree_map(place, (args, kwargs))
 
     def dump(self, obj: SPUObject, paths: List[str]):
         assert obj.device == self, "obj must be owned by this device."
         ret = []
         for i, actor in enumerate(self.actors.values()):
@@ -1637,50 +1823,78 @@
 
                 _, out_tree = jax.tree_util.tree_flatten(out_shape)
                 return jax.tree_util.tree_unflatten(out_tree, all_atomic_spu_objects)
 
         return wrapper
 
     def infeed_shares(
-        self, shares: List[Union[ray.ObjectRef, fed.FedObject]]
+        self,
+        io_info: Union[ray.ObjectRef, fed.FedObject],
+        shares_chunk: List[Union[ray.ObjectRef, fed.FedObject]],
     ) -> List[Union[ray.ObjectRef, fed.FedObject]]:
-        assert len(shares) == len(self.actors)
+        assert (
+            len(shares_chunk) % len(self.actors) == 0
+        ), f"{len(shares_chunk)} , {len(self.actors)}"
+        chunks_pre_actor = int(len(shares_chunk) / len(self.actors))
 
         ret = []
         for i, actor in enumerate(self.actors.values()):
-            ret.append(actor.infeed_share.remote(shares[i]))
+            start_pos = i * chunks_pre_actor
+            end_pos = (i + 1) * chunks_pre_actor
+            ret.append(
+                actor.infeed_share.remote(io_info, *shares_chunk[start_pos:end_pos])
+            )
 
         return ret
 
     def outfeed_shares(
         self, shares_name: List[Union[ray.ObjectRef, fed.FedObject]]
-    ) -> List[Union[ray.ObjectRef, fed.FedObject]]:
+    ) -> Tuple[
+        Union[ray.ObjectRef, fed.FedObject],
+        List[Union[ray.ObjectRef, fed.FedObject]],
+    ]:
         assert len(shares_name) == len(self.actors)
 
+        shares_chunk_count = sfd.get(
+            (next(iter(self.actors.values()))).outfeed_shares_chunk_count.remote(
+                shares_name[0]
+            )
+        )
+
         ret = []
         for i, actor in enumerate(self.actors.values()):
-            ret.append(actor.outfeed_share.remote(shares_name[i]))
+            remote_ret = actor.outfeed_share.options(
+                num_returns=1 + shares_chunk_count
+            ).remote(shares_name[i])
 
-        return ret
+            if shares_chunk_count == 0:
+                io_info = remote_ret
+            else:
+                io_info, *shares_chunk = remote_ret
+                ret.extend(shares_chunk)
+
+        return io_info, ret
 
     def psi_df(
         self,
         key: Union[str, List[str], Dict[Device, List[str]]],
-        dfs: List['PYUObject'],
+        dfs: List[PYUObject],
         receiver: str,
         protocol='KKRT_PSI_2PC',
         precheck_input=True,
         sort=True,
         broadcast_result=True,
         bucket_size=1 << 20,
         curve_type="CURVE_25519",
         preprocess_path=None,
         ecdh_secret_key_path=None,
         dppsi_bob_sub_sampling=0.9,
         dppsi_epsilon=3,
+        progress_callbacks: Callable[[str, ProgressData], None] = None,
+        callbacks_interval_ms: int = 5 * 1000,
     ):
         """Private set intersection with DataFrame.
 
         Args:
             key (str, List[str], Dict[Device, List[str]]): Column(s) used to join.
             dfs (List[PYUObject]): DataFrames to be joined, which
             should be colocated with SPU runtimes.
@@ -1693,14 +1907,16 @@
             Larger values consume more memory.
             curve_type (str): curve for ecdh psi.
             preprocess_path (str): preprocess file path for unbalanced psi.
             ecdh_secret_key_path (str): ecdh_oprf secretkey file path, binary format, 32B, for unbalanced psi.
             dppsi_bob_sub_sampling (double): bob subsampling bernoulli_distribution
                 probability of dp psi
             dppsi_epsilon (double): epsilon of dp psi
+            progress_callbacks (Callable[[str, ProgressData], None]): Callbacks for update progress
+            callbacks_interval_ms (int): The interval at which the callbacks is called
 
         Returns:
             List[PYUObject]: Joined DataFrames with order reserved.
         """
         return dispatch(
             'psi_df',
             self,
@@ -1713,14 +1929,16 @@
             broadcast_result,
             bucket_size,
             curve_type,
             preprocess_path,
             ecdh_secret_key_path,
             dppsi_bob_sub_sampling,
             dppsi_epsilon,
+            progress_callbacks,
+            callbacks_interval_ms,
         )
 
     def psi_csv(
         self,
         key: Union[str, List[str], Dict[Device, List[str]]],
         input_path: Union[str, Dict[Device, str]],
         output_path: Union[str, Dict[Device, str]],
@@ -1731,14 +1949,16 @@
         broadcast_result=True,
         bucket_size=1 << 20,
         curve_type="CURVE_25519",
         preprocess_path=None,
         ecdh_secret_key_path=None,
         dppsi_bob_sub_sampling=0.9,
         dppsi_epsilon=3,
+        progress_callbacks: Callable[[str, ProgressData], None] = None,
+        callbacks_interval_ms: int = 5 * 1000,
     ):
         """Private set intersection with csv file.
 
         Args:
             key (str, List[str], Dict[Device, List[str]]): Column(s) used to join.
             input_path: CSV files to be joined, comma separated and contains header.
                 Use an absolute path.
@@ -1755,14 +1975,16 @@
             Larger values consume more memory.
             curve_type (str): curve for ecdh psi.
             preprocess_path (str): preprocess file path for unbalanced psi.
             ecdh_secret_key_path (str): ecdh_oprf secretkey file path, binary format, 32B.
             dppsi_bob_sub_sampling (double): bob subsampling bernoulli_distribution
                 probability of dp psi
             dppsi_epsilon (double): epsilon of dp psi
+            progress_callbacks (Callable[[str, ProgressData], None]): Callbacks for update progress
+            callbacks_interval_ms (int): The interval at which the callbacks is called
 
         Returns:
             List[Dict]: PSI reports output by SPU with order reserved.
         """
 
         return dispatch(
             'psi_csv',
@@ -1777,36 +1999,42 @@
             broadcast_result,
             bucket_size,
             curve_type,
             preprocess_path,
             ecdh_secret_key_path,
             dppsi_bob_sub_sampling,
             dppsi_epsilon,
+            progress_callbacks,
+            callbacks_interval_ms,
         )
 
     def psi_join_df(
         self,
         key: Union[str, List[str], Dict[Device, List[str]]],
-        dfs: List['PYUObject'],
+        dfs: List[PYUObject],
         receiver: str,
         join_party: str,
         protocol='KKRT_PSI_2PC',
         bucket_size=1 << 20,
         curve_type="CURVE_25519",
+        progress_callbacks: Callable[[str, ProgressData], None] = None,
+        callbacks_interval_ms: int = 5 * 1000,
     ):
         """Private set intersection with DataFrame.
 
         Args:
             key (str, List[str], Dict[Device, List[str]]): Column(s) used to join.
             dfs (List[PYUObject]): DataFrames to be joined, which should be colocated with SPU runtimes.
             receiver (str): Which party can get joined data. Others won't generate output file and `intersection_count` get `-1`
             join_party (str): party can get joined data
             protocol (str): PSI protocol.
             bucket_size (int): Specified the hash bucket size used in psi. Larger values consume more memory.
             curve_type (str): curve for ecdh psi
+            progress_callbacks (Callable[[str, ProgressData], None]): Callbacks for update progress
+            callbacks_interval_ms (int): The interval at which the callbacks is called
 
         Returns:
             List[PYUObject]: Joined DataFrames with order reserved.
         """
 
         return dispatch(
             'psi_join_df',
@@ -1814,26 +2042,30 @@
             key,
             dfs,
             receiver,
             join_party,
             protocol,
             bucket_size,
             curve_type,
+            progress_callbacks,
+            callbacks_interval_ms,
         )
 
     def psi_join_csv(
         self,
         key: Union[str, List[str], Dict[Device, List[str]]],
         input_path: Union[str, Dict[Device, str]],
         output_path: Union[str, Dict[Device, str]],
         receiver: str,
         join_party: str,
         protocol='KKRT_PSI_2PC',
         bucket_size=1 << 20,
         curve_type="CURVE_25519",
+        progress_callbacks: Callable[[str, ProgressData], None] = None,
+        callbacks_interval_ms: int = 5 * 1000,
     ):
         """Private set intersection with csv file.
 
         Args:
             key (str, List[str], Dict[Device, List[str]]): Column(s) used to join.
             input_path: CSV files to be joined, comma separated and contains header.
                 Use an absolute path.
@@ -1841,14 +2073,16 @@
                 Use an absolute path.
             receiver (str): Which party can get joined data. Others won't generate output file and `intersection_count` get `-1`
             join_party (str): party can get joined data
             protocol (str): PSI protocol.
             precheck_input (bool): Whether check input data before joining, for now, it will check if key duplicate.
             bucket_size (int): Specified the hash bucket size used in psi. Larger values consume more memory.
             curve_type (str): curve for ecdh psi
+            progress_callbacks (Callable[[str, ProgressData], None]): Callbacks for update progress
+            callbacks_interval_ms (int): The interval at which the callbacks is called
 
         Returns:
             List[Dict]: PSI reports output by SPU with order reserved.
         """
 
         return dispatch(
             'psi_join_csv',
@@ -1857,14 +2091,16 @@
             input_path,
             output_path,
             receiver,
             join_party,
             protocol,
             bucket_size,
             curve_type,
+            progress_callbacks,
+            callbacks_interval_ms,
         )
 
     def pir_setup(
         self,
         server: str,
         input_path: Union[str, Dict[Device, str]],
         key_columns: Union[str, List[str]],
```

## secretflow/device/kernels/heu.py

```diff
@@ -88,49 +88,50 @@
     evaluator_parties = [ev for ev in heu.evaluator_names() if ev in spu.actors.keys()]
 
     # protocol is restricted to SEMI2K.
     assert spu.conf.protocol == spu_pb2.SEMI2K
 
     res = (
         heu.get_participant(self.location)
-        .h2a_make_share.options(num_returns=len(evaluator_parties) + 2)
+        .h2a_make_share.options(num_returns=len(evaluator_parties) + 3)
         .remote(
             self.data,
             evaluator_parties,
             spu.conf.protocol,
             spu.conf.field,
             0,
         )
     )
 
-    meta, sk_keeper_data, refs = (
+    meta, sk_keeper_data, io_info, chunks = (
         res[0],
         res[1],
-        res[2:],
+        res[2],
+        res[3:],
     )
 
     # sk_keeper: set data_with_mask as shard
-    sk_keeper_data = heu.sk_keeper.h2a_decrypt_make_share.remote(
+    sk_keeper_chunk = heu.sk_keeper.h2a_decrypt_make_share.remote(
         sk_keeper_data, spu.conf.field
     )
 
     # make sure sk_keeper_data would be sent to the correct spu actor.
     spu_actor_idx_for_keeper = -1
     for idx, name in enumerate(spu.actors.keys()):
         if name == heu.sk_keeper_name():
             spu_actor_idx_for_keeper = idx
             break
 
     assert (
         spu_actor_idx_for_keeper != -1
     ), f"couldn't find {heu.sk_keeper_name()} in spu actor list."
 
-    refs.insert(spu_actor_idx_for_keeper, sk_keeper_data)
+    chunks.insert(spu_actor_idx_for_keeper, sk_keeper_chunk)
 
-    return SPUObject(spu, meta, spu.infeed_shares(refs))
+    return SPUObject(spu, meta, spu.infeed_shares(io_info, chunks))
 
 
 # Data flows inside the HEU, across network
 def heu_to_same_heu(self: HEUObject, config: HEUMoveConfig):
     if self.location == config.heu_dest_party:
         return self  # nothing to do
```

## secretflow/device/kernels/pyu.py

```diff
@@ -15,14 +15,15 @@
 
 import logging
 import secrets
 from typing import Any, Callable, List, Union
 
 from spu import Visibility
 
+import secretflow.distributed as sfd
 from secretflow.device import (
     HEU,
     PYU,
     SPU,
     SPUIO,
     TEEU,
     DeviceType,
@@ -59,23 +60,36 @@
         the transferred SPUObject.
     """
     assert isinstance(spu, SPU), f'Expect an SPU but got {type(spu)}'
     assert spu_vis in ('secret', 'public'), f'vis must be public or secret'
 
     vtype = Visibility.VIS_PUBLIC if spu_vis == 'public' else Visibility.VIS_SECRET
 
+    def get_shares_chunk_count(data, runtime_config, world_size, vtype) -> int:
+        io = SPUIO(runtime_config, world_size)
+        return io.get_shares_chunk_count(data, vtype)
+
     def run_spu_io(data, runtime_config, world_size, vtype):
         io = SPUIO(runtime_config, world_size)
-        return io.make_shares(data, vtype)
+        ret = io.make_shares(data, vtype)
+        return ret
 
-    meta, *shares = self.device(run_spu_io, num_returns=(1 + spu.world_size))(
+    shares_chunk_count = self.device(get_shares_chunk_count)(
         self.data, spu.conf, spu.world_size, vtype
     )
+    shares_chunk_count = sfd.get(shares_chunk_count.data)
+
+    meta, io_info, *shares_chunk = self.device(
+        run_spu_io, num_returns=(2 + shares_chunk_count * spu.world_size)
+    )(self.data, spu.conf, spu.world_size, vtype)
+
     return SPUObject(
-        spu, meta.data, spu.infeed_shares([share.data for share in shares])
+        spu,
+        meta.data,
+        spu.infeed_shares(io_info.data, [s.data for s in shares_chunk]),
     )
 
 
 @register_to(DeviceType.PYU, DeviceType.HEU)
 def pyu_to_heu(self: PYUObject, heu: HEU, config: HEUMoveConfig = None):
     assert isinstance(heu, HEU), f'Expect an HEU but got {type(heu)}'
     if config is None:
```

## secretflow/device/kernels/spu.py

```diff
@@ -8,47 +8,48 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, List, Union
+from typing import Callable, Dict, List, Union
 
 import secretflow.distributed as sfd
 from secretflow.device import (
     HEU,
     PYU,
     SPU,
     SPUIO,
     Device,
     DeviceType,
     HEUObject,
     PYUObject,
     SPUObject,
     register,
 )
+from secretflow.utils.progress import ProgressData
 from secretflow.device.device.base import register_to
 from secretflow.device.device.heu import HEUMoveConfig
 
 
 @register_to(DeviceType.SPU, DeviceType.PYU)
 def spu_to_pyu(self: SPUObject, pyu: Device, config: HEUMoveConfig = None):
     assert isinstance(pyu, PYU), f'Expect a PYU but got {type(pyu)}.'
     if config is None:
         config = HEUMoveConfig()
 
-    def reveal(conf, world_size, refs, meta):
+    def reveal(conf, world_size, io_info, share_chunks, meta):
         io = SPUIO(conf, world_size)
-        return io.reconstruct(refs, meta)
+        return io.reconstruct(share_chunks, io_info, meta)
 
     return pyu(reveal)(
         self.device.conf,
         self.device.world_size,
-        self.device.outfeed_shares(self.shares_name),
+        *self.device.outfeed_shares(self.shares_name),
         self.meta,
     )
 
 
 @register_to(DeviceType.SPU, DeviceType.SPU)
 def spu_to_spu(self: SPUObject, spu: SPU):
     assert isinstance(spu, SPU), f'Expect an SPU but got {type(spu)}.'
@@ -60,16 +61,16 @@
     assert (
         spu.conf.protocol == self.device.conf.protocol
         and spu.conf.field == self.device.conf.field
         and spu.conf.fxp_fraction_bits == self.device.conf.fxp_fraction_bits
         and spu.world_size == self.device.world_size
     )
 
-    shares = self.device.outfeed_shares(self.shares_name)
-    shares_name = spu.infeed_shares(shares)
+    io_info, shares_chunk = self.device.outfeed_shares(self.shares_name)
+    shares_name = spu.infeed_shares(io_info, shares_chunk)
 
     # TODO: do we need reshare shares.
     return SPUObject(spu, self.meta, shares_name)
 
 
 @register_to(DeviceType.SPU, DeviceType.HEU)
 def spu_to_heu(self: SPUObject, heu: Device, config: HEUMoveConfig = None):
@@ -89,19 +90,26 @@
 
     heu_parties = list(heu.evaluator_names()) + [heu.sk_keeper_name()]
     assert set(self.device.actors.keys()).issubset(
         heu_parties
     ), f'Mismatch SPU and HEU parties, spu: {list(self.device.actors.keys())}, heu:{heu_parties}'
 
     # TODO(@xibin.wxb): support pytree
+    io_info, shares_chunk = self.device.outfeed_shares(self.shares_name)
+    assert (
+        len(shares_chunk) % len(self.device.actors) == 0
+    ), f"{len(shares_chunk)} % {len(self.device.actors)}"
+    chunks_count_pre_party = int(len(shares_chunk) / len(self.device.actors))
+    chunks_pre_party = [
+        shares_chunk[i * chunks_count_pre_party : (i + 1) * chunks_count_pre_party]
+        for i in range(len(self.device.actors))
+    ]
     shards = {
-        p: actor.a2h.remote(ref, heu.cleartext_type, heu.schema)
-        for (p, actor), ref in zip(
-            self.device.actors.items(), self.device.outfeed_shares(self.shares_name)
-        )
+        p: actor.a2h.remote(io_info, heu.cleartext_type, heu.schema, *chunks)
+        for (p, actor), chunks in zip(self.device.actors.items(), chunks_pre_party)
     }
     shards = [
         heu.get_participant(p).encrypt.remote(shard, config.heu_audit_log)
         if p != config.heu_dest_party
         else shard
         for p, shard in shards.items()
     ]
@@ -122,14 +130,16 @@
     broadcast_result=True,
     bucket_size=1 << 20,
     curve_type="CURVE_25519",
     preprocess_path=None,
     ecdh_secret_key_path=None,
     dppsi_bob_sub_sampling=0.9,
     dppsi_epsilon=3,
+    progress_callbacks: Callable[[str, ProgressData], None] = None,
+    callbacks_interval_ms: int = 5 * 1000,
 ) -> List[PYUObject]:
     assert isinstance(device, SPU), f'device must be SPU device'
     assert isinstance(
         key, (str, List, Dict)
     ), f'invalid key, must be str of list of str or dict of str list'
     assert len(set([df.device for df in dfs])) == len(
         dfs
@@ -161,14 +171,16 @@
                     broadcast_result,
                     bucket_size,
                     curve_type,
                     preprocess_path,
                     ecdh_secret_key_path,
                     dppsi_bob_sub_sampling,
                     dppsi_epsilon,
+                    progress_callbacks,
+                    callbacks_interval_ms,
                 ),
             )
         )
 
     return res
 
 
@@ -185,14 +197,16 @@
     broadcast_result=True,
     bucket_size=1 << 20,
     curve_type="CURVE_25519",
     preprocess_path: Union[str, Dict[Device, str]] = None,
     ecdh_secret_key_path=None,
     dppsi_bob_sub_sampling=0.9,
     dppsi_epsilon=3,
+    progress_callbacks: Callable[[str, ProgressData], None] = None,
+    callbacks_interval_ms: int = 5 * 1000,
 ):
     assert isinstance(device, SPU), f'device must be SPU device'
     assert isinstance(
         key, (str, List, Dict)
     ), f'invalid key, must be str of list of str or dict of list str'
     assert isinstance(input_path, (str, Dict)), f'input_path must be str or dict of str'
     assert isinstance(
@@ -261,14 +275,16 @@
                     broadcast_result,
                     bucket_size,
                     curve_type,
                     p,
                     ecdh_secret_key_path,
                     dppsi_bob_sub_sampling,
                     dppsi_epsilon,
+                    progress_callbacks,
+                    callbacks_interval_ms,
                 )
             )
     else:
         for dev, ipath in input_path.items():
             opath = output_path[dev]
             actor = device.actors[dev.party]
             k = key[dev] if isinstance(key, Dict) else key
@@ -289,14 +305,16 @@
                     broadcast_result,
                     bucket_size,
                     curve_type,
                     p,
                     ecdh_secret_key_path,
                     dppsi_bob_sub_sampling,
                     dppsi_epsilon,
+                    progress_callbacks,
+                    callbacks_interval_ms,
                 )
             )
 
     # wait for all tasks done
     return sfd.get(res)
 
 
@@ -306,14 +324,16 @@
     key: Union[str, List[str], Dict[Device, List[str]]],
     dfs: List['PYUObject'],
     receiver: str,
     join_party: str,
     protocol='KKRT_PSI_2PC',
     bucket_size=1 << 20,
     curve_type="CURVE_25519",
+    progress_callbacks: Callable[[str, ProgressData], None] = None,
+    callbacks_interval_ms: int = 5 * 1000,
 ) -> List[PYUObject]:
     assert isinstance(device, SPU), f'device must be SPU device'
     assert isinstance(
         key, (str, List, Dict)
     ), f'invalid key, must be str of list of str or dict of str list'
     assert len(set([df.device for df in dfs])) == len(
         dfs
@@ -339,14 +359,16 @@
                     k,
                     df.data,
                     receiver,
                     join_party,
                     protocol,
                     bucket_size,
                     curve_type,
+                    progress_callbacks,
+                    callbacks_interval_ms,
                 ),
             )
         )
 
     return res
 
 
@@ -357,14 +379,16 @@
     input_path: Union[str, Dict[Device, str]],
     output_path: Union[str, Dict[Device, str]],
     receiver: str,
     join_party: str,
     protocol='KKRT_PSI_2PC',
     bucket_size=1 << 20,
     curve_type="CURVE_25519",
+    progress_callbacks: Callable[[str, ProgressData], None] = None,
+    callbacks_interval_ms: int = 5 * 1000,
 ):
     assert isinstance(device, SPU), f'device must be SPU device'
     assert isinstance(
         key, (str, List, Dict)
     ), f'invalid key, must be str of list of str or dict of list str'
     assert isinstance(input_path, (str, Dict)), f'input_path must be str or dict of str'
     assert isinstance(
@@ -412,14 +436,16 @@
                     input_path,
                     output_path,
                     receiver,
                     join_party,
                     protocol,
                     bucket_size,
                     curve_type,
+                    progress_callbacks,
+                    callbacks_interval_ms,
                 )
             )
     else:
         for dev, ipath in input_path.items():
             opath = output_path[dev]
             actor = device.actors[dev.party]
             k = key[dev] if isinstance(key, Dict) else key
@@ -429,14 +455,16 @@
                     ipath,
                     opath,
                     receiver,
                     join_party,
                     protocol,
                     bucket_size,
                     curve_type,
+                    progress_callbacks,
+                    callbacks_interval_ms,
                 )
             )
 
     # wait for all tasks done
     return sfd.get(res)
```

## secretflow/ml/boost/sgb_v/__init__.py

```diff
@@ -9,11 +9,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from .model import SgbModel
-from .sgb import Sgb
-from .factory import SGBFactory
+from .factory import SGBFactory as Sgb
+from .core.params import get_classic_lightGBM_params, get_classic_XGB_params, SGBParams
 
-__all__ = ['SgbModel', 'Sgb', 'SGBFactory']
+__all__ = [
+    'SgbModel',
+    'Sgb',
+    'get_classic_lightGBM_params',
+    'get_classic_XGB_params',
+    'SGBParams',
+]
```

## secretflow/ml/boost/sgb_v/model.py

```diff
@@ -20,15 +20,15 @@
 
 from secretflow.data import FedNdarray, PartitionWay
 from secretflow.data.vertical import VDataFrame
 from secretflow.device import PYU, PYUObject, reveal, wait
 
 from .core.distributed_tree.distributed_tree import DistributedTree
 from .core.distributed_tree.distributed_tree import from_dict as dt_from_dict
-from .core.preprocessing.params import RegType
+from .core.params import RegType
 from secretflow.ml.boost.core.data_preprocess import prepare_dataset
 from .core.pure_numpy_ops.pred import sigmoid
 
 
 common_path_postfix = "/common.json"
 leaf_weight_postfix = "/leaf_weight.json"
 split_tree_postfix = "/split_tree.json"
```

## secretflow/ml/boost/sgb_v/factory/factory.py

```diff
@@ -8,24 +8,30 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
+import logging
 from dataclasses import dataclass
 from typing import Union
 
 from heu import phe
 
 from secretflow.data import FedNdarray
 from secretflow.data.vertical import VDataFrame
 from secretflow.device import HEU
-from secretflow.ml.boost.sgb_v.factory.params import TreeGrowingMethod, default_params
+from secretflow.ml.boost.sgb_v.core.params import (
+    TreeGrowingMethod,
+    default_params,
+    get_unused_params,
+    type_and_range_check,
+)
+from secretflow.ml.boost.sgb_v.factory.components.logging import logging_params_names
 
 from ..model import SgbModel
 from .booster import GlobalOrdermapBooster
 from .components import LeafWiseTreeTrainer, LevelWiseTreeTrainer
 
 
 @dataclass
@@ -39,26 +45,31 @@
 
     Attributes:
         params_dict (dict): A dict contain params for the factory, booster and its components.
         factory_params (SGBFactoryParams): validated params for the factory.
         heu: the device for HE computations. must be set before training.
     """
 
-    def __init__(self):
+    def __init__(self, heu=None):
         # params_dict is either default or user set, should not change by program
-        self.params_dict = {'tree_growing_method': "level"}
+        self.params_dict = {'tree_growing_method': default_params.tree_growing_method}
         self.factory_params = SGBFactoryParams()
-        self.heu = None
+        self.heu = heu
 
     def set_params(self, params: dict):
         """Set params by a dictionary."""
-        tree_grow_method = params.get('tree_growing_method', "level")
-        assert (
-            tree_grow_method == "level" or tree_grow_method == "leaf"
-        ), f"tree growing method must one one of 'level' or 'leaf', got {tree_grow_method}"
+        type_and_range_check(params)
+        unused_params = get_unused_params(params)
+        unused_params -= logging_params_names
+        if len(unused_params) > 0:
+            logging.warning(f"The following params are not effective: {unused_params}")
+
+        tree_grow_method = params.get(
+            'tree_growing_method', default_params.tree_growing_method
+        )
         self.params_dict = params
         self.factory_params.tree_growing_method = TreeGrowingMethod(tree_grow_method)
 
     def set_heu(self, heu: HEU):
         self.heu = heu
 
     def _produce(self) -> GlobalOrdermapBooster:
@@ -122,12 +133,12 @@
     ) -> SgbModel:
         booster = self._produce()
         return booster.fit(dataset, label)
 
     def train(
         self,
         params: dict,
-        dataset: Union[FedNdarray, VDataFrame],
+        dtrain: Union[FedNdarray, VDataFrame],
         label: Union[FedNdarray, VDataFrame],
     ) -> SgbModel:
         self.set_params(params)
-        return self.fit(dataset, label)
+        return self.fit(dtrain, label)
```

## secretflow/ml/boost/sgb_v/factory/booster/global_ordermap_booster.py

```diff
@@ -12,21 +12,22 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import copy
 import logging
 import time
 from dataclasses import dataclass
-from typing import Union
+from typing import List, Union
 
 import numpy as np
 
 from secretflow.data import FedNdarray
 from secretflow.data.vertical import VDataFrame
 from secretflow.device import HEU, wait
+from secretflow.ml.boost.sgb_v.core.params import default_params
 
 from ...model import SgbModel
 from ..components import DataPreprocessor, ModelBuilder, OrderMapManager, TreeTrainer
 from ..components.component import Composite, Devices, print_params
 from ..sgb_actor import SGBActor
 
 
@@ -47,16 +48,18 @@
     first_tree_with_label_holder_feature: bool, default=False
                 Whether to train the first tree with label holder's own features.
                 Can increase training speed and label security.
                 The training loss may increase.
                 If label holder has no feature, set this to False.
     """
 
-    num_boost_round: int = 10
-    first_tree_with_label_holder_feature: bool = False
+    num_boost_round: int = default_params.num_boost_round
+    first_tree_with_label_holder_feature: bool = (
+        default_params.first_tree_with_label_holder_feature
+    )
 
 
 class GlobalOrdermapBooster(Composite):
     """
     This class provides both classification and regression tree boosting (also known as GBDT, GBM)
     for vertical split dataset setting by using level wise boost.
     """
@@ -65,19 +68,19 @@
         self.components = GlobalOrdermapBoosterComponents()
         self.params = GlobalOrdermapBoosterParams()
         self.user_config = {}
         self.heu = heu
         self.tree_trainer = tree_trainer
 
     def _set_booster_params(self, params: dict):
-        trees = int(params.get('num_boost_round', 10))
-        assert 1 <= trees <= 1024, f"num_boost_round should in [1, 1024], got {trees}"
+        trees = params.get('num_boost_round', default_params.num_boost_round)
         self.params.num_boost_round = trees
-        self.params.first_tree_with_label_holder_feature = bool(
-            params.get('first_tree_with_label_holder_feature', False)
+        self.params.first_tree_with_label_holder_feature = params.get(
+            'first_tree_with_label_holder_feature',
+            default_params.first_tree_with_label_holder_feature,
         )
 
     def _get_booster_params(self, params: dict):
         params['num_boost_round'] = self.params.num_boost_round
         params[
             'first_tree_with_label_holder_feature'
         ] = self.params.first_tree_with_label_holder_feature
@@ -99,44 +102,54 @@
         self._get_booster_params(params)
         return params
 
     def set_devices(self, devices: Devices):
         super().set_devices(devices)
         self.tree_trainer.set_devices(devices)
 
-    def set_actors(self, actors: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
         super().set_actors(actors)
         self.tree_trainer.set_actors(actors)
 
+    def del_actors(self):
+        super().del_actors()
+        self.tree_trainer.del_actors()
+
     def fit(
         self,
         dataset: Union[FedNdarray, VDataFrame],
         label: Union[FedNdarray, VDataFrame],
     ) -> SgbModel:
         x, x_shape, y, _ = self.components.preprocessor.validate(dataset, label)
 
         # set devices
         devices = Devices(y.device, [*x.partitions.keys()], self.heu)
         self.set_devices(devices)
 
         # set actors
         actors = [SGBActor(device=device) for device in devices.workers]
+        logging.debug("actors are created.")
         self.set_actors(actors)
+        logging.debug("actors are set.")
 
         pred = self.components.model_builder.init_pred(x_shape[0])
+        logging.debug("pred initialized.")
         self.components.order_map_manager.build_order_map(x)
+        logging.debug("ordermap built.")
         self.components.model_builder.init_model()
+        logging.debug("model initialized.")
 
         for tree_index in range(self.params.num_boost_round):
             start = time.perf_counter()
             if self.params.first_tree_with_label_holder_feature and tree_index == 0:
                 # we are sure the config is small, so ok to copy
                 config = copy.deepcopy(self.user_config)
                 config['label_holder_feature_only'] = True
                 self.tree_trainer.set_params(config)
+                logging.info("training the first tree with label holder only.")
             tree = self.tree_trainer.train_tree(
                 tree_index, self.components.order_map_manager, y, pred, x_shape
             )
             if tree is None:
                 logging.info(
                     f"early_stopped, current tree num: {self.components.model_builder.get_tree_num()}"
                 )
@@ -154,9 +167,9 @@
                 wait([pred])
             else:
                 wait(tree)
 
             logging.info(
                 f"epoch {cur_tree_num - 1} time {time.perf_counter() - start}s"
             )
-
+        self.del_actors()
         return self.components.model_builder.finish()
```

## secretflow/ml/boost/sgb_v/factory/components/component.py

```diff
@@ -42,15 +42,19 @@
         pass
 
     @abc.abstractmethod
     def set_devices(self, devices: Devices):
         pass
 
     @abc.abstractmethod
-    def set_actors(self, actors: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
+        pass
+
+    @abc.abstractmethod
+    def del_actors(self):
         pass
 
 
 class Composite(Component):
     def __init__(self) -> None:
         self.components = None
 
@@ -67,15 +71,19 @@
         for field in fields(self.components):
             getattr(self.components, field.name).set_params(params)
 
     def set_devices(self, devices: Devices):
         for field in fields(self.components):
             getattr(self.components, field.name).set_devices(devices)
 
-    def set_actors(self, actors: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
         for field in fields(self.components):
             getattr(self.components, field.name).set_actors(actors)
 
+    def del_actors(self):
+        for field in fields(self.components):
+            getattr(self.components, field.name).del_actors()
+
 
 def print_params(params):
     for field in fields(params):
         print(field.name, getattr(params, field.name))
```

## secretflow/ml/boost/sgb_v/factory/components/logging.py

```diff
@@ -17,14 +17,15 @@
 from dataclasses import dataclass
 from typing import Any
 
 from secretflow.device import wait
 
 
 # logging params will be set at each component
+# these parameters are helpful for developers
 @dataclass
 class LoggingParams:
     """
     'verbose': bool. If write more logs.
 
         default: False.
 
@@ -44,14 +45,17 @@
         default: False.
     """
 
     verbose: bool = False
     wait_execution: bool = False
 
 
+logging_params_names = {'verbose', 'wait_execution'}
+
+
 class LoggingTools:
     @staticmethod
     def logging_params_from_dict(params: dict) -> LoggingParams:
         verbose = bool(params.get('verbose', False))
         wait_execution = bool(params.get('wait_execution', False))
         return LoggingParams(verbose, wait_execution)
```

## secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/bucket_sum_calculator.py

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Dict, List, Tuple
 
 from secretflow.data import FedNdarray
 from secretflow.device import PYU, HEUObject, PYUObject
-from secretflow.ml.boost.sgb_v.factory.params import default_params
+from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
 from ....core.pure_numpy_ops.bucket_sum import batch_select_sum, regroup_bucket_sums
 from ....core.pure_numpy_ops.grad import split_GH
 from ..cache.level_wise_cache import LevelWiseCache
 from ..component import Composite, Devices, print_params
 from ..gradient_encryptor import GradientEncryptor
 from ..logging import LoggingParams, LoggingTools
@@ -31,15 +31,15 @@
 @dataclass
 class BucketSumCalculatorParams:
     """
     'label_holder_feature_only': bool. if true, non-label holder will not do bucket sum
         default: False
     """
 
-    label_holder_feature_only: bool = default_params.label_holder_feature_only
+    label_holder_feature_only: bool = False
 
 
 @dataclass
 class BucketSumCalculatorComponents:
     level_wise_cache: LevelWiseCache = LevelWiseCache()
 
 
@@ -65,17 +65,20 @@
 
     def set_devices(self, devices: Devices):
         super().set_devices(devices)
         self.label_holder = devices.label_holder
         self.workers = devices.workers
         self.party_num = len(self.workers)
 
-    def set_actors(self, actors):
+    def set_actors(self, actors: List[SGBActor]):
         super().set_actors(actors)
 
+    def del_actors(self):
+        super().del_actors()
+
     @LoggingTools.enable_logging
     def calculate_bucket_sum_level_wise(
         self,
         shuffler: Shuffler,
         encrypted_gh_dict: Dict[PYU, HEUObject],
         children_split_node_selects: List[PYUObject],
         is_lefts: List[bool],
```

## secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/leaf_wise_bucket_sum_calculator.py

```diff
@@ -13,15 +13,14 @@
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Dict, List, Tuple
 
 from secretflow.data import FedNdarray
 from secretflow.device import PYU, HEUObject, PYUObject
-from secretflow.ml.boost.sgb_v.factory.params import default_params
 from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
 from ....core.pure_numpy_ops.bucket_sum import batch_select_sum, regroup_bucket_sums
 from ....core.pure_numpy_ops.grad import split_GH
 from ..cache.node_wise_bucket_sum_cache import NodeWiseCache
 from ..component import Composite, Devices, print_params
 from ..gradient_encryptor import GradientEncryptor
@@ -32,15 +31,15 @@
 @dataclass
 class LeafWiseBucketSumCalculatorParams:
     """
     'label_holder_feature_only': bool. if true, non-label holder will not do bucket sum
         default: False
     """
 
-    label_holder_feature_only: bool = default_params.label_holder_feature_only
+    label_holder_feature_only: bool = False
 
 
 @dataclass
 class LeafWiseBucketSumCalculatorComponents:
     node_wise_cache: NodeWiseCache = NodeWiseCache()
 
 
@@ -52,31 +51,34 @@
 
     def show_params(self):
         print_params(self.logging_params)
         print_params(self.params)
 
     def set_params(self, params: dict):
         self.logging_params = LoggingTools.logging_params_from_dict(params)
-        self.params.label_holder_feature_only = bool(
-            params.get('label_holder_feature_only', False)
+        self.params.label_holder_feature_only = params.get(
+            'label_holder_feature_only', False
         )
 
     def get_params(self, params: dict):
         LoggingTools.logging_params_write_dict(params, self.logging_params)
         params['label_holder_feature_only'] = self.params.label_holder_feature_only
 
     def set_devices(self, devices: Devices):
         super().set_devices(devices)
         self.label_holder = devices.label_holder
         self.workers = devices.workers
         self.party_num = len(self.workers)
 
-    def set_actors(self, actors: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
         return super().set_actors(actors)
 
+    def del_actors(self):
+        return super().del_actors()
+
     @LoggingTools.enable_logging
     def calculate_bucket_sum(
         self,
         shuffler: Shuffler,
         encrypted_gh_dict: Dict[PYU, HEUObject],
         selected_children_node_indices: List[int],
         all_children_node_indices: List[int],
```

## secretflow/ml/boost/sgb_v/factory/components/cache/level_wise_cache.py

```diff
@@ -48,14 +48,17 @@
             if actor.device == self.label_holder:
                 self.worker_level_caches[self.label_holder] = actor
                 break
         self.worker_level_caches[self.label_holder].register_class(
             'LevelCache', LevelCache
         )
 
+    def del_actors(self):
+        del self.worker_level_caches
+
     def reset_level_caches(self):
         for device in self.worker_level_caches.keys():
             if device != self.label_holder:
                 self.worker_level_caches[device].reset_level_nodes_GH()
             else:
                 self.worker_level_caches[device].invoke_class_method(
                     'LevelCache', 'reset_level_nodes_GH'
```

## secretflow/ml/boost/sgb_v/factory/components/cache/node_wise_bucket_sum_cache.py

```diff
@@ -50,14 +50,17 @@
         for actor in actors:
             if actor.device == self.label_holder:
                 self.worker_caches[self.label_holder] = actor
                 break
 
         self.worker_caches[self.label_holder].register_class('NodeCache', NodeCache)
 
+    def del_actors(self):
+        del self.worker_caches
+
     def reset(self):
         for device in self.worker_caches:
             if device != self.label_holder:
                 self.worker_caches[device].reset()
             else:
                 self.worker_caches[device].invoke_class_method('NodeCache', 'reset')
```

## secretflow/ml/boost/sgb_v/factory/components/data_preprocessor/data_preprocessor.py

```diff
@@ -36,11 +36,14 @@
 
     def set_devices(self, _):
         return
 
     def set_actors(self, _):
         return
 
+    def del_actors(self):
+        return
+
     def validate(
         self, dataset, label
     ) -> Tuple[FedNdarray, Tuple[int, int], PYUObject, Tuple[int, int]]:
         return validate(dataset, label)
```

## secretflow/ml/boost/sgb_v/factory/components/gradient_encryptor/gradient_encryptor.py

```diff
@@ -16,16 +16,15 @@
 from typing import Dict, Union
 
 import numpy as np
 from heu import phe
 
 from secretflow.device import PYU, HEUObject, PYUObject
 from secretflow.device.device.heu import HEUMoveConfig
-from secretflow.ml.boost.sgb_v.factory.params import default_params
-from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
+from secretflow.ml.boost.sgb_v.core.params import default_params
 
 from ..component import Component, Devices, print_params
 from ..logging import LoggingParams, LoggingTools
 
 
 @dataclass
 class GradientEncryptorParams:
@@ -45,15 +44,15 @@
         default: False
         if turned on, gh won't be sent to workers in anyway.
     'audit_paths': dict. {device : path to save log for audit}
     """
 
     fixed_point_parameter: int = default_params.fixed_point_parameter
     batch_encoding_enabled: bool = default_params.batch_encoding_enabled
-    label_holder_feature_only: bool = default_params.label_holder_feature_only
+    label_holder_feature_only: bool = False
     audit_paths: dict = field(default_factory=dict)
 
 
 def define_encoder(params: GradientEncryptorParams):
     fxp_scale = np.power(2, params.fixed_point_parameter)
     if params.batch_encoding_enabled:
         return phe.BatchFloatEncoderParams(scale=fxp_scale)
@@ -78,36 +77,37 @@
         self.workers = devices.workers
         self.heu = devices.heu
         label_holder_party_name = self.label_holder.party
         assert (
             label_holder_party_name == self.heu.sk_keeper_name()
         ), f"HEU sk keeper party {self.heu.sk_keeper_name()}, mismatch with label_holder device's party {label_holder_party_name}"
 
-    def set_actors(self, _: SGBActor):
+    def set_actors(self, _):
+        return
+
+    def del_actors(self):
         return
 
     def get_params(self, params: dict):
         params['fixed_point_parameter'] = self.params.fixed_point_parameter
         params['batch_encoding_enabled'] = self.params.batch_encoding_enabled
         params['audit_paths'] = self.params.audit_paths
         params['label_holder_feature_only'] = self.params.label_holder_feature_only
         LoggingTools.logging_params_write_dict(params, self.logging_params)
 
     def set_params(self, params: dict):
         # validation
-        fxp_r = params.get('fixed_point_parameter', 20)
-        assert (
-            fxp_r >= 1 and fxp_r <= 100
-        ), f"fixed_point_parameter should in [1, 100], got {fxp_r}"
-
-        enable_batch_encoding = bool(params.get('batch_encoding_enabled', True))
+        fxp_r = params.get(
+            'fixed_point_parameter', default_params.fixed_point_parameter
+        )
+        enable_batch_encoding = params.get(
+            'batch_encoding_enabled', default_params.batch_encoding_enabled
+        )
         audit_paths = params.get('audit_paths', {})
-        label_holder_feature_only = bool(params.get('label_holder_feature_only', False))
-        assert isinstance(audit_paths, dict), " audit paths must be a dict"
-
+        label_holder_feature_only = params.get('label_holder_feature_only', False)
         # set params
         self.params.label_holder_feature_only = label_holder_feature_only
         self.params.fixed_point_parameter = fxp_r
         self.params.batch_encoding_enabled = enable_batch_encoding
         self.params.audit_paths = audit_paths
 
         # calculate attributes
```

## secretflow/ml/boost/sgb_v/factory/components/leaf_manager/leaf_manager.py

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from dataclasses import dataclass
 from typing import List
 
 import numpy as np
 
-from secretflow.ml.boost.sgb_v.factory.params import default_params
+from secretflow.ml.boost.sgb_v.core.params import default_params
 from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
 from ..component import Component, Devices, print_params
 from .leaf_actor import LeafActor
 
 
 @dataclass
@@ -42,39 +42,36 @@
     def __init__(self) -> None:
         self.params = LeafManagerParams()
 
     def show_params(self):
         print_params(self.params)
 
     def set_params(self, params: dict):
-        reg_lambda = float(params.get('reg_lambda', 0.1))
-        assert (
-            reg_lambda >= 0 and reg_lambda <= 10000
-        ), f"reg_lambda should in [0, 10000], got {reg_lambda}"
-
-        lr = float(params.get('learning_rate', 0.3))
-        assert lr > 0 and lr <= 1, f"learning_rate should in (0, 1], got {lr}"
-
+        reg_lambda = params.get('reg_lambda', default_params.reg_lambda)
+        lr = params.get('learning_rate', default_params.learning_rate)
         self.params.reg_lambda = reg_lambda
         self.params.learning_rate = lr
 
     def get_params(self, params: dict):
         params['reg_lambda'] = self.params.reg_lambda
         params['learning_rate'] = self.params.learning_rate
 
     def set_devices(self, devices: Devices):
         self.label_holder = devices.label_holder
 
-    def set_actors(self, actors: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
         for actor in actors:
             if actor.device == self.label_holder:
                 self.leaf_actor = actor
                 break
         self.leaf_actor.register_class('LeafActor', LeafActor)
 
+    def del_actors(self):
+        del self.leaf_actor
+
     def clear_leaves(self):
         self.leaf_actor.invoke_class_method('LeafActor', 'clear_leaves')
 
     def extend_leaves(
         self, pruned_node_selects: List[np.ndarray], pruned_node_indices: List[int]
     ):
         self.leaf_actor.invoke_class_method(
```

## secretflow/ml/boost/sgb_v/factory/components/loss_computer/loss_computer.py

```diff
@@ -9,32 +9,26 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Tuple, Union
+from typing import Tuple, Union, List
 
 import numpy as np
 
-from secretflow.device import PYUObject, reveal
-from secretflow.ml.boost.sgb_v.factory.params import default_params
+from secretflow.device import PYUObject
+from secretflow.ml.boost.sgb_v.core.params import default_params
 from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
-from ....core.preprocessing.params import RegType
-from ....core.pure_numpy_ops.grad import (
-    compute_gh_linear,
-    compute_gh_logistic,
-    compute_relative_scaling_factor,
-    compute_sum_abs,
-    scale,
-)
+from ....core.params import RegType
 from ..component import Component, Devices, print_params
 from ..logging import LoggingParams, LoggingTools
+from .loss_computer_actor import LossComputerActor
 
 
 @dataclass
 class LossComputerParams:
     """
     'objective': Specify the learning objective.
         default: 'logistic'
@@ -77,41 +71,32 @@
 
     def show_params(self):
         print_params(self.params)
         print_params(self.logging_params)
 
     def set_params(self, params: dict):
         obj = params.get('objective', 'logistic')
-        assert obj in [
-            e.value for e in RegType
-        ], f"objective should in {[e.value for e in RegType]}, got {obj}"
         obj = RegType(obj)
 
-        self.params.enable_quantization = bool(
-            params.get('enable_quantization', default_params.enable_quantization)
+        self.params.enable_quantization = params.get(
+            'enable_quantization', default_params.enable_quantization
         )
-        quantization_scale = float(
-            params.get('quantization_scale', default_params.quantization_scale)
+        quantization_scale = params.get(
+            'quantization_scale', default_params.quantization_scale
+        )
+
+        early_stop_criterion_g_abs_sum = params.get(
+            'early_stop_criterion_g_abs_sum',
+            default_params.early_stop_criterion_g_abs_sum,
+        )
+
+        early_stop_criterion_g_abs_sum_change_ratio = params.get(
+            'early_stop_criterion_g_abs_sum_change_ratio',
+            default_params.early_stop_criterion_g_abs_sum_change_ratio,
         )
-        early_stop_criterion_g_abs_sum = float(
-            params.get(
-                'early_stop_criterion_g_abs_sum',
-                default_params.early_stop_criterion_g_abs_sum,
-            )
-        )
-        early_stop_criterion_g_abs_sum_change_ratio = float(
-            params.get(
-                'early_stop_criterion_g_abs_sum_change_ratio',
-                default_params.early_stop_criterion_g_abs_sum_change_ratio,
-            )
-        )
-        assert (
-            early_stop_criterion_g_abs_sum_change_ratio >= 0
-            and early_stop_criterion_g_abs_sum_change_ratio <= 1
-        ), "ratio must be in range [0,1]"
 
         self.params.quantization_scale = quantization_scale
         self.params.early_stop_criterion_g_abs_sum = early_stop_criterion_g_abs_sum
         self.params.early_stop_criterion_g_abs_sum_change_ratio = (
             early_stop_criterion_g_abs_sum_change_ratio
         )
         self.params.objective = obj
@@ -129,107 +114,64 @@
             'early_stop_criterion_g_abs_sum_change_ratio'
         ] = self.params.early_stop_criterion_g_abs_sum_change_ratio
         LoggingTools.logging_params_write_dict(params, self.logging_params)
 
     def set_devices(self, devices: Devices):
         self.label_holder = devices.label_holder
 
-    def set_actors(self, _: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
+        for actor in actors:
+            if actor.device == self.label_holder:
+                self.actor = actor
+                break
+        self.actor.register_class('LossComputerActor', LossComputerActor)
+        return
+
+    def del_actors(self):
+        del self.actor
         return
 
     @LoggingTools.enable_logging
     def compute_gh(
         self, y: Union[PYUObject, np.ndarray], pred: Union[PYUObject, np.ndarray]
     ) -> Tuple[PYUObject, PYUObject]:
         obj = self.params.objective
-        if obj == RegType.Linear:
-            g, h = self.label_holder(compute_gh_linear, num_returns=2)(y, pred)
-        elif obj == RegType.Logistic:
-            g, h = self.label_holder(compute_gh_logistic, num_returns=2)(y, pred)
-        else:
-            raise f"unknown objective {obj}"
-        return g, h
+        return self.actor.invoke_class_method_two_ret(
+            'LossComputerActor', 'compute_gh', y, pred, obj
+        )
 
     def compute_abs_sums(self, g: PYUObject, h: PYUObject):
-        abs_g_sum = self.label_holder(compute_sum_abs)(g)
-        abs_h_sum = self.label_holder(compute_sum_abs)(h)
-
-        self.last_abs_g_cache = self.abs_g_cache
-        self.abs_g_cache = abs_g_sum
-        self.abs_h_cache = abs_h_sum
-
-        return
+        return self.actor.invoke_class_method(
+            'LossComputerActor', 'compute_abs_sums', g, h
+        )
 
     def compute_scales(self):
         scaling = self.params.quantization_scale
-        abs_g_sum = self.abs_g_cache
-        abs_h_sum = self.abs_h_cache
-        self.g_scale = self.label_holder(compute_relative_scaling_factor)(
-            abs_g_sum, scaling
-        )
-        self.h_scale = self.label_holder(compute_relative_scaling_factor)(
-            abs_h_sum, scaling
+        return self.actor.invoke_class_method(
+            'LossComputerActor', 'compute_scales', scaling
         )
-        return
 
-    def check_abs_g_sum_early_stop(self) -> Union[bool, PYUObject]:
-        threshold = self.params.early_stop_criterion_g_abs_sum
-        # early stopping happened, and this is known by all parties
-        abs_sum = self.abs_g_cache
-        if abs_sum is None:
-            return False
-        return self.label_holder(lambda abs_sum, threshold: abs_sum <= threshold)(
-            abs_sum, threshold
-        )
-
-    def check_abs_g_sum_change_ratio_early_stop(self) -> Union[bool, PYUObject]:
-        threshold = self.params.early_stop_criterion_g_abs_sum_change_ratio
-        old = self.last_abs_g_cache
-        current_abs_sum = self.abs_g_cache
-        if old is None or current_abs_sum is None:
-            return False
-        return self.label_holder(
-            lambda old, abs_sum, threshold: delta_ratio(old, abs_sum) <= threshold
-        )(old, current_abs_sum, threshold)
-
-    def check_early_stop(self) -> bool:
-        g_sum_es = self.check_abs_g_sum_early_stop()
-        g_sum_delta_es = self.check_abs_g_sum_change_ratio_early_stop()
-        return reveal(self.label_holder(lambda a, b: a or b)(g_sum_es, g_sum_delta_es))
+    def check_early_stop(self) -> PYUObject:
+        abs_sum_threshold = self.params.early_stop_criterion_g_abs_sum
+        abs_sum_change_ratio_threshold = (
+            self.params.early_stop_criterion_g_abs_sum_change_ratio
+        )
+        return self.actor.invoke_class_method(
+            'LossComputerActor',
+            'check_early_stop',
+            abs_sum_threshold,
+            abs_sum_change_ratio_threshold,
+        )
 
     def scale_gh(self, g: PYUObject, h: PYUObject) -> Tuple[PYUObject, PYUObject]:
-        if self.params.enable_quantization:
-            g_scale = self.g_scale
-            h_scale = self.h_scale
-            return self.label_holder(
-                lambda g, h, g_scale, h_scale: (
-                    scale(g, g_scale),
-                    scale(h, h_scale),
-                ),
-                num_returns=2,
-            )(g, h, g_scale, h_scale)
-        else:
-            return g, h
+        enable_quantization = self.params.enable_quantization
+        return self.actor.invoke_class_method_two_ret(
+            'LossComputerActor', 'scale_gh', g, h, enable_quantization
+        )
 
     def reverse_scale_gh(
         self, g: PYUObject, h: PYUObject
     ) -> Tuple[PYUObject, PYUObject]:
-        if self.params.enable_quantization:
-            # scale 0 means abs g sum is finitely large, cannot happen.
-            g_scale = self.g_scale
-            h_scale = self.h_scale
-            return self.label_holder(
-                lambda g, h, g_scale, h_scale: (
-                    scale(g, 1 / g_scale),
-                    scale(h, 1 / h_scale),
-                ),
-                num_returns=2,
-            )(g, h, g_scale, h_scale)
-        else:
-            return g, h
-
-
-def delta_ratio(old, new):
-    if old > 0:
-        return abs(old - new / old)
-    else:
-        return 0
+        enable_quantization = self.params.enable_quantization
+        return self.actor.invoke_class_method_two_ret(
+            'LossComputerActor', 'reverse_scale_gh', g, h, enable_quantization
+        )
```

## secretflow/ml/boost/sgb_v/factory/components/model_builder/model_builder.py

```diff
@@ -11,19 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
 
 from secretflow.device import PYUObject
-from secretflow.ml.boost.sgb_v.factory.params import default_params
-from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
+from secretflow.ml.boost.sgb_v.core.params import default_params
 
 from ....core.distributed_tree.distributed_tree import DistributedTree
-from ....core.preprocessing.params import RegType
+from ....core.params import RegType
 from ....core.pure_numpy_ops.pred import init_pred
 from ....model import SgbModel
 from ..component import Component, Devices, print_params
 
 
 @dataclass
 class ModelBuilderParams:
@@ -45,30 +44,30 @@
     def __init__(self):
         self.params = ModelBuilderParams()
 
     def show_params(self):
         print_params(self.params)
 
     def set_params(self, params: dict):
-        obj = params.get('objective', 'logistic')
-        assert obj in [
-            e.value for e in RegType
-        ], f"objective should in {[e.value for e in RegType]}, got {obj}"
+        obj = params.get('objective', default_params.objective.value)
         obj = RegType(obj)
         self.params.objective = obj
-        self.params.base_score = float(params.get('base_score', 0))
+        self.params.base_score = params.get('base_score', default_params.base_score)
 
     def get_params(self, params: dict):
         params['base_score'] = self.params.base_score
         params['objective'] = self.params.objective
 
     def set_devices(self, devices: Devices):
         self.label_holder = devices.label_holder
 
-    def set_actors(self, _: SGBActor):
+    def set_actors(self, _):
+        return
+
+    def del_actors(self):
         return
 
     def init_pred(self, sample_num: int) -> PYUObject:
         base = self.params.base_score
         return self.label_holder(init_pred)(base=base, samples=sample_num)
 
     def init_model(self):
```

## secretflow/ml/boost/sgb_v/factory/components/node_selector/node_selector.py

```diff
@@ -12,15 +12,14 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import List, Tuple, Union
 
 import numpy as np
 
 from secretflow.device import PYUObject
-from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
 from ....core.pure_numpy_ops.node_select import get_child_select, root_select
 from ..component import Component, Devices
 
 
 class NodeSelector(Component):
     def __init__(self) -> None:
@@ -34,15 +33,18 @@
 
     def get_params(self, _: dict):
         return
 
     def set_devices(self, devices: Devices):
         self.label_holder = devices.label_holder
 
-    def set_actors(self, _: SGBActor):
+    def set_actors(self, _):
+        return
+
+    def del_actors(self):
         return
 
     def root_select(self, sample_num):
         return root_select(samples=sample_num)
 
     def is_list_empty(self, any_list: Union[PYUObject, List]) -> PYUObject:
         return self.label_holder(lambda any_list: len(any_list) == 0)(any_list)
```

## secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_actor.py

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 
 
 from typing import List, Tuple, Union
 
 import numpy as np
 
-from ....core.split_tree_trainer.order_map_context import OrderMapContext
+from .order_map_context import OrderMapContext
 
 
 # handle order map building for one party
 class OrderMapActor:
     def __init__(self, idx: int) -> None:
         self.idx = idx
         self.ordermap_context = OrderMapContext()
```

## secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_manager.py

```diff
@@ -14,15 +14,15 @@
 
 import math
 from dataclasses import dataclass
 from typing import Dict, List, Union
 
 from secretflow.data import FedNdarray, PartitionWay
 from secretflow.device import PYUObject
-from secretflow.ml.boost.sgb_v.factory.params import default_params
+from secretflow.ml.boost.sgb_v.core.params import default_params
 from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
 from ..component import Component, Devices, print_params
 from ..logging import LoggingParams, LoggingTools
 from .order_map_actor import OrderMapActor
 
 
@@ -50,22 +50,19 @@
 
     def show_params(self):
         print_params(self.params)
         print_params(self.logging_params)
 
     def set_params(self, params: Dict):
         # validate
-        sketch = params.get('sketch_eps', 0.1)
-        assert (
-            sketch > 0 and sketch <= 1
-        ), f"sketch_eps should in (0, 1], got {sketch}"  # set
+        sketch = params.get('sketch_eps', default_params.sketch_eps)
         self.params.sketch_eps = sketch
         # derive attributes
         self.buckets = eps_inverse(sketch)
-        self.params.seed = int(params.get('seed', 1212))
+        self.params.seed = params.get('seed', default_params.seed)
 
         self.logging_params = LoggingTools.logging_params_from_dict(params)
 
     def get_params(self, params: dict):
         params['sketch_eps'] = self.params.sketch_eps
         params['seed'] = self.params.seed
 
@@ -75,14 +72,17 @@
         self.workers = devices.workers
 
     def set_actors(self, actors: SGBActor):
         self.order_map_actors = actors
         for i, actor in enumerate(self.order_map_actors):
             actor.register_class('OrderMapActor', OrderMapActor, i)
 
+    def del_actors(self):
+        del self.order_map_actors
+
     @LoggingTools.enable_logging
     def build_order_map(self, x: FedNdarray) -> FedNdarray:
         # we assumed x's devices match when setting up devices.
         buckets, seed = self.buckets, self.params.seed
         self.order_map = FedNdarray(
             {
                 order_map_actor.device: order_map_actor.invoke_class_method(
```

## secretflow/ml/boost/sgb_v/factory/components/sampler/sample_actor.py

```diff
@@ -60,19 +60,19 @@
         # shuffle to protect top set
         self.rng.shuffle(used_set)
         w = w[used_set]
 
         return used_set, w
 
     def generate_row_choices(
-        self, row_num: int, row_sample_rate: float
+        self, row_num: int, rowsample_by_tree: float
     ) -> Union[None, np.ndarray]:
-        if row_sample_rate == 1:
+        if rowsample_by_tree == 1:
             return None
-        sample_num_in_tree = math.ceil(row_num * row_sample_rate)
+        sample_num_in_tree = math.ceil(row_num * rowsample_by_tree)
 
         sample_num, choices = (
             row_num,
             sample_num_in_tree,
         )
 
         return self.rng.choice(sample_num, choices, replace=False, shuffle=True)
```

## secretflow/ml/boost/sgb_v/factory/components/sampler/sampler.py

```diff
@@ -15,28 +15,28 @@
 from dataclasses import dataclass
 from typing import List, Tuple, Union
 
 import numpy as np
 
 from secretflow.data import FedNdarray, PartitionWay
 from secretflow.device import PYUObject
-from secretflow.ml.boost.sgb_v.factory.params import default_params
+from secretflow.ml.boost.sgb_v.core.params import default_params
 from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
 from ..component import Component, Devices, print_params
 from .sample_actor import SampleActor
 
 
 @dataclass
 class SamplerParams:
     """
-    'row_sample_rate': float. Row sub sample ratio of the training instances.
+    'rowsample_by_tree': float. Row sub sample ratio of the training instances.
         default: 1
         range: (0, 1]
-    'col_sample_rate': float. Col sub sample ratio of columns when constructing each tree.
+    'colsample_by_tree': float. Col sub sample ratio of columns when constructing each tree.
         default: 1
         range: (0, 1]
     'seed': int. Pseudorandom number generator seed.
         default: 1212
     'label_holder_feature_only': bool. affects col sampling.
         default: False
         if turned on, all non-label holder's col sample rate will be 0.
@@ -46,96 +46,84 @@
         default: 0.3
         range: (0, 1), but top_rate + bottom_rate < 1
     'bottom_rate': float. GOSS-specific parameter. The fraction of small gradients to sample.
         default: 0.5
         range: (0, 1), but top_rate + bottom_rate < 1
     """
 
-    row_sample_rate: float = default_params.row_sample_rate
-    col_sample_rate: float = default_params.col_sample_rate
+    rowsample_by_tree: float = default_params.rowsample_by_tree
+    colsample_by_tree: float = default_params.colsample_by_tree
     seed: int = default_params.seed
-    label_holder_feature_only: bool = default_params.label_holder_feature_only
+    label_holder_feature_only: bool = False
     enable_goss: bool = default_params.enable_goss
     top_rate: float = default_params.top_rate
     bottom_rate: float = default_params.bottom_rate
 
 
 class Sampler(Component):
     def __init__(self):
         self.params = SamplerParams()
 
     def show_params(self):
         print_params(self.params)
 
     def set_params(self, params: dict):
-        subsample = float(params.get('row_sample_rate', 1))
-        assert (
-            subsample > 0 and subsample <= 1
-        ), f"row_sample_rate should in (0, 1], got {subsample}"
-
-        colsample = float(params.get('col_sample_rate', 1))
-        assert (
-            colsample > 0 and colsample <= 1
-        ), f"col_sample_rate should in (0, 1], got {colsample}"
-
-        top_rate = float(params.get('top_rate', 0.3))
-        assert (
-            top_rate > 0 and top_rate < 1
-        ), f"top_rate should in (0, 1), got {top_rate}"
-
-        bottom_rate = float(params.get('bottom_rate', 0.5))
-        assert (
-            bottom_rate > 0 and bottom_rate < 1
-        ), f"bottom_rate should in (0, 1), got {bottom_rate}"
+        subsample = params.get('rowsample_by_tree', default_params.rowsample_by_tree)
+        colsample = params.get('colsample_by_tree', default_params.colsample_by_tree)
+        top_rate = params.get('top_rate', default_params.top_rate)
+        bottom_rate = params.get('bottom_rate', default_params.bottom_rate)
 
         assert (
             bottom_rate + top_rate < 1
         ), f"the sum of top_rate and bottom_rate should be less than 1, got {bottom_rate + top_rate}"
 
-        self.params.row_sample_rate = subsample
-        self.params.col_sample_rate = colsample
-        self.params.seed = int(params.get('seed', 1212))
-        self.params.label_holder_feature_only = bool(
-            params.get('label_holder_feature_only', False)
+        self.params.rowsample_by_tree = subsample
+        self.params.colsample_by_tree = colsample
+        self.params.seed = params.get('seed', default_params.seed)
+        self.params.label_holder_feature_only = params.get(
+            'label_holder_feature_only', False
         )
 
-        self.params.enable_goss = bool(params.get('enable_goss', False))
+        self.params.enable_goss = params.get('enable_goss', default_params.enable_goss)
         self.params.top_rate = top_rate
         self.params.bottom_rate = bottom_rate
 
     def get_params(self, params: dict):
         params['seed'] = self.params.seed
-        params['row_sample_rate'] = self.params.row_sample_rate
-        params['col_sample_rate'] = self.params.col_sample_rate
+        params['rowsample_by_tree'] = self.params.rowsample_by_tree
+        params['colsample_by_tree'] = self.params.colsample_by_tree
         params['label_holder_feature_only'] = self.params.label_holder_feature_only
         params['enable_goss'] = self.params.enable_goss
         params['top_rate'] = self.params.top_rate
         params['bottom_rate'] = self.params.bottom_rate
 
     def set_devices(self, devices: Devices):
         self.label_holder = devices.label_holder
         self.workers = devices.workers
 
     def set_actors(self, actors: SGBActor):
         self.sample_actors = {actor.device: actor for actor in actors}
         for actor in self.sample_actors.values():
             actor.register_class('SampleActor', SampleActor, self.params.seed)
 
+    def del_actors(self):
+        del self.sample_actors
+
     def generate_col_choices(
         self, feature_buckets: List[PYUObject]
     ) -> Tuple[List[PYUObject], List[PYUObject]]:
         """Generate column sample choices.
 
         Args:
             feature_buckets (List[PYUObject]): Behind PYUObject is List[int], bucket num for each feature.
 
         Returns:
             Tuple[List[PYUObject], List[PYUObject]]: first list is column choices, second is total number of buckets after sampling
         """
-        colsample = self.params.col_sample_rate
+        colsample = self.params.colsample_by_tree
 
         if self.params.label_holder_feature_only:
             col_choices, total_buckets = zip(
                 *[
                     self.sample_actors[fb.device].invoke_class_method_two_ret(
                         'SampleActor',
                         'generate_one_partition_col_choices',
@@ -181,31 +169,31 @@
         if self.params.enable_goss:
             top_rate = self.params.top_rate
             bottom_rate = self.params.bottom_rate
             return self.sample_actors[g.device].invoke_class_method_two_ret(
                 'SampleActor', 'goss', row_num, g, top_rate, bottom_rate
             )
         else:
-            sample_rate = self.params.row_sample_rate
+            sample_rate = self.params.rowsample_by_tree
             choices = self.sample_actors[g.device].invoke_class_method(
                 'SampleActor', 'generate_row_choices', row_num, sample_rate
             )
             return choices, None
 
     def _should_row_subsampling(self) -> bool:
-        return self.params.row_sample_rate < 1 or self.params.enable_goss
+        return self.params.rowsample_by_tree < 1 or self.params.enable_goss
 
     def _apply_vector_sampling(
         self,
         x: PYUObject,
         indices: Union[PYUObject, np.ndarray],
     ):
         """Sample x for a single partition. Assuming we have a column vector.
         Assume the indices was generated from row sampling by sampler"""
-        if self.params.row_sample_rate < 1:
+        if self.params.rowsample_by_tree < 1:
             return x.device(lambda x, indices: x.reshape(-1, 1)[indices, :])(x, indices)
         else:
             return x.device(lambda x: x.reshape(-1, 1))(x)
 
     def apply_vector_sampling_weighted(
         self,
         x: PYUObject,
@@ -241,15 +229,15 @@
 
         Returns:
             X_sub (FedNdarray): subsampled X
             shape (Tuple[int, int]): shape of X_sub
         """
         X_sub = X
         # sample cols and rows of bucket_map
-        if self.params.col_sample_rate < 1 and self._should_row_subsampling():
+        if self.params.colsample_by_tree < 1 and self._should_row_subsampling():
             # sub choices is stored in context owned by label_holder and shared to all workers.
             X_sub = FedNdarray(
                 partitions={
                     pyu: pyu(lambda x, y, z: x[y, :][:, z])(
                         partition,
                         row_choices.to(pyu)
                         if isinstance(row_choices, PYUObject)
@@ -257,15 +245,15 @@
                         col_choices[i],
                     )
                     for i, (pyu, partition) in enumerate(X.partitions.items())
                 },
                 partition_way=PartitionWay.VERTICAL,
             )
         # only sample cols
-        elif self.params.col_sample_rate < 1:
+        elif self.params.colsample_by_tree < 1:
             X_sub = FedNdarray(
                 partitions={
                     pyu: pyu(lambda x, y: x[:, y])(partition, col_choices[i])
                     for i, (pyu, partition) in enumerate(X.partitions.items())
                 },
                 partition_way=PartitionWay.VERTICAL,
             )
```

## secretflow/ml/boost/sgb_v/factory/components/shuffler/shuffler.py

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import List, Union
 
 from secretflow.device import PYUObject
-from secretflow.ml.boost.sgb_v.factory.params import default_params
+from secretflow.ml.boost.sgb_v.core.params import default_params
 from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
 from ..component import Component, Devices, print_params
 from .worker_shuffler import WorkerShuffler
 
 
 @dataclass
@@ -38,29 +38,32 @@
         self.worker_shufflers = []
         self.params = ShufflerParams()
 
     def show_params(self):
         print_params(self.params)
 
     def set_params(self, params: dict):
-        self.params.seed = int(params.get('seed', 1212))
+        self.params.seed = params.get('seed', default_params.seed)
 
     def get_params(self, params: dict):
         params['seed'] = self.params.seed
 
     def set_devices(self, devices: Devices):
         self.workers = devices.workers
         return
 
-    def set_actors(self, actors: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
         self.worker_shufflers = actors
         for worker in self.worker_shufflers:
             # may change random state initialie methods latter
             worker.register_class('WorkerShuffler', WorkerShuffler, self.params.seed)
 
+    def del_actors(self):
+        del self.worker_shufflers
+
     def reset_shuffle_masks(self):
         for ws in self.worker_shufflers:
             ws.invoke_class_method('WorkerShuffler', 'reset_shuffle_mask')
 
     def create_shuffle_mask(
         self, worker_index: int, key: int, bucket_list: List[PYUObject]
     ) -> List[int]:
```

## secretflow/ml/boost/sgb_v/factory/components/shuffler/worker_shuffler.py

```diff
@@ -14,15 +14,15 @@
 
 from typing import List
 
 import numpy as np
 
 from secretflow.device import PYUObject
 
-from ....core.split_tree_trainer.shuffler import Shuffler
+from .shuffler_core import Shuffler
 
 
 class WorkerShuffler:
     def __init__(self, seed: int):
         np.random.seed(seed)
         self.shuffler = Shuffler()
```

## secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_heap.py

```diff
@@ -89,14 +89,18 @@
                     split_gains[i],
                     split_buckets[i],
                 )
 
     def pop(self) -> SplitCandidate:
         return heapq.heappop(self.heap)
 
+    def is_heap_empty(self) -> bool:
+        return len(self.heap) == 0
+
+    # check heap not empty before extract
     def extract_best_split_info(self) -> Tuple[int, np.ndarray, int]:
         best_candidate = self.pop()
         return (
             best_candidate.node_index,
             best_candidate.info.sample_selects,
             best_candidate.info.split_bucket,
         )
```

## secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_manager.py

```diff
@@ -46,21 +46,24 @@
 
     def get_params(self, _: dict):
         return
 
     def set_devices(self, devices: Devices):
         self.label_holder = devices.label_holder
 
-    def set_actors(self, actors: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
         for actor in actors:
             if actor.device == self.label_holder:
                 self.heap = actor
                 break
         self.heap.register_class('SplitCandidateHeap', SplitCandidateHeap)
 
+    def del_actors(self):
+        del self.heap
+
     def batch_push(
         self,
         node_indices: List[int],
         node_sample_selects: List[np.ndarray],
         split_buckets: np.ndarray,
         split_gains: np.ndarray,
         gain_is_cost_effective: List[bool],
@@ -87,14 +90,17 @@
             'push',
             node_index,
             sample_selects,
             max_gain,
             split_bucket,
         )
 
+    def is_no_candidate_left(self) -> bool:
+        return self.heap.invoke_class_method('SplitCandidateHeap', 'is_heap_empty')
+
     def extract_best_split_info(self) -> Tuple[int, np.ndarray, int]:
         return self.heap.invoke_class_method_three_ret(
             'SplitCandidateHeap', 'extract_best_split_info'
         )
 
     def extract_all_nodes(self) -> Tuple[List[int], List[np.ndarray]]:
         """Get all sample ids and sample selects and clean the heap"""
```

## secretflow/ml/boost/sgb_v/factory/components/split_finder/leaf_wise_split_finder.py

```diff
@@ -14,16 +14,15 @@
 
 import pickle
 from dataclasses import dataclass, field
 from typing import Tuple
 
 import numpy as np
 
-from secretflow.ml.boost.sgb_v.factory.params import default_params
-from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
+from secretflow.ml.boost.sgb_v.core.params import default_params
 
 from ....core.pure_numpy_ops.boost import find_best_splits
 from ..component import Component, Devices, print_params
 
 
 # This is the leaf wise split finder
 # given gains, find the best split.
@@ -50,38 +49,31 @@
     def __init__(self) -> None:
         self.params = SplitFinderParams()
 
     def show_params(self):
         print_params(self.params)
 
     def set_params(self, params: dict):
-        gamma = float(params.get('gamma', 0))
-        assert gamma >= 0 and gamma <= 10000, f"gamma should in [0, 10000], got {gamma}"
-
-        reg_lambda = float(params.get('reg_lambda', 0.1))
-        assert (
-            reg_lambda >= 0 and reg_lambda <= 10000
-        ), f"reg_lambda should in [0, 10000], got {reg_lambda}"
-
+        gamma = params.get('gamma', default_params.gamma)
+        reg_lambda = params.get('reg_lambda', default_params.reg_lambda)
         audit_paths = params.get('audit_paths', {})
-        assert isinstance(audit_paths, dict), " audit paths must be a dict"
 
         self.params.gamma = gamma
         self.params.reg_lambda = reg_lambda
         self.params.audit_paths = audit_paths
 
     def get_params(self, params: dict):
         params['gamma'] = self.params.gamma
         params['reg_lambda'] = self.params.reg_lambda
         params['audit_path'] = self.params.audit_paths
 
     def set_devices(self, devices: Devices):
         self.label_holder = devices.label_holder
 
-    def set_actors(self, _: SGBActor):
+    def set_actors(self, _):
         return
 
     def find_best_splits(
         self, G: np.ndarray, H: np.ndarray, tree_num: int, level: int
     ) -> Tuple[np.ndarray, np.ndarray]:
         reg_lambda = self.params.reg_lambda
         gamma = self.params.gamma
```

## secretflow/ml/boost/sgb_v/factory/components/split_finder/split_finder.py

```diff
@@ -14,16 +14,15 @@
 
 import pickle
 from dataclasses import dataclass, field
 from typing import Tuple
 
 import numpy as np
 
-from secretflow.ml.boost.sgb_v.factory.params import default_params
-from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
+from secretflow.ml.boost.sgb_v.core.params import default_params
 
 from ....core.pure_numpy_ops.boost import find_best_splits, find_best_splits_with_gains
 from ..component import Component, Devices, print_params
 
 
 @dataclass
 class SplitFinderParams:
@@ -48,38 +47,34 @@
     def __init__(self) -> None:
         self.params = SplitFinderParams()
 
     def show_params(self):
         print_params(self.params)
 
     def set_params(self, params: dict):
-        gamma = float(params.get('gamma', 0))
-        assert gamma >= 0 and gamma <= 10000, f"gamma should in [0, 10000], got {gamma}"
-
-        reg_lambda = float(params.get('reg_lambda', 0.1))
-        assert (
-            reg_lambda >= 0 and reg_lambda <= 10000
-        ), f"reg_lambda should in [0, 10000], got {reg_lambda}"
-
+        gamma = params.get('gamma', default_params.gamma)
+        reg_lambda = params.get('reg_lambda', default_params.reg_lambda)
         audit_paths = params.get('audit_paths', {})
-        assert isinstance(audit_paths, dict), " audit paths must be a dict"
 
         self.params.gamma = gamma
         self.params.reg_lambda = reg_lambda
         self.params.audit_paths = audit_paths
 
     def get_params(self, params: dict):
         params['gamma'] = self.params.gamma
         params['reg_lambda'] = self.params.reg_lambda
         params['audit_path'] = self.params.audit_paths
 
     def set_devices(self, devices: Devices):
         self.label_holder = devices.label_holder
 
-    def set_actors(self, _: SGBActor):
+    def set_actors(self, _):
+        return
+
+    def del_actors(self):
         return
 
     def find_best_splits_with_gains(
         self, G: np.ndarray, H: np.ndarray, tree_num: int, leaf: int
     ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
         reg_lambda = self.params.reg_lambda
         gamma = self.params.gamma
```

## secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/split_tree_builder.py

```diff
@@ -35,19 +35,22 @@
     def get_params(self, _):
         return
 
     def set_devices(self, devices: Devices):
         self.workers = devices.workers
         self.label_holder = devices.label_holder
 
-    def set_actors(self, actors: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
         self.split_tree_builder_actors = actors
         for i, actor in enumerate(self.split_tree_builder_actors):
             actor.register_class('SplitTreeActor', SplitTreeActor, i)
 
+    def del_actors(self):
+        del self.split_tree_builder_actors
+
     def reset(self):
         for actor in self.split_tree_builder_actors:
             actor.invoke_class_method('SplitTreeActor', 'reset')
 
     def set_col_choices_and_buckets(
         self,
         col_choices: List[PYUObject],
```

## secretflow/ml/boost/sgb_v/factory/components/tree_trainer/leaf_wise_tree_trainer.py

```diff
@@ -15,15 +15,15 @@
 import logging
 from dataclasses import dataclass
 from typing import List, Tuple, Union
 
 import numpy as np
 
 from secretflow.device import PYUObject, reveal
-from secretflow.ml.boost.sgb_v.factory.params import default_params
+from secretflow.ml.boost.sgb_v.core.params import default_params
 from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
 from ....core.distributed_tree.distributed_tree import DistributedTree
 from ..bucket_sum_calculator import LeafWiseBucketSumCalculator
 from ..component import Devices, print_params
 from ..gradient_encryptor import GradientEncryptor
 from ..leaf_manager import LeafManager
@@ -86,42 +86,43 @@
 
     def set_devices(self, devices: Devices):
         super().set_devices(devices)
         self.workers = devices.workers
         self.label_holder = devices.label_holder
         self.party_num = len(self.workers)
 
-    def set_actors(self, actors: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
         return super().set_actors(actors)
 
+    def del_actors(self):
+        super().del_actors()
+
     def _get_trainer_params(self, params: dict):
         params['max_leaf'] = self.params.max_leaf
         LoggingTools.logging_params_write_dict(params, self.logging_params)
 
     def _set_trainer_params(self, params: dict):
-        leaf_num = int(params.get('max_leaf', 15))
-        assert (
-            leaf_num > 0 and leaf_num <= 2**15
-        ), f"max_depth should in [1, 2**15], got {leaf_num}"
-
+        leaf_num = params.get('max_leaf', default_params.max_leaf)
         self.params.max_leaf = leaf_num
         self.logging_params = LoggingTools.logging_params_from_dict(params)
 
     def train_tree_context_setup(
         self,
         cur_tree_num: int,
         order_map_manager: OrderMapManager,
         y: PYUObject,
         pred: Union[PYUObject, np.ndarray],
         x_shape: Tuple[int, int],
     ):
+        logging.info("train tree context set up.")
         # reset caches
         self.components.split_tree_builder.reset()
         self.components.leaf_manager.clear_leaves()
         self.components.bucket_sum_calculator.reset_cache()
+        logging.debug("cache resetted.")
 
         # sub sampling
         feature_buckets = order_map_manager.get_feature_buckets()
         col_choices, total_buckets = self.components.sampler.generate_col_choices(
             feature_buckets
         )
         self.components.split_tree_builder.set_col_choices_and_buckets(
@@ -137,56 +138,69 @@
             order_map, row_choices, col_choices
         )
 
         self.order_map_sub = order_map_sub
         self.bucket_num = order_map_manager.buckets
         self.row_choices = row_choices
 
+        logging.debug("sub sampled (per tree).")
+
         # compute g, h and encryption
         g = self.components.sampler.apply_vector_sampling_weighted(
             g, row_choices, weight
         )
         h = self.components.sampler.apply_vector_sampling_weighted(
             h, row_choices, weight
         )
         self.components.loss_computer.compute_abs_sums(g, h)
-        self.should_stop = self.components.loss_computer.check_early_stop()
+
+        logging.debug("g h computed.")
+
+        self.should_stop = reveal(self.components.loss_computer.check_early_stop())
         if self.should_stop:
+            logging.debug("early stopped.")
             return
+        logging.debug("not early stopped.")
         self.components.loss_computer.compute_scales()
 
         g, h = self.components.loss_computer.scale_gh(g, h)
         self.g = g
         self.h = h
+        logging.debug("g h scaled.")
+
         gh = self.components.gradient_encryptor.pack(g, h)
         encrypted_gh = self.components.gradient_encryptor.encrypt(gh, cur_tree_num)
         self.encrypted_gh_dict = self.components.gradient_encryptor.cache_to_workers(
             encrypted_gh, gh
         )
+        logging.debug("g h encrypted.")
 
     @LoggingTools.enable_logging
     def train_tree(
         self,
         cur_tree_num: int,
         order_map_manager: OrderMapManager,
         y: PYUObject,
         pred: Union[PYUObject, np.ndarray],
         x_shape: Tuple[int, int],
     ) -> Union[None, DistributedTree]:
         self.train_tree_context_setup(cur_tree_num, order_map_manager, y, pred, x_shape)
         if self.should_stop:
             return None
+        logging.info("begin train tree.")
         row_num = self.order_map_sub.shape[0]
         root_select = self.components.node_selector.root_select(row_num)
         g, h = self.g, self.h
         # leaf wise train begins
         new_split_node_selects = root_select
         new_split_node_indices = [0]
 
+        logging.debug("begin leaf wise training")
         for leaf in range(self.params.max_leaf):
+            logging.debug(f"training leaf {leaf}.")
             new_split_node_selects, new_split_node_indices = self._train_leaf(
                 new_split_node_selects,
                 new_split_node_indices,
                 leaf,
                 cur_tree_num,
                 order_map_manager,
             )
@@ -309,14 +323,16 @@
             pruned_node_indices,
             pruned_s,
         ) = self.components.node_selector.get_pruned_indices_and_selects(
             new_split_node_indices, new_split_node_selects, gain_is_cost_effective
         )
         self.components.leaf_manager.extend_leaves(pruned_s, pruned_node_indices)
 
+        if reveal(self.components.split_candidate_manager.is_no_candidate_left()):
+            return [], []
         # select the best candidate to do split
         (
             node_index,
             sample_selects,
             split_bucket,
         ) = self.components.split_candidate_manager.extract_best_split_info()
```

## secretflow/ml/boost/sgb_v/factory/components/tree_trainer/level_wise_tree_trainer.py

```diff
@@ -8,21 +8,22 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import logging
 from dataclasses import dataclass
 from typing import List, Tuple, Union
 
 import numpy as np
 
 from secretflow.device import PYUObject, reveal
-from secretflow.ml.boost.sgb_v.factory.params import default_params
+from secretflow.ml.boost.sgb_v.core.params import default_params
 from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
 from ....core.distributed_tree.distributed_tree import DistributedTree
 from ..bucket_sum_calculator import BucketSumCalculator
 from ..component import Devices, print_params
 from ..gradient_encryptor import GradientEncryptor
 from ..leaf_manager import LeafManager
@@ -91,32 +92,33 @@
         return super().set_actors(actors)
 
     def _get_trainer_params(self, params: dict):
         params['max_depth'] = self.params.max_depth
         LoggingTools.logging_params_write_dict(params, self.logging_params)
 
     def _set_trainer_params(self, params: dict):
-        depth = int(params.get('max_depth', 5))
-        assert depth > 0 and depth <= 16, f"max_depth should in [1, 16], got {depth}"
+        depth = params.get('max_depth', default_params.max_depth)
 
         self.params.max_depth = depth
         self.logging_params = LoggingTools.logging_params_from_dict(params)
 
     def train_tree_context_setup(
         self,
         cur_tree_num: int,
         order_map_manager: OrderMapManager,
         y: PYUObject,
         pred: Union[PYUObject, np.ndarray],
         x_shape: Tuple[int, int],
     ):
+        logging.info("train tree context set up.")
         # reset caches
         self.components.split_tree_builder.reset()
         self.components.leaf_manager.clear_leaves()
         self.components.bucket_sum_calculator.reset_cache()
+        logging.debug("cache resetted.")
 
         # sub sampling
         feature_buckets = order_map_manager.get_feature_buckets()
         col_choices, total_buckets = self.components.sampler.generate_col_choices(
             feature_buckets
         )
         self.components.split_tree_builder.set_col_choices_and_buckets(
@@ -130,58 +132,68 @@
         order_map = order_map_manager.get_order_map()
         self.bucket_lists = order_map_manager.get_bucket_lists(col_choices)
         self.order_map_sub = self.components.sampler.apply_v_fed_sampling(
             order_map, row_choices, col_choices
         )
         self.row_choices = row_choices
         self.bucket_num = order_map_manager.buckets
+        logging.debug("sub sampled (per tree).")
 
         # compute g, h and encryption
         g = self.components.sampler.apply_vector_sampling_weighted(
             g, row_choices, weight
         )
         h = self.components.sampler.apply_vector_sampling_weighted(
             h, row_choices, weight
         )
         self.components.loss_computer.compute_abs_sums(g, h)
-        self.should_stop = self.components.loss_computer.check_early_stop()
+        logging.debug("g h computed.")
+
+        self.should_stop = reveal(self.components.loss_computer.check_early_stop())
         if self.should_stop:
+            logging.debug("early stopped.")
             return
+        logging.debug("not early stopped.")
         self.components.loss_computer.compute_scales()
 
         g, h = self.components.loss_computer.scale_gh(g, h)
         self.g = g
         self.h = h
+        logging.debug("g h scaled.")
+
         gh = self.components.gradient_encryptor.pack(g, h)
         encrypted_gh = self.components.gradient_encryptor.encrypt(gh, cur_tree_num)
         self.encrypted_gh_dict = self.components.gradient_encryptor.cache_to_workers(
             encrypted_gh, gh
         )
+        logging.debug("g h encrypted.")
 
     @LoggingTools.enable_logging
     def train_tree(
         self,
         cur_tree_num,
         order_map_manager: OrderMapManager,
         y: PYUObject,
         pred: Union[PYUObject, np.ndarray],
         x_shape: Tuple[int, int],
     ) -> DistributedTree:
         self.train_tree_context_setup(cur_tree_num, order_map_manager, y, pred, x_shape)
         if self.should_stop:
             return None
+        logging.info("begin train tree.")
         row_num = self.order_map_sub.shape[0]
         g, h = self.g, self.h
         root_select = self.components.node_selector.root_select(row_num)
 
         # level wise train begins
         split_node_selects = root_select
         split_node_indices = [0]
-
+        logging.debug("beging level wise training.")
         for level in range(self.params.max_depth):
+            logging.debug(f"training level {level}.")
             split_node_selects, split_node_indices = self._train_level(
                 split_node_selects,
                 split_node_indices,
                 level,
                 cur_tree_num,
                 order_map_manager,
             )
```

## secretflow/ml/boost/sgb_v/factory/components/tree_trainer/tree_trainer.py

```diff
@@ -9,14 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import abc
+from typing import List
 
 from secretflow.ml.boost.sgb_v.factory.sgb_actor import SGBActor
 
 from ....core.distributed_tree.distributed_tree import DistributedTree
 from ..component import Composite, Devices
 
 
@@ -26,15 +27,15 @@
 
     def set_params(self, params: dict):
         super().set_params(params)
 
     def set_devices(self, devices: Devices):
         super().set_devices(devices)
 
-    def set_actors(self, actors: SGBActor):
+    def set_actors(self, actors: List[SGBActor]):
         return super().set_actors(actors)
 
     @abc.abstractmethod
     def train_tree(
         self, cur_tree_num, order_map_manager, y, pred, x_shape
     ) -> DistributedTree:
         """train on training data"""
```

## secretflow/ml/nn/fl/fl_model.py

```diff
@@ -318,14 +318,15 @@
         max_batch_size=20000,
         prefetch_buffer_size=None,
         sampler_method='batch',
         random_seed=None,
         dp_spent_step_freq=None,
         audit_log_dir=None,
         dataset_builder: Dict[PYU, Callable] = None,
+        wait_steps=100,
     ) -> History:
         """Horizontal federated training interface
 
         Args:
             x: feature, FedNdArray, HDataFrame or Dict {PYU: model_path}
             y: label, FedNdArray, HDataFrame or str(column name of label)
             batch_size: Number of samples per gradient update, int or Dict, recommend 64 or more for safety
@@ -343,14 +344,15 @@
             max_batch_size: Max limit of batch size
             prefetch_buffer_size: An int specifying the number of feature batches to prefetch for performance improvement. Only for csv reader
             sampler_method: The name of sampler method
             random_seed: Prg seed for shuffling
             dp_spent_step_freq: specifies how many training steps to check the budget of dp
             audit_log_dir: path of audit log dir, checkpoint will be save if audit_log_dir is not None
             dataset_builder: Callable function about hot to build the dataset. must return (dataset, steps_per_epoch)
+            wait_steps: A step size to indicate how many concurrent tasks should be waited, which could prevent the stuck of ray when more tasks join (default 100).
         Returns:
             A history object. It's history.global_history attribute is a
             aggregated record of training loss values and metrics, while
             history.local_history attribute is a record of training loss
             values and metrics of each party.
         """
         if not random_seed:
@@ -418,14 +420,15 @@
         if self.server:
             server_weight = initial_weight
         for device, worker in self._workers.items():
             worker.init_training(callbacks, epochs=epochs)
             worker.on_train_begin()
         model_params = None
         for epoch in range(epochs):
+            res = []
             report_list = []
             pbar = tqdm(total=self.steps_per_epoch)
             # do train
             report_list.append(f"epoch: {epoch+1}/{epochs} - ")
             [worker.on_epoch_begin(epoch) for worker in self._workers.values()]
             for step in range(0, self.steps_per_epoch, aggregate_freq):
                 if verbose == 1:
@@ -441,14 +444,15 @@
                         aggregate_freq
                         if step + aggregate_freq < self.steps_per_epoch
                         else self.steps_per_epoch - step,
                         **self.kwargs,
                     )
                     client_param_list.append(client_params)
                     sample_num_list.append(sample_num)
+                    res.append(client_params)
 
                 model_params = self._aggregator.average(
                     client_param_list, axis=0, weights=sample_num_list
                 )
 
                 # Do weight sparsify
                 if self.strategy in COMPRESS_STRATEGY:
@@ -479,15 +483,17 @@
                         epoch * self.steps_per_epoch / aggregate_freq
                     ) + int(step / aggregate_freq)
                     if current_dp_step % dp_spent_step_freq == 0:
                         privacy_spent = self.dp_strategy.get_privacy_spent(
                             current_dp_step
                         )
                         logging.debug(f'DP privacy accountant {privacy_spent}')
-
+                if len(res) == wait_steps:
+                    wait(res)
+                    res = []
             local_metrics_obj = []
             for device, worker in self._workers.items():
                 worker.on_epoch_end(epoch)
                 local_metrics_obj.append(worker.wrap_local_metrics())
 
             local_metrics = reveal(local_metrics_obj)
             for local_metric in local_metrics:
```

## secretflow/ml/nn/sl/sl_model.py

```diff
@@ -87,14 +87,16 @@
             device_agg=self.device_agg if self.device_agg else self.device_y,
             parties=list(base_model_dict.keys()),
             device_y=self.device_y,
             agg_method=agg_method,
             backend=backend,
             compressor=self.compressor,
         )
+        self.pipeline_size = kwargs.get('pipeline_size', 1)
+        assert self.pipeline_size >= 1, f"invalid pipeline size: {self.pipeline_size}"
 
         if backend.lower() == "tensorflow":
             import secretflow.ml.nn.sl.backend.tensorflow.strategy  # noqa
         elif backend.lower() == "torch":
             import secretflow.ml.nn.sl.backend.torch.strategy  # noqa
         else:
             raise Exception(f"Invalid backend = {backend}")
@@ -117,14 +119,15 @@
                 device=device,
                 base_local_steps=kwargs.get('base_local_steps', 1),
                 fuse_local_steps=kwargs.get('fuse_local_steps', 1),
                 bound_param=kwargs.get('bound_param', 0.0),
                 loss_thres=kwargs.get('loss_thres', 0.01),
                 split_steps=kwargs.get('split_steps', 1),
                 max_fuse_local_steps=kwargs.get('max_fuse_local_steps', 1),
+                pipeline_size=kwargs.get('pipeline_size', 1),
             )
 
     def handle_data(
         self,
         x: Union[
             VDataFrame,
             FedNdarray,
@@ -157,32 +160,54 @@
                     if sample_weight is not None
                     else None
                 )
             else:
                 y_partitions = None
                 s_w_partitions = None
 
-            xs = [xi.partitions[device] for xi in x]
-            xs = [t.data if isinstance(t, Partition) else t for t in xs]
             if dataset_builder:
-                assert (
-                    device in dataset_builder
-                ), f"party={device} does not provide dataset_builder, please check"
+                # in dataset builder mode, xi cannot be none, or else datasetbuilder in worker cannot parse label
+                xs = (
+                    [
+                        xi.partitions[device].data  # xi is FedDataframe
+                        if isinstance(xi.partitions[device], Partition)
+                        else xi.partitions[device]  # xi is FedNdarray
+                        for xi in x
+                    ]
+                    if device in dataset_builder
+                    else [None]
+                )
+                if device not in dataset_builder:
+                    logging.warning("party={device} does not provide dataset_builder")
+                    dataset_partition = None
+                    if device in self.base_model_dict:
+                        raise Exception(
+                            "dataset builder must be supply when base_net is not none"
+                        )
+                else:
+                    dataset_partition = dataset_builder[device]
                 ret = worker.build_dataset_from_builder(
                     *xs,
                     y=y_partitions,
                     s_w=s_w_partitions,
                     batch_size=batch_size,
                     random_seed=random_seed,
                     stage=stage,
-                    dataset_builder=dataset_builder[device],
+                    dataset_builder=dataset_partition,
                 )
                 worker_steps.append(ret)
 
             else:
+                # if don't have feature, driver will pass None to device worker
+                xs = (
+                    [xi.partitions[device] for xi in x]
+                    if device in self.base_model_dict
+                    else [None]
+                )
+                xs = [t.data if isinstance(t, Partition) else t for t in xs]
                 worker.build_dataset_from_numeric(
                     *xs,
                     y=y_partitions,
                     s_w=s_w_partitions,
                     batch_size=batch_size,
                     buffer_size=batch_size * 8,
                     shuffle=shuffle,
@@ -192,14 +217,17 @@
                 )
 
         parties_length = [shape[0] for shape in x[0].partition_shape().values()]
         assert len(set(parties_length)) == 1, "length of all parties must be same"
         steps_per_epoch = math.ceil(parties_length[0] / batch_size)
         if dataset_builder:
             worker_steps_per_epoch = reveal(worker_steps)
+            worker_steps_per_epoch = [
+                steps for steps in worker_steps_per_epoch if steps != -1
+            ]
             assert (
                 len(set(worker_steps_per_epoch)) == 1
             ), "steps_per_epoch of all parties must be same, Please check whether the batchsize or steps_per_epoch of all parties are consistent"
             # set worker_steps_per_epoch[0] to steps_per_epoch if databuilder return steps_per_epoch else use driver calculate result
             if worker_steps_per_epoch[0] > 0:
                 steps_per_epoch = worker_steps_per_epoch[0]
 
@@ -237,15 +265,17 @@
             - FedNdArray: a vertically aligned ndarray.
             - List[Union[HDataFrame, VDataFrame, FedNdarray]]: list of dataframe or ndarray.
 
             y: Target data. It could be a VDataFrame or FedNdarray which has only one partition, or a PYUObject.
             batch_size: Number of samples per gradient update.
             epochs: Number of epochs to train the model
             verbose: 0, 1. Verbosity mode
-            callbacks: List of `keras.callbacks.Callback` instances.
+            callbacks: List of Callback or Dict[device, Callback]. Callback can be:
+            - `keras.callbacks.Callback` for tensorflow backend
+            - `secretflow.ml.nn.sl.backend.torch.callback.Callback` for torch backend
             validation_data: Data on which to validate
             shuffle: Whether shuffle dataset or not
             validation_freq: specifies how many training epochs to run before a new validation run is performed
             sample_weight: weights for the training samples
             dp_spent_step_freq: specifies how many training steps to check the budget of dp
             dataset_builder: Callable function, its input is `x` or `[x, y]` if y is set, it should return a
                 dataset.
@@ -305,43 +335,57 @@
                 valid_y,
                 sample_weight=valid_sample_weight,
                 batch_size=batch_size,
                 epochs=epochs,
                 stage="eval",
                 dataset_builder=dataset_builder,
             )
-
-        self._workers[self.device_y].init_training(callbacks, epochs=epochs)
+        if isinstance(callbacks, dict):
+            for dev, callback_builder in callbacks.items():
+                self._workers[dev].init_training(callback_builder, epochs=epochs)
+        else:
+            self._workers[self.device_y].init_training(callbacks, epochs=epochs)
         [worker.on_train_begin() for worker in self._workers.values()]
         wait_steps = min(min(self.get_cpus()) * 2, 100)
         for epoch in range(epochs):
             res = []
             report_list = []
             report_list.append(f"epoch: {epoch+1}/{epochs} - ")
             if verbose == 1:
                 pbar = tqdm(total=steps_per_epoch)
             self._workers[self.device_y].reset_metrics()
             [worker.on_epoch_begin(epoch) for worker in self._workers.values()]
 
-            for step in range(0, steps_per_epoch):
-                if verbose == 1:
-                    pbar.update(1)
-                hiddens = {}
-                self._workers[self.device_y].on_train_batch_begin(step=step)
-                for device, worker in self._workers.items():
-                    # 1. Local calculation of basenet
-                    hidden = worker.base_forward(stage="train")
-                    # 2. The results of basenet are sent to fusenet
-                    hiddens[device] = hidden
+            hiddens_buf = [None] * (self.pipeline_size - 1)
+            for step in range(0, steps_per_epoch + self.pipeline_size - 1):
+                if step < steps_per_epoch:
+                    if verbose == 1:
+                        pbar.update(1)
+                    hiddens = {}
+                    self._workers[self.device_y].on_train_batch_begin(step=step)
+                    for device, worker in self._workers.items():
+                        # 1. Local calculation of basenet
+                        hidden = worker.base_forward(stage="train")
+                        # 2. The results of basenet are sent to fusenet
+
+                        hiddens[device] = hidden
+                    hiddens_buf.append(hiddens)
+                # clean up buffer
+                hiddens = hiddens_buf.pop(0)
+                # Async transfer hiddens to label side
+                if hiddens is None:
+                    continue
+                # During pipeline strategy, the backpropagation process of the model will lag n cycles behind the forward propagation process.
+                step = step - self.pipeline_size + 1
+
                 # do agglayer forward
                 agg_hiddens = self.agglayer.forward(hiddens, axis=0)
-                if isinstance(agg_hiddens, PYUObject):
-                    agg_hiddens = [agg_hiddens]
+
                 # 3. Fusenet do local calculates and return gradients
-                gradients = self._workers[self.device_y].fuse_net(*agg_hiddens)
+                gradients = self._workers[self.device_y].fuse_net(agg_hiddens)
 
                 # In some strategies, we need to bypass the backpropagation step.
                 skip_gradient = False
                 if self.check_skip_grad:
                     skip_gradient = reveal(
                         self._workers[self.device_y].get_skip_gradient()
                     )
@@ -351,37 +395,45 @@
                     scatter_gradients = self.agglayer.backward(gradients)
                     for device, worker in self._workers.items():
                         if device in scatter_gradients.keys():
                             worker.base_backward(scatter_gradients[device])
 
                 r_count = self._workers[self.device_y].on_train_batch_end(step=step)
                 res.append(r_count)
+                [
+                    worker.on_train_batch_end(step=step)
+                    for dev, worker in self._workers.items()
+                    if dev != self.device_y
+                ]
+
                 if self.dp_strategy_dict is not None and dp_spent_step_freq is not None:
                     current_step = epoch * steps_per_epoch + step
                     if current_step % dp_spent_step_freq == 0:
                         privacy_device = {}
                         for device, dp_strategy in self.dp_strategy_dict.items():
                             privacy_dict = dp_strategy.get_privacy_spent(current_step)
                             privacy_device[device] = privacy_dict
                 if len(res) == wait_steps:
                     wait(res)
                     res = []
+            assert (
+                len(hiddens_buf) == 0
+            ), f'hiddens buffer unfinished, len: {len(hiddens_buf)}'
             if validation and epoch % validation_freq == 0:
                 # validation
                 self._workers[self.device_y].reset_metrics()
                 res = []
                 for step in range(0, valid_steps):
                     hiddens = {}  # driver end
                     for device, worker in self._workers.items():
                         hidden = worker.base_forward("eval")
                         hiddens[device] = hidden
                     agg_hiddens = self.agglayer.forward(hiddens, axis=0)
-                    if isinstance(agg_hiddens, PYUObject):
-                        agg_hiddens = [agg_hiddens]
-                    metrics = self._workers[self.device_y].evaluate(*agg_hiddens)
+
+                    metrics = self._workers[self.device_y].evaluate(agg_hiddens)
                     res.append(metrics)
                     if len(res) == wait_steps:
                         wait(res)
                         res = []
                 wait(res)
                 self._workers[self.device_y].on_validation(metrics)
 
@@ -404,88 +456,117 @@
                     self.save_model(
                         base_model_path=epoch_base_model_path,
                         fuse_model_path=epoch_fuse_model_path,
                         is_test=self.simulation,
                         **audit_log_params,
                     )
             epoch_log = self._workers[self.device_y].on_epoch_end(epoch)
+            call_res = [
+                worker.on_epoch_end(epoch)
+                for dev, worker in self._workers.items()
+                if dev != self.device_y
+            ]
+            wait(call_res)
             for name, metric in reveal(epoch_log).items():
                 report_list.append(f"{name}:{metric} ")
             report = " ".join(report_list)
             if verbose == 1:
                 pbar.set_postfix_str(report)
                 pbar.close()
             if reveal(self._workers[self.device_y].get_stop_training()):
                 break
 
         history = self._workers[self.device_y].on_train_end()
+        call_res = [
+            worker.on_train_end()
+            for dev, worker in self._workers.items()
+            if dev != self.device_y
+        ]
+        wait(call_res)
         return reveal(history)
 
     def predict(
         self,
         x: Union[
             VDataFrame,
             FedNdarray,
             List[Union[HDataFrame, VDataFrame, FedNdarray]],
         ],
         batch_size=32,
         verbose=0,
         dataset_builder: Callable[[List], Tuple[int, Iterable]] = None,
+        callbacks=None,
     ):
         """Vertical split learning offline prediction interface
 
         Args:
             x: Input data. It could be:
 
             - VDataFrame: a vertically aligned dataframe.
             - FedNdArray: a vertically aligned ndarray.
             - List[Union[HDataFrame, VDataFrame, FedNdarray]]: list of dataframe or ndarray.
 
             batch_size: Number of samples per gradient update, Int
             verbose: 0, 1. Verbosity mode
             dataset_builder: Callable function, its input is `x` or `[x, y]` if y is set, it should return
               steps_per_epoch and iterable dataset. Dataset builder is mainly for building graph dataset.
+            callbacks: List of `keras.callbacks.Callback` instances.
         """
 
         assert (
             isinstance(batch_size, int) and batch_size > 0
         ), f"batch_size should be integer > 0"
 
         predict_steps = self.handle_data(
             x,
             None,
             batch_size=batch_size,
             stage="eval",
             epochs=1,
             dataset_builder=dataset_builder,
         )
+        [
+            worker.init_predict(callbacks, steps=predict_steps)
+            for worker in self._workers.values()
+        ]
         if verbose > 0:
             pbar = tqdm(total=predict_steps)
             pbar.set_description('Predict Processing:')
         result = []
         wait_steps = min(min(self.get_cpus()) * 2, 100)
         res = []
+        [worker.on_predict_begin() for worker in self._workers.values()]
         for step in range(0, predict_steps):
+            [
+                worker.on_predict_batch_begin(batch=step)
+                for worker in self._workers.values()
+            ]
             forward_data_dict = {}
             for device, worker in self._workers.items():
+                if device not in self.base_model_dict:
+                    continue
                 f_data = worker.base_forward(stage="eval")
                 forward_data_dict[device] = f_data
             agg_hiddens = self.agglayer.forward(forward_data_dict, axis=0)
-            if isinstance(agg_hiddens, PYUObject):
-                agg_hiddens = [agg_hiddens]
+
             if verbose > 0:
                 pbar.update(1)
-            y_pred = self._workers[self.device_y].predict(*agg_hiddens)
+            y_pred = self._workers[self.device_y].predict(agg_hiddens)
             result.append(y_pred)
 
+            [
+                worker.on_predict_batch_end(batch=step)
+                for worker in self._workers.values()
+            ]
             res.append(y_pred)
             if len(res) == wait_steps:
                 wait(res)
                 res = []
         wait(res)
+        [worker.on_predict_end() for worker in self._workers.values()]
         return result
 
     @reveal
     def evaluate(
         self,
         x: Union[
             VDataFrame,
@@ -547,17 +628,16 @@
             hiddens = {}  # driver端
             for device, worker in self._workers.items():
                 hidden = worker.base_forward(stage="eval")
                 hiddens[device] = hidden
             if verbose > 0:
                 pbar.update(1)
             agg_hiddens = self.agglayer.forward(hiddens, axis=0)
-            if isinstance(agg_hiddens, PYUObject):
-                agg_hiddens = [agg_hiddens]
-            metrics = self._workers[self.device_y].evaluate(*agg_hiddens)
+
+            metrics = self._workers[self.device_y].evaluate(agg_hiddens)
             if (step + 1) % wait_steps == 0:
                 wait(metrics)
         report_list = [f"{k}:{v}" for k, v in reveal(metrics).items()]
         report = " ".join(report_list)
         if verbose == 1:
             pbar.set_postfix_str(report)
             pbar.close()
@@ -581,15 +661,15 @@
             >>> save_params = {'save_traces' : True,
             >>>                'save_format' : 'h5',}
             >>> slmodel.save_model(base_model_path,
             >>>                    fuse_model_path,)
             >>>                    is_test=True,)
             >>> # just passing params in
             >>> slmodel.save_model(base_model_path,
-            >>>                    fuse_model_path,)
+            >>>                    fuse_model_path,
             >>>                    is_test=True,
             >>>                    save_traces=True,
             >>>                    save_format='h5')
         """
         assert isinstance(
             base_model_path, (str, Dict)
         ), f'Model path accepts string or dict but got {type(base_model_path)}.'
```

## secretflow/ml/nn/sl/agglayer/agg_layer.py

```diff
@@ -25,14 +25,15 @@
 import secretflow as sf
 from secretflow.device import HEU, PYU, SPU, DeviceObject, PYUObject
 from secretflow.ml.nn.sl.agglayer.agg_method import AggMethod
 from secretflow.utils.communicate import ForwardData
 from secretflow.utils.compressor import Compressor, SparseCompressor
 from secretflow.utils.errors import InvalidArgumentError
 
+
 COMPRESS_DEVICE_LIST = (PYU,)
 
 
 class AggLayer(object):
     """
     The aggregation layer is situated between Basenet and Fusenet and is responsible for feature fusion, communication compression, and other intermediate layer logic.
     Attributes:
@@ -149,79 +150,126 @@
     ):
         return ForwardData(
             hidden=hidden,
             losses=losses,
         )
 
     @staticmethod
-    def handle_sparse_hiddens(hidden_features, compressor):
-        iscompressed = compressor.iscompressed(hidden_features)
-        # save fuse_sparse_masks to apply on gradients
-        fuse_sparse_masks = None
-        if isinstance(compressor, SparseCompressor):
-            fuse_sparse_masks = list(
+    def decompress_hiddens(data, compressor):
+        def _decompress_hiddens(datum):
+            """Decompress the hidden if needed"""
+
+            if isinstance(datum, ForwardData):
+                hidden_features = datum.hidden
+            else:
+                hidden_features = datum
+            iscompressed = compressor.iscompressed(hidden_features)
+            # save fuse_sparse_masks to apply on gradients if it is sparse compression.
+            fuse_sparse_masks = None
+            if isinstance(compressor, SparseCompressor):
+                fuse_sparse_masks = list(
+                    map(
+                        # Get a sparse matrix mask with dtype=bool.
+                        # Using <bool> as the dtype will ensure that the data type of gradients after applying the mask does not change.
+                        lambda d, compressed: (d != 0) if compressed else None,
+                        hidden_features,
+                        iscompressed,
+                    )
+                )
+
+            # decompress if needed
+            hidden_features = list(
                 map(
-                    # Get a sparse matrix mask with dtype=bool.
-                    # Using <bool> as the dtype will ensure that the data type of gradients after applying the mask does not change.
-                    lambda d, compressed: (d != 0) if compressed else None,
+                    lambda d, compressed: compressor.decompress(d) if compressed else d,
                     hidden_features,
                     iscompressed,
                 )
             )
-        # decompress
-        hidden_features = list(
-            map(
-                lambda d, compressed: compressor.decompress(d) if compressed else d,
-                hidden_features,
-                iscompressed,
+
+            if isinstance(datum, ForwardData):
+                datum.hidden = hidden_features
+            else:
+                datum = hidden_features
+            return datum, fuse_sparse_masks, iscompressed
+
+        if isinstance(data, Tuple) and len(data) == 1:
+            # The case is after packing and unpacking using PYU, a tuple of length 1 will be obtained,
+            # if 'num_return' is not specified to PYU.
+            data = data[0]
+        if isinstance(data, (List, Tuple)):
+            hidden_features = [
+                f.hidden if isinstance(f, ForwardData) else f for f in data
+            ]
+            hidden_features[:] = (h for h in hidden_features if h is not None)
+            # Deal with sparse input, record the mask and is_compressed.
+            dense_data, fuse_sparse_masks, iscompressed = _decompress_hiddens(
+                hidden_features
             )
-        )
-        return hidden_features, fuse_sparse_masks, iscompressed
+
+            ret_data = []
+            for f_d, d in zip(data, dense_data):
+                if isinstance(f_d, ForwardData):
+                    ret_data.append(ForwardData(d, f_d.losses))
+                else:
+                    ret_data.append(d)
+            return ret_data, fuse_sparse_masks, iscompressed
+        else:
+            data, fuse_sparse_masks, iscompressed = _decompress_hiddens(data)
+            return data, fuse_sparse_masks, iscompressed
 
     @staticmethod
-    def handle_sparse_gradients(gradient, sparse_masks, compressor, iscompressed):
-        gradient = [g.numpy() for g in gradient]
-        # apply fuse_sparse_masks on gradients
-        if sparse_masks:
-            assert len(sparse_masks) == len(
-                gradient
-            ), f'length of fuse_sparse_masks and gradient mismatch: {len(sparse_masks)} - {len(gradient)}'
-
-            def apply_mask(m, d):
-                if m is not None:
-                    return m.multiply(d).tocsr()
-                return d
+    def compress_gradients(gradients, fuse_sparse_masks, compressor, iscompressed):
+        """compress gradients to sparse format"""
 
-            gradient = list(map(apply_mask, sparse_masks, gradient))
+        def apply_sparse_mask(m, d):
+            if m is not None:
+                return m.multiply(d).tocsr()
+            return d
+
+        gradients = [g.numpy() for g in gradients]
+        # when using sparse compressor, use masks on gradients can avoid compressing twice.
+        if (
+            isinstance(compressor, SparseCompressor)
+            and fuse_sparse_masks is not None
+            and fuse_sparse_masks[0] is not None
+        ):
+            assert len(fuse_sparse_masks) == len(
+                gradients
+            ), f'length of fuse_sparse_masks and gradient mismatch: {len(fuse_sparse_masks)} - {len(gradients)}'
+            gradients = list(map(apply_sparse_mask, fuse_sparse_masks, gradients))
         else:
-            gradient = list(
-                map(lambda d, compressed: compressor.compress(d) if compressed else d),
-                gradient,
-                iscompressed,
+            gradients = list(
+                map(
+                    lambda d, compressed: compressor.compress(d) if compressed else d,
+                    gradients,
+                    iscompressed,
+                )
             )
-        return gradient
+        return gradients
 
     def split_to_parties(self, data: List) -> List[PYUObject]:
         assert (
             self.basenet_output_num is not None
         ), "Agglayer should know output num of each participates"
         assert len(data) == sum(
             self.basenet_output_num.values()
         ), f"data length in backward = {len(data)} is not consistent with basenet need = {sum(self.basenet_output_num.values())},"
 
         result = []
         start_idx = 0
         for p in self.parties:
             data_slice = data[start_idx : start_idx + self.basenet_output_num[p]]
             result.append(data_slice)
-            start_idx = start_idx + start_idx + self.basenet_output_num[p]
+            start_idx = start_idx + self.basenet_output_num[p]
         return result
 
     def collect(self, data: Dict[PYU, DeviceObject]) -> List[DeviceObject]:
-        """Collect data from participates"""
+        """Collect data from participates
+        TODO: Support compress communication when using agg method.
+        """
         assert data, 'Data to aggregate should not be None or empty!'
 
         # Record the values of fields in ForwardData except for hidden
         self.losses = []
 
         coverted_data = []
         for device, f_datum in data.items():
@@ -240,15 +288,20 @@
             coverted_data.append(hidden)
         # do transfer
         server_data = [d.to(self.device_agg) for d in coverted_data]
 
         # do decompress after recieve data from each parties
         if isinstance(self.device_agg, COMPRESS_DEVICE_LIST) and self.compressor:
             server_data = [
-                self.device_agg(self.compressor.decompress)(d) for d in server_data
+                self.device_agg(
+                    lambda compressor, d: compressor.decompress(d)
+                    if compressor.iscompressed(d)
+                    else d
+                )(self.compressor, d)
+                for d in server_data
             ]
             return server_data
         return server_data
 
     @staticmethod
     def parse_gradients(gradients):
         if isinstance(gradients, List) and len(gradients) == 1:
@@ -306,45 +359,56 @@
             agg_hiddens = self.device_agg(
                 self.agg_method.forward, static_argnames="axis"
             )(*server_data, axis=axis, weights=weights)
 
             # send to device y
             agg_hiddens = agg_hiddens.to(self.device_y)
 
-            # TODO: This is not dead code, it will automatically take effect after agglayer supports sparse calculation. @juxing
-            if self.compressor:
-                agg_hiddens, fuse_sparse_masks, is_compressed = self.device_y(
-                    self.handle_sparse_hiddens,
-                    num_returns=3,
-                )([agg_hiddens], self.compressor)
-                self.fuse_sparse_masks = fuse_sparse_masks
-                self.is_compressed = is_compressed
+            # TODO: Supports sparse calculation and add compression from device_agg to device_y. #juxing
+            # if self.compressor:
+            #     agg_hiddens, fuse_sparse_masks, is_compressed = self.device_y(
+            #         self.decompress_hiddens,
+            #         num_returns=3,
+            #     )([agg_hiddens], self.compressor)
+            #     self.fuse_sparse_masks = fuse_sparse_masks
+            #     self.is_compressed = is_compressed
 
             # convert to tensor on device y
             agg_hidden_tensor = self.device_y(self.convert_to_tensor)(
                 agg_hiddens, self.backend
             )
 
             # make new ForwardData and return
             agg_forward_data = self.device_y(self.set_forward_data)(
                 agg_hidden_tensor, self.losses
             )
 
             return agg_forward_data
         else:
-            data = [datum.to(self.device_y) for datum in data.values()]
+            compute_data = []
+            for device in data:
+                working_data = data[device]
+                if self.compressor and device != self.device_y:
+                    working_data = device(self.compressor.compress)(working_data)
+                if device != self.device_y:
+                    working_data = working_data.to(self.device_y)
+                compute_data.append(working_data)
             if self.compressor:
-                data, fuse_sparse_masks, is_compressed = self.device_y(
-                    self.handle_sparse_hiddens,
+                # decompress if needed.
+                (
+                    compute_data,
+                    self.fuse_sparse_masks,
+                    self.is_compressed,
+                ) = self.device_y(
+                    self.decompress_hiddens,
                     num_returns=3,
-                )(data, self.compressor)
-                self.fuse_sparse_masks = fuse_sparse_masks
-                self.is_compressed = is_compressed
-
-            return data
+                )(
+                    compute_data, self.compressor
+                )
+            return compute_data
 
     def backward(
         self,
         gradient: DeviceObject,
         weights=None,
     ) -> Dict[PYU, DeviceObject]:
         """Backward split the gradients to all parties according to the agg_method
@@ -353,24 +417,26 @@
             gradient: Gradient, tensor format calculated from fusenet
             weights: Weights of each side, default to be none
         Returns:
             scatter_gragient: Return gradients computed following the agg_method.backward and send to each parties
         """
         assert gradient, 'gradient to aggregate should not be None or empty!'
         if self.agg_method:
-            if self.compressor:
-                gradient = self.device_y(self.handle_sparse_gradients)(
-                    gradient,
-                    self.fuse_sparse_masks,
-                    self.compressor,
-                    self.is_compressed,
-                )
+            # TODO: Supports sparse calculation and add compression from device_y to device_agg. #juxing
+            # if self.compressor:
+            #     gradient = self.device_y(self.compress_gradients)(
+            #         gradient,
+            #         self.fuse_sparse_masks,
+            #         self.compressor,
+            #         self.is_compressed,
+            #     )
 
             if isinstance(gradient, DeviceObject):
                 gradient = gradient.to(self.device_agg)
+
             if isinstance(weights, (Tuple, List)):
                 weights = [
                     w.to(self.device_agg) if isinstance(w, DeviceObject) else w
                     for w in weights
                 ]
             # convert to numpy
             gradient_numpy = self.device_agg(self.convert_to_ndarray)(gradient)
@@ -397,31 +463,41 @@
                     *gradient_numpy,
                     weights=weights,
                     inputs=self.hiddens,
                     parties_num=len(self.parties),
                 )
             scatter_g = self.scatter(p_gradient)
         else:
+            # default branch, input gradients is from fusenet, belong to device_y
             assert (
                 gradient.device == self.device_y
             ), "The device of gradients(PYUObject) must located on party device_y "
             if self.compressor:
-                gradient = self.device_y(self.handle_sparse_gradients)(
+                # Compress if needed (same device will be passed)
+                gradient = self.device_y(self.compress_gradients)(
                     gradient,
                     self.fuse_sparse_masks,
                     self.compressor,
                     self.is_compressed,
                 )
+
+            # split gradients to parties by index
             p_gradient = self.device_y(
                 self.split_to_parties,
                 num_returns=len(self.parties),
             )(
                 gradient,
             )
+
+            # handle single feature mode
             if isinstance(p_gradient, PYUObject):
-                p_gradient = self.device_agg(self.parse_gradients)(p_gradient)
+                p_gradient = self.device_y(self.parse_gradients)(p_gradient)
                 p_gradient = [p_gradient]
+
             scatter_g = {}
-            for p, g in zip(self.parties, p_gradient):
-                p_g = g.to(p)
-                scatter_g[p] = p_g
+            for device, gradient in zip(self.parties, p_gradient):
+                if device != self.device_y:
+                    gradient = gradient.to(device)
+                if self.compressor and device != self.device_y:
+                    gradient = device(self.compressor.decompress)(gradient)
+                scatter_g[device] = gradient
         return scatter_g
```

## secretflow/ml/nn/sl/backend/tensorflow/sl_base.py

```diff
@@ -17,15 +17,15 @@
 
 
 """sl model base
 """
 import copy
 from abc import ABC, abstractmethod
 from pathlib import Path
-from typing import Callable, Dict, List, Optional, Tuple
+from typing import Callable, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 import pandas as pd
 import tensorflow as tf
 from tensorflow.python.keras import callbacks as callbacks_module
 
 from secretflow.device import PYUObject, proxy
@@ -72,21 +72,24 @@
         self.eval_set = None
         self.valid_set = None
         self.tape = None
         self.h = None
         self.train_x, self.train_y = None, None
         self.eval_x, self.eval_y = None, None
         self.kwargs = {}
+        self.train_has_x = False
         self.train_has_y = False
         self.train_has_s_w = False
+        self.eval_has_x = False
         self.eval_has_y = False
         self.eval_has_s_w = False
         self.train_sample_weight = None
         self.eval_sample_weight = None
         self.fuse_callbacks = None
+        self.predict_callbacks = None
         self.logs = None
         self.epoch_logs = None
         self.training_logs = None
         self.steps_per_epoch = None
         self.skip_gradient = False
         if random_seed is not None:
             tf.keras.utils.set_random_seed(random_seed)
@@ -147,38 +150,51 @@
             batch_size: Number of samples per gradient update
             buffer_size: buffer size for shuffling
             shuffle: whether shuffle the dataset or not
             repeat_count: num of repeats
             stage: stage of this datset
             random_seed: Prg seed for shuffling
         """
-        assert x and x[0] is not None, "X can not be None, please check"
-        x = [xi for xi in x]
+        data_tuple = []
+        has_x = False
+        if x is not None:
+            has_x = True
+            x = [xi for xi in x]
+            data_tuple.extend(x)
         has_y = False
         has_s_w = False
         if y is not None and len(y.shape) > 0:
             has_y = True
-            x.append(y)
+            data_tuple.append(y)
             if s_w is not None and len(s_w.shape) > 0:
                 has_s_w = True
-                x.append(s_w)
+                data_tuple.append(s_w)
 
         # convert pandas.DataFrame to numpy.ndarray
-        x = [t.values if isinstance(t, pd.DataFrame) else t for t in x]
+
+        data_tuple = [
+            t.values if isinstance(t, pd.DataFrame) else t for t in data_tuple
+        ]
         # https://github.com/tensorflow/tensorflow/issues/20481
-        x = x[0] if len(x) == 1 else tuple(x)
+        data_tuple = data_tuple[0] if len(data_tuple) == 1 else tuple(data_tuple)
 
         data_set = (
-            tf.data.Dataset.from_tensor_slices(x).batch(batch_size).repeat(repeat_count)
+            tf.data.Dataset.from_tensor_slices(data_tuple)
+            .batch(batch_size)
+            .repeat(repeat_count)
         )
         if shuffle:
             data_set = data_set.shuffle(buffer_size, seed=random_seed)
 
         self.set_dataset_stage(
-            data_set=data_set, stage=stage, has_y=has_y, has_s_w=has_s_w
+            data_set=data_set,
+            stage=stage,
+            has_x=has_x,
+            has_y=has_y,
+            has_s_w=has_s_w,
         )
 
     def build_dataset_from_builder(
         self,
         *x: List[np.ndarray],
         y: Optional[np.ndarray] = None,
         s_w: Optional[np.ndarray] = None,
@@ -198,26 +214,35 @@
             batch_size: Number of samples per gradient update.
             shuffle: Whether to shuffle dataset
             buffer_size: buffer size for shuffling
             random_seed: Prg seed for shuffling
             stage: stage of this datset
             dataset_builder: dataset build callable function of worker
         """
-        assert x and x[0] is not None, "X can not be None, please check"
-        x = [xi for xi in x]
+        if not dataset_builder:
+            return -1
+        data_tuple = []
+        has_x = False
+
+        #  x is (None,) if dont have feature
+        if x[0] is not None:
+            has_x = True
+            x = [xi for xi in x]
+            data_tuple.extend(x)
+
         has_y = False
         has_s_w = False
         if y is not None and len(y.shape) > 0:
             has_y = True
-            x.append(y)
+            data_tuple.append(y)
             if s_w is not None and len(s_w.shape) > 0:
                 has_s_w = True
-                x.append(s_w)
+                data_tuple.append(s_w)
 
-        data_set = dataset_builder(x)
+        data_set = dataset_builder(data_tuple)
         # Compatible with existing gnn databuilder
         if hasattr(data_set, 'steps_per_epoch'):
             return data_set.steps_per_epoch
 
         if shuffle:
             data_set = data_set.shuffle(buffer_size, seed=random_seed)
         # Infer batch size
@@ -239,46 +264,50 @@
             raise Exception(
                 f"Unable to get batchsize from dataset, please spcify batchsize in 'fit'"
             )
 
         self.set_dataset_stage(
             data_set=data_set,
             stage=stage,
+            has_x=has_x,
             has_y=has_y,
             has_s_w=has_s_w,
         )
         if isinstance(data_set, tf.data.Dataset):
             import math
 
             return math.ceil(len(x[0]) / batch_size)  # use ceil to avoid drop_last
         else:
             raise Exception("Unknown databuilder")
 
     def set_dataset_stage(
         self,
         data_set,
         stage="train",
+        has_x=None,
         has_y=None,
         has_s_w=None,
     ):
         data_set = iter(data_set)
         if stage == "train":
             self.train_set = data_set
+            self.train_has_x = has_x
             self.train_has_y = has_y
             self.train_has_s_w = has_s_w
         elif stage == "eval":
             self.eval_set = data_set
+            self.eval_has_x = has_x
             self.eval_has_y = has_y
             self.eval_has_s_w = has_s_w
         else:
             raise Exception(f"Illegal argument stage={stage}")
 
     @tf.function
-    def _base_forward_internal(self, data_x):
-        h = self.model_base(data_x)
+    def _base_forward_internal(self, data_x, training=True):
+        h = self.model_base(data_x, training=training)
 
         # Embedding differential privacy
         if self.embedding_dp is not None:
             if isinstance(h, List):
                 h = [self.embedding_dp(hi) for hi in h]
             else:
                 h = self.embedding_dp(h)
@@ -288,68 +317,77 @@
         """compute hidden embedding
         Args:
             stage: Which stage of the base forward
         Returns: hidden embedding
         """
         data_x = None
         self.init_data()
+        training = True
         if stage == "train":
             train_data = next(self.train_set)
             if self.train_has_y:
                 if self.train_has_s_w:
-                    data_x = train_data[:-2]
+                    data_x = train_data[:-2] if self.train_has_x else None
                     train_y = train_data[-2]
                     self.train_sample_weight = train_data[-1]
                 else:
-                    data_x = train_data[:-1]
+                    data_x = train_data[:-1] if self.train_has_x else None
                     train_y = train_data[-1]
                 # Label differential privacy
                 if self.label_dp is not None:
                     dp_train_y = self.label_dp(train_y.numpy())
                     self.train_y = tf.convert_to_tensor(dp_train_y)
                 else:
                     self.train_y = train_y
             else:
-                data_x = train_data
+                if self.train_has_x:
+                    data_x = train_data
+                else:
+                    raise Exception("x,y cannot be none at same time!")
         elif stage == "eval":
+            training = False
             eval_data = next(self.eval_set)
             if self.eval_has_y:
                 if self.eval_has_s_w:
-                    data_x = eval_data[:-2]
+                    data_x = eval_data[:-2] if self.eval_has_x else None
                     eval_y = eval_data[-2]
                     self.eval_sample_weight = eval_data[-1]
                 else:
-                    data_x = eval_data[:-1]
+                    data_x = eval_data[:-1] if self.eval_has_x else None
                     eval_y = eval_data[-1]
                 # Label differential privacy
                 if self.label_dp is not None:
                     dp_eval_y = self.label_dp(eval_y.numpy())
                     self.eval_y = tf.convert_to_tensor(dp_eval_y)
                 else:
                     self.eval_y = eval_y
             else:
-                data_x = eval_data
+                if self.eval_has_x:
+                    data_x = eval_data
+                else:
+                    raise Exception("x,y cannot be none at same time!")
         else:
             raise Exception("invalid stage")
 
-        # Strip tuple of length one, e.g: (x,) -> x
-        data_x = data_x[0] if isinstance(data_x, Tuple) and len(data_x) == 1 else data_x
+        # model_base is none equal to x is none
         if not self.model_base:
             return None
+
+        # Strip tuple of length one, e.g: (x,) -> x
+        data_x = data_x[0] if isinstance(data_x, Tuple) and len(data_x) == 1 else data_x
         self.tape = tf.GradientTape(persistent=True)
         with self.tape:
-            self.h = self._base_forward_internal(
-                data_x,
-            )
+            self.h = self._base_forward_internal(data_x, training=training)
         self.data_x = data_x
 
         forward_data = ForwardData()
         if len(self.model_base.losses) > 0:
             forward_data.losses = tf.add_n(self.model_base.losses)
-        forward_data.hidden = self.h
+        # The compressor can only recognize np type but not tensor.
+        forward_data.hidden = self.h.numpy() if tf.is_tensor(self.h) else self.h
         return forward_data
 
     def _base_backward_internal(self, gradients, trainable_vars):
         self.model_base.optimizer.apply_gradients(zip(gradients, trainable_vars))
 
     def base_backward(self, gradient):
         """backward on fusenet
@@ -400,14 +438,28 @@
                 epochs=epochs,
                 steps=steps,
                 steps_per_epoch=self.steps_per_epoch,
             )
         else:
             raise NotImplementedError
 
+    def init_predict(self, callbacks, steps=1, verbose=0):
+        if not isinstance(callbacks, callbacks_module.CallbackList):
+            self.predict_callbacks = callbacks_module.CallbackList(
+                callbacks,
+                add_history=True,
+                add_progbar=verbose != 0,
+                model=self.model_base,
+                verbose=verbose,
+                epochs=1,
+                steps=steps,
+            )
+        else:
+            raise NotImplementedError
+
     def get_stop_training(self):
         return self.model_fuse.stop_training
 
     def on_train_begin(self):
         if self.fuse_callbacks:
             self.fuse_callbacks.on_train_begin()
 
@@ -435,41 +487,61 @@
             self.fuse_callbacks.on_epoch_end(epoch, self.epoch_logs)
         self.training_logs = self.epoch_logs
         return self.epoch_logs
 
     def on_train_end(self):
         if self.fuse_callbacks:
             self.fuse_callbacks.on_train_end(logs=self.training_logs)
-        return self.model_fuse.history.history
+        if self.model_fuse is not None:
+            return self.model_fuse.history.history
+        return None
+
+    def on_predict_batch_begin(self, batch):
+        if self.predict_callbacks:
+            self.predict_callbacks.on_predict_batch_begin(batch)
+
+    def on_predict_batch_end(self, batch):
+        if self.predict_callbacks:
+            self.predict_callbacks.on_predict_batch_end(batch)
+
+    def on_predict_begin(self):
+        if self.predict_callbacks:
+            self.predict_callbacks.on_predict_begin()
+
+    def on_predict_end(self):
+        if self.predict_callbacks:
+            self.predict_callbacks.on_predict_end()
 
     def set_sample_weight(self, sample_weight, stage="train"):
         if stage == "train":
             self.train_sample_weight = sample_weight
         elif stage == "eval":
             self.eval_sample_weight = sample_weight
         else:
             raise Exception("Illegal Argument")
 
     def fuse_net(
         self,
-        *forward_data: List[ForwardData],
+        forward_data: Union[List[ForwardData], ForwardData],
         _num_returns: int = 2,
     ):
         """Fuses the hidden layer and calculates the reverse gradient
         only on the side with the label
 
         Args:
             forward_data: A list of ForwardData containing hidden layers, losses, etc.
                 that are uploaded by each party for computation.
         Returns:
             gradient Of hiddens
         """
         assert (
             self.model_fuse is not None
         ), "Fuse model cannot be none, please give model define"
+        if isinstance(forward_data, ForwardData):
+            forward_data = [forward_data]
         forward_data = list(forward_data)
         forward_data[:] = (h for h in forward_data if h is not None)
         for i, h in enumerate(forward_data):
             assert h.hidden is not None, f"hidden cannot be found in forward_data[{i}]"
             if isinstance(h.losses, List) and h.losses[0] is None:
                 h.losses = None
         # get reg losses:
@@ -559,27 +631,29 @@
         )
 
         result = {}
         for m in self.model_fuse.metrics:
             result[m.name] = m.result()
         return result
 
-    def evaluate(self, *forward_data: List[ForwardData]):
+    def evaluate(self, forward_data: Union[List[ForwardData], ForwardData]):
         """Returns the loss value & metrics values for the model in test mode.
 
         Args:
             forward_data: A list of data dictionaries containing hidden layers, losses, etc.
                 that are uploaded by each party for computation.
         Returns:
             map of model metrics.
         """
 
         assert (
             self.model_fuse is not None
         ), "model cannot be none, please give model define"
+        if isinstance(forward_data, ForwardData):
+            forward_data = [forward_data]
         forward_data = list(forward_data)
         forward_data[:] = (h for h in forward_data if h is not None)
         for i, h in enumerate(forward_data):
             assert h.hidden is not None, f"hidden cannot be found in forward_data[{i}]"
             if isinstance(h.losses, List) and h.losses[0] is None:
                 h.losses = None
         # get reg losses:
@@ -645,29 +719,31 @@
         return wraped_metrics
 
     def metrics(self):
         return self.wrap_local_metrics()
 
     @tf.function
     def _predict_internal(self, hiddens):
-        y_pred = self.model_fuse(hiddens)
+        y_pred = self.model_fuse(hiddens, training=False)
         return y_pred
 
-    def predict(self, *forward_data: List[ForwardData]):
+    def predict(self, forward_data: Union[List[ForwardData], ForwardData]):
         """Generates output predictions for the input hidden layer features.
 
         Args:
             forward_data: A list of data dictionaries containing hidden layers,
                 that are uploaded by each party for computation.
         Returns:
             Array(s) of predictions.
         """
         assert (
             self.model_fuse is not None
         ), "Fuse model cannot be none, please give model define"
+        if isinstance(forward_data, ForwardData):
+            forward_data = [forward_data]
         forward_data = list(forward_data)
         forward_data[:] = (h for h in forward_data if h is not None)
         for i, h in enumerate(forward_data):
             assert h.hidden is not None, f"hidden cannot be found in forward_data[{i}]"
             if isinstance(h.losses, List) and h.losses[0] is None:
                 h.losses = None
         hidden_features = [h.hidden for h in forward_data]
```

## secretflow/ml/nn/sl/backend/tensorflow/strategy/__init__.py

```diff
@@ -10,12 +10,14 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from .split_async import PYUSLAsyncTFModel
 from .split_state_async import PYUSLStateAsyncTFModel
+from .pipeline import PYUPipelineTFModel
 
 __all__ = [
     "PYUSLAsyncTFModel",
     "PYUSLStateAsyncTFModel",
+    "PYUPipelineTFModel",
 ]
```

## secretflow/ml/nn/sl/backend/tensorflow/strategy/split_async.py

```diff
@@ -51,16 +51,16 @@
             **kwargs,
         )
         self.base_local_steps = base_local_steps
         self.fuse_local_steps = fuse_local_steps
         self.bound_param = bound_param
 
     @tf.function
-    def _base_forward_internal(self, data_x, use_dp: bool = True):
-        h = self.model_base(data_x)
+    def _base_forward_internal(self, data_x, use_dp: bool = True, training=True):
+        h = self.model_base(data_x, training=training)
 
         # Embedding differential privacy
         if use_dp and self.embedding_dp is not None:
             if isinstance(h, List):
                 h = [self.embedding_dp(hi) for hi in h]
             else:
                 h = self.embedding_dp(h)
@@ -75,15 +75,19 @@
 
         for local_step in range(self.base_local_steps):
             return_hiddens = []
             with self.tape:
                 if local_step == 0 and self.h is not None:
                     h = self.h
                 else:
-                    h = self._base_forward_internal(self.data_x, use_dp=False)
+                    h = self._base_forward_internal(
+                        self.data_x,
+                        use_dp=False,
+                        training=True,  # backward will only in training procedure
+                    )
                 if len(gradient) == len(h):
                     for i in range(len(gradient)):
                         return_hiddens.append(self.fuse_op(h[i], gradient[i]))
                 else:
                     gradient = gradient[0]
                     return_hiddens.append(self.fuse_op(h, gradient))
                 # add model.losses into graph
```

## secretflow/ml/nn/sl/backend/torch/sl_base.py

```diff
@@ -130,16 +130,18 @@
         self.eval_sample_weight = None
         self.fuse_callbacks = None
         self.logs = None
         self.epoch_logs = None
         self.training_logs = None
         self.history = {}
         self.steps_per_epoch = None
+        self.shuffle = False
         if random_seed is not None:
             torch.manual_seed(random_seed)
+            self.random_seed = random_seed
         # used in backward propagation gradients from fuse model to base model
         self.fuse_op = FuseOp()
         super().__init__(builder_base, builder_fuse)
 
     def init_data(self):
         self.train_x, self.train_y = None, None
         self.eval_x, self.eval_y = None, None
@@ -245,14 +247,15 @@
             repeat_count: num of repeats
             stage: stage of this datset
             random_seed: Prg seed for shuffling
         """
         assert x and x[0] is not None, "X can not be None, please check"
 
         if shuffle and random_seed is not None:
+            self.shuffle = shuffle
             random.seed(random_seed)
             torch.manual_seed(random_seed)  # set random seed for cpu
             torch.cuda.manual_seed(random_seed)  # set random seed for cuda
             torch.backends.cudnn.deterministic = True
 
         x = [xi for xi in x]
 
@@ -282,16 +285,21 @@
             has_s_w=has_s_w,
         )
 
     def init_training(self, callbacks, epochs=1, steps=0, verbose=0):
         assert (
             self.model_base is not None
         ), "model cannot be none, please give model define"
+
         if callbacks is not None:
-            raise Exception("Callback is not supported yet")
+            self.fuse_callbacks = callbacks()
+            self.fuse_callbacks.init_model(self.model_base, self.model_fuse)
+
+    def init_predict(self, callbacks, steps=1, verbose=0):
+        pass
 
     def build_dataset_from_builder(
         self,
         *x: List[np.ndarray],
         y: Optional[np.ndarray] = None,
         s_w: Optional[np.ndarray] = None,
         batch_size=-1,
@@ -457,50 +465,81 @@
     def get_stop_training(self):
         return False  # currently not supported
 
     def on_train_begin(self):
         self.training_logs = {}
         self.epoch = []
 
+        if self.fuse_callbacks is not None:
+            self.fuse_callbacks.on_train_begin()
+
     def on_train_end(self):
+        if self.fuse_callbacks is not None:
+            self.fuse_callbacks.on_train_end()
+
         return self.history
 
     def on_epoch_begin(self, epoch):
+        if self.shuffle:
+            # FIXME: need a better way to handle global random state
+            torch.manual_seed(self.random_seed)
         if self.train_set is not None:
             self.train_iter = iter(self.train_set)
 
         if self.eval_set is not None:
             self.eval_iter = iter(self.eval_set)
 
         self._current_epoch = epoch
         if self.model_fuse is not None:
             self.epoch_logs = {}
             for m in self.metrics_fuse:
                 m.reset()
 
+        if self.fuse_callbacks is not None:
+            self.fuse_callbacks.on_epoch_begin(epoch)
+
     def on_epoch_end(self, epoch):
         self.epoch.append(epoch)
 
+        if self.fuse_callbacks is not None:
+            self.fuse_callbacks.on_epoch_end(epoch)
+
         if self.epoch_logs is not None:
             for k, v in self.epoch_logs.items():
                 self.history.setdefault(k, []).append(v)
             self.training_logs = self.epoch_logs
             return self.epoch_logs
 
     def on_train_batch_begin(self, step=None):
         assert step is not None, "Step cannot be none"
+        if self.fuse_callbacks is not None:
+            self.fuse_callbacks.on_batch_begin(step)
 
     def on_train_batch_end(self, step=None):
         assert step is not None, "Step cannot be none"
         self.epoch_logs = copy.deepcopy(self.logs)
+        if self.fuse_callbacks is not None:
+            self.fuse_callbacks.on_batch_end(step)
 
     def on_validation(self, val_logs):
         val_logs = {'val_' + name: val for name, val in val_logs.items()}
         self.epoch_logs.update(val_logs)
 
+    def on_predict_batch_begin(self, batch):
+        assert batch is not None, "Batch cannot be none"
+
+    def on_predict_batch_end(self, batch):
+        assert batch is not None, "Batch cannot be none"
+
+    def on_predict_begin(self):
+        pass
+
+    def on_predict_end(self):
+        pass
+
     def set_sample_weight(self, sample_weight, stage="train"):
         if stage == "train":
             self.train_sample_weight = sample_weight
         elif stage == "eval":
             self.eval_sample_weight = sample_weight
         else:
             raise Exception("Illegal Argument")
@@ -524,15 +563,17 @@
             y_pred,
             eval_y,
         )
         logs["val_loss"] = loss.detach().numpy()
 
         # Step 3: update metrics
         for m in self.metrics_fuse:
-            if len(eval_y.shape) > 1 and eval_y.shape[1] > 1:  # in case eval_y is of shape [batch_size, 1]
+            if (
+                len(eval_y.shape) > 1 and eval_y.shape[1] > 1
+            ):  # in case eval_y is of shape [batch_size, 1]
                 m.update(y_pred, eval_y.argmax(-1))
             else:
                 m.update(y_pred, eval_y.int())
 
         result = {}
         for m in self.metrics_fuse:
             result[m.__class__.__name__] = m.compute()
@@ -549,19 +590,26 @@
 
         assert (
             self.model_fuse is not None
         ), "model cannot be none, please give model define"
 
         hiddens = []
         for h in hidden_features:
+            # the evaluate function only recognizes torch.tensor, but inputs may be np type via compression.
             if isinstance(h, List):
-                for i in range(len(h)):
-                    hiddens.append(h[i])
+                for e in h:
+                    hiddens.append(
+                        torch.as_tensor(e)
+                        if isinstance(e, (np.matrix, np.ndarray))
+                        else e
+                    )
             else:
-                hiddens.append(h)
+                hiddens.append(torch.as_tensor(h)) if isinstance(
+                    h, (np.matrix, np.ndarray)
+                ) else hiddens.append(h)
         result = {}
         metrics = self._evaluate_internal(
             hiddens=hiddens,
             eval_y=self.eval_y,
             eval_sample_weight=self.eval_sample_weight,
             logs=result,
         )
```

## secretflow/ml/nn/sl/backend/torch/strategy/split_nn.py

```diff
@@ -41,16 +41,16 @@
             self.model_base is not None
         ), "Base model cannot be none, please give model define or load a trained model"
 
         data_x = self.get_batch_data(stage=stage)
         self.h = self.base_forward_internal(
             data_x,
         )
-
-        return self.h
+        # The compressor in forward can only recognize np type but not tensor.
+        return self.h.detach().numpy() if isinstance(self.h, torch.Tensor) else self.h
 
     def base_backward(self, gradient):
         """backward on fusenet
 
         Args:
             gradient: gradient of fusenet hidden layer
         """
@@ -67,15 +67,16 @@
                 else torch.tensor(gradient[0])
             )
             return_hiddens.append(self.fuse_op.apply(self.h, gradient))
 
         # apply gradients for base net
         self.optim_base.zero_grad()
         for rh in return_hiddens:
-            rh.sum().backward(retain_graph=True)
+            if rh.requires_grad:
+                rh.sum().backward(retain_graph=True)
         self.optim_base.step()
 
         # clear intermediate results
         self.tape = None
         self.h = None
         self.kwargs = {}
```

## secretflow/protos/component/cluster_pb2.py

```diff
@@ -13,17 +13,17 @@
 
 
 
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='secretflow/protos/component/cluster.proto',
   package='secretflow.component',
   syntax='proto3',
-  serialized_options=None,
+  serialized_options=b'\n\036org.secretflow.proto.component',
   create_key=_descriptor._internal_create_key,
-  serialized_pb=b'\n)secretflow/protos/component/cluster.proto\x12\x14secretflow.component\"\xd4\x01\n\rSFClusterDesc\x12\x12\n\nsf_version\x18\x01 \x01(\t\x12\x12\n\npy_version\x18\x02 \x01(\t\x12\x0f\n\x07parties\x18\x03 \x03(\t\x12?\n\x07\x64\x65vices\x18\x04 \x03(\x0b\x32..secretflow.component.SFClusterDesc.DeviceDesc\x1aI\n\nDeviceDesc\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04type\x18\x02 \x01(\t\x12\x0f\n\x07parties\x18\x03 \x03(\t\x12\x0e\n\x06\x63onfig\x18\x04 \x01(\t\"\x7f\n\rStorageConfig\x12\x0c\n\x04type\x18\x01 \x01(\t\x12\x43\n\x08local_fs\x18\x02 \x01(\x0b\x32\x31.secretflow.component.StorageConfig.LocalFSConfig\x1a\x1b\n\rLocalFSConfig\x12\n\n\x02wd\x18\x01 \x01(\t\"\x9e\x05\n\x0fSFClusterConfig\x12\x31\n\x04\x64\x65sc\x18\x01 \x01(\x0b\x32#.secretflow.component.SFClusterDesc\x12I\n\rpublic_config\x18\x02 \x01(\x0b\x32\x32.secretflow.component.SFClusterConfig.PublicConfig\x12K\n\x0eprivate_config\x18\x03 \x01(\x0b\x32\x33.secretflow.component.SFClusterConfig.PrivateConfig\x1aL\n\x0cRayFedConfig\x12\x0f\n\x07parties\x18\x01 \x03(\t\x12\x11\n\taddresses\x18\x02 \x03(\t\x12\x18\n\x10listen_addresses\x18\x03 \x03(\t\x1aW\n\tSPUConfig\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07parties\x18\x02 \x03(\t\x12\x11\n\taddresses\x18\x03 \x03(\t\x12\x18\n\x10listen_addresses\x18\x04 \x03(\t\x1a\x9f\x01\n\x0cPublicConfig\x12I\n\rrayfed_config\x18\x01 \x01(\x0b\x32\x32.secretflow.component.SFClusterConfig.RayFedConfig\x12\x44\n\x0bspu_configs\x18\x02 \x03(\x0b\x32/.secretflow.component.SFClusterConfig.SPUConfig\x1aw\n\rPrivateConfig\x12\x12\n\nself_party\x18\x01 \x01(\t\x12\x15\n\rray_head_addr\x18\x02 \x01(\t\x12;\n\x0estorage_config\x18\x03 \x01(\x0b\x32#.secretflow.component.StorageConfigb\x06proto3'
+  serialized_pb=b'\n)secretflow/protos/component/cluster.proto\x12\x14secretflow.component\"\xd4\x01\n\rSFClusterDesc\x12\x12\n\nsf_version\x18\x01 \x01(\t\x12\x12\n\npy_version\x18\x02 \x01(\t\x12\x0f\n\x07parties\x18\x03 \x03(\t\x12?\n\x07\x64\x65vices\x18\x04 \x03(\x0b\x32..secretflow.component.SFClusterDesc.DeviceDesc\x1aI\n\nDeviceDesc\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04type\x18\x02 \x01(\t\x12\x0f\n\x07parties\x18\x03 \x03(\t\x12\x0e\n\x06\x63onfig\x18\x04 \x01(\t\"\x7f\n\rStorageConfig\x12\x0c\n\x04type\x18\x01 \x01(\t\x12\x43\n\x08local_fs\x18\x02 \x01(\x0b\x32\x31.secretflow.component.StorageConfig.LocalFSConfig\x1a\x1b\n\rLocalFSConfig\x12\n\n\x02wd\x18\x01 \x01(\t\"\x9e\x05\n\x0fSFClusterConfig\x12\x31\n\x04\x64\x65sc\x18\x01 \x01(\x0b\x32#.secretflow.component.SFClusterDesc\x12I\n\rpublic_config\x18\x02 \x01(\x0b\x32\x32.secretflow.component.SFClusterConfig.PublicConfig\x12K\n\x0eprivate_config\x18\x03 \x01(\x0b\x32\x33.secretflow.component.SFClusterConfig.PrivateConfig\x1aL\n\x0cRayFedConfig\x12\x0f\n\x07parties\x18\x01 \x03(\t\x12\x11\n\taddresses\x18\x02 \x03(\t\x12\x18\n\x10listen_addresses\x18\x03 \x03(\t\x1aW\n\tSPUConfig\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07parties\x18\x02 \x03(\t\x12\x11\n\taddresses\x18\x03 \x03(\t\x12\x18\n\x10listen_addresses\x18\x04 \x03(\t\x1a\x9f\x01\n\x0cPublicConfig\x12I\n\rrayfed_config\x18\x01 \x01(\x0b\x32\x32.secretflow.component.SFClusterConfig.RayFedConfig\x12\x44\n\x0bspu_configs\x18\x02 \x03(\x0b\x32/.secretflow.component.SFClusterConfig.SPUConfig\x1aw\n\rPrivateConfig\x12\x12\n\nself_party\x18\x01 \x01(\t\x12\x15\n\rray_head_addr\x18\x02 \x01(\t\x12;\n\x0estorage_config\x18\x03 \x01(\x0b\x32#.secretflow.component.StorageConfigB \n\x1eorg.secretflow.proto.componentb\x06proto3'
 )
 
 
 
 
 _SFCLUSTERDESC_DEVICEDESC = _descriptor.Descriptor(
   name='DeviceDesc',
@@ -510,8 +510,9 @@
 _sym_db.RegisterMessage(SFClusterConfig)
 _sym_db.RegisterMessage(SFClusterConfig.RayFedConfig)
 _sym_db.RegisterMessage(SFClusterConfig.SPUConfig)
 _sym_db.RegisterMessage(SFClusterConfig.PublicConfig)
 _sym_db.RegisterMessage(SFClusterConfig.PrivateConfig)
 
 
+DESCRIPTOR._options = None
 # @@protoc_insertion_point(module_scope)
```

## secretflow/protos/component/comp_pb2.py

```diff
@@ -14,17 +14,17 @@
 
 
 
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='secretflow/protos/component/comp.proto',
   package='secretflow.component',
   syntax='proto3',
-  serialized_options=None,
+  serialized_options=b'\n\036org.secretflow.proto.component',
   create_key=_descriptor._internal_create_key,
-  serialized_pb=b'\n&secretflow/protos/component/comp.proto\x12\x14secretflow.component\"z\n\tAttribute\x12\t\n\x01\x66\x18\x01 \x01(\x02\x12\x0b\n\x03i64\x18\x02 \x01(\x03\x12\t\n\x01s\x18\x03 \x01(\t\x12\t\n\x01\x62\x18\x04 \x01(\x08\x12\n\n\x02\x66s\x18\x05 \x03(\x02\x12\x0c\n\x04i64s\x18\x06 \x03(\x03\x12\n\n\x02ss\x18\x07 \x03(\t\x12\n\n\x02\x62s\x18\x08 \x03(\x08\x12\r\n\x05is_na\x18\t \x01(\x08\"\xdf\x05\n\x0c\x41ttributeDef\x12\x10\n\x08prefixes\x18\x01 \x03(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x03 \x01(\t\x12,\n\x04type\x18\x04 \x01(\x0e\x32\x1e.secretflow.component.AttrType\x12\x41\n\x06\x61tomic\x18\x05 \x01(\x0b\x32\x31.secretflow.component.AttributeDef.AtomicAttrDesc\x12\x44\n\x05union\x18\x06 \x01(\x0b\x32\x35.secretflow.component.AttributeDef.UnionAttrGroupDesc\x1a\xb8\x03\n\x0e\x41tomicAttrDesc\x12!\n\x19list_min_length_inclusive\x18\x01 \x01(\x03\x12!\n\x19list_max_length_inclusive\x18\x02 \x01(\x03\x12\x13\n\x0bis_optional\x18\x03 \x01(\x08\x12\x36\n\rdefault_value\x18\x04 \x01(\x0b\x32\x1f.secretflow.component.Attribute\x12\x37\n\x0e\x61llowed_values\x18\x05 \x01(\x0b\x32\x1f.secretflow.component.Attribute\x12\x17\n\x0fhas_lower_bound\x18\x06 \x01(\x08\x12\x34\n\x0blower_bound\x18\x07 \x01(\x0b\x32\x1f.secretflow.component.Attribute\x12\x1d\n\x15lower_bound_inclusive\x18\x08 \x01(\x08\x12\x17\n\x0fhas_upper_bound\x18\t \x01(\x08\x12\x34\n\x0bupper_bound\x18\n \x01(\x0b\x32\x1f.secretflow.component.Attribute\x12\x1d\n\x15upper_bound_inclusive\x18\x0b \x01(\x08\x1a/\n\x12UnionAttrGroupDesc\x12\x19\n\x11\x64\x65\x66\x61ult_selection\x18\x01 \x01(\t\"\x98\x02\n\x05IoDef\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\r\n\x05types\x18\x03 \x03(\t\x12\x37\n\x05\x61ttrs\x18\x04 \x03(\x0b\x32(.secretflow.component.IoDef.TableAttrDef\x1a\xaa\x01\n\x0cTableAttrDef\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\r\n\x05types\x18\x03 \x03(\t\x12\x1d\n\x15\x63ol_min_cnt_inclusive\x18\x04 \x01(\x03\x12\x1d\n\x15\x63ol_max_cnt_inclusive\x18\x05 \x01(\x03\x12\x31\n\x05\x61ttrs\x18\x06 \x03(\x0b\x32\".secretflow.component.AttributeDef\"\xd9\x01\n\x0c\x43omponentDef\x12\x0e\n\x06\x64omain\x18\x01 \x01(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x03 \x01(\t\x12\x0f\n\x07version\x18\x04 \x01(\t\x12\x31\n\x05\x61ttrs\x18\x05 \x03(\x0b\x32\".secretflow.component.AttributeDef\x12+\n\x06inputs\x18\x06 \x03(\x0b\x32\x1b.secretflow.component.IoDef\x12,\n\x07outputs\x18\x07 \x03(\x0b\x32\x1b.secretflow.component.IoDef\"m\n\x0b\x43ompListDef\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\x0f\n\x07version\x18\x03 \x01(\t\x12\x31\n\x05\x63omps\x18\x04 \x03(\x0b\x32\".secretflow.component.ComponentDef*\xca\x01\n\x08\x41ttrType\x12\x10\n\x0c\x41T_UNDEFINED\x10\x00\x12\x0c\n\x08\x41T_FLOAT\x10\x01\x12\n\n\x06\x41T_INT\x10\x02\x12\r\n\tAT_STRING\x10\x03\x12\x0b\n\x07\x41T_BOOL\x10\x04\x12\r\n\tAT_FLOATS\x10\x05\x12\x0b\n\x07\x41T_INTS\x10\x06\x12\x0e\n\nAT_STRINGS\x10\x07\x12\x0c\n\x08\x41T_BOOLS\x10\x08\x12\x13\n\x0f\x41T_STRUCT_GROUP\x10\t\x12\x12\n\x0e\x41T_UNION_GROUP\x10\n\x12\x13\n\x0f\x41T_SF_TABLE_COL\x10\x0b\x62\x06proto3'
+  serialized_pb=b'\n&secretflow/protos/component/comp.proto\x12\x14secretflow.component\"z\n\tAttribute\x12\t\n\x01\x66\x18\x01 \x01(\x02\x12\x0b\n\x03i64\x18\x02 \x01(\x03\x12\t\n\x01s\x18\x03 \x01(\t\x12\t\n\x01\x62\x18\x04 \x01(\x08\x12\n\n\x02\x66s\x18\x05 \x03(\x02\x12\x0c\n\x04i64s\x18\x06 \x03(\x03\x12\n\n\x02ss\x18\x07 \x03(\t\x12\n\n\x02\x62s\x18\x08 \x03(\x08\x12\r\n\x05is_na\x18\t \x01(\x08\"\xdf\x05\n\x0c\x41ttributeDef\x12\x10\n\x08prefixes\x18\x01 \x03(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x03 \x01(\t\x12,\n\x04type\x18\x04 \x01(\x0e\x32\x1e.secretflow.component.AttrType\x12\x41\n\x06\x61tomic\x18\x05 \x01(\x0b\x32\x31.secretflow.component.AttributeDef.AtomicAttrDesc\x12\x44\n\x05union\x18\x06 \x01(\x0b\x32\x35.secretflow.component.AttributeDef.UnionAttrGroupDesc\x1a\xb8\x03\n\x0e\x41tomicAttrDesc\x12!\n\x19list_min_length_inclusive\x18\x01 \x01(\x03\x12!\n\x19list_max_length_inclusive\x18\x02 \x01(\x03\x12\x13\n\x0bis_optional\x18\x03 \x01(\x08\x12\x36\n\rdefault_value\x18\x04 \x01(\x0b\x32\x1f.secretflow.component.Attribute\x12\x37\n\x0e\x61llowed_values\x18\x05 \x01(\x0b\x32\x1f.secretflow.component.Attribute\x12\x17\n\x0fhas_lower_bound\x18\x06 \x01(\x08\x12\x34\n\x0blower_bound\x18\x07 \x01(\x0b\x32\x1f.secretflow.component.Attribute\x12\x1d\n\x15lower_bound_inclusive\x18\x08 \x01(\x08\x12\x17\n\x0fhas_upper_bound\x18\t \x01(\x08\x12\x34\n\x0bupper_bound\x18\n \x01(\x0b\x32\x1f.secretflow.component.Attribute\x12\x1d\n\x15upper_bound_inclusive\x18\x0b \x01(\x08\x1a/\n\x12UnionAttrGroupDesc\x12\x19\n\x11\x64\x65\x66\x61ult_selection\x18\x01 \x01(\t\"\x98\x02\n\x05IoDef\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\r\n\x05types\x18\x03 \x03(\t\x12\x37\n\x05\x61ttrs\x18\x04 \x03(\x0b\x32(.secretflow.component.IoDef.TableAttrDef\x1a\xaa\x01\n\x0cTableAttrDef\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\r\n\x05types\x18\x03 \x03(\t\x12\x1d\n\x15\x63ol_min_cnt_inclusive\x18\x04 \x01(\x03\x12\x1d\n\x15\x63ol_max_cnt_inclusive\x18\x05 \x01(\x03\x12\x31\n\x05\x61ttrs\x18\x06 \x03(\x0b\x32\".secretflow.component.AttributeDef\"\xd9\x01\n\x0c\x43omponentDef\x12\x0e\n\x06\x64omain\x18\x01 \x01(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x03 \x01(\t\x12\x0f\n\x07version\x18\x04 \x01(\t\x12\x31\n\x05\x61ttrs\x18\x05 \x03(\x0b\x32\".secretflow.component.AttributeDef\x12+\n\x06inputs\x18\x06 \x03(\x0b\x32\x1b.secretflow.component.IoDef\x12,\n\x07outputs\x18\x07 \x03(\x0b\x32\x1b.secretflow.component.IoDef\"m\n\x0b\x43ompListDef\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\x0f\n\x07version\x18\x03 \x01(\t\x12\x31\n\x05\x63omps\x18\x04 \x03(\x0b\x32\".secretflow.component.ComponentDef*\xca\x01\n\x08\x41ttrType\x12\x10\n\x0c\x41T_UNDEFINED\x10\x00\x12\x0c\n\x08\x41T_FLOAT\x10\x01\x12\n\n\x06\x41T_INT\x10\x02\x12\r\n\tAT_STRING\x10\x03\x12\x0b\n\x07\x41T_BOOL\x10\x04\x12\r\n\tAT_FLOATS\x10\x05\x12\x0b\n\x07\x41T_INTS\x10\x06\x12\x0e\n\nAT_STRINGS\x10\x07\x12\x0c\n\x08\x41T_BOOLS\x10\x08\x12\x13\n\x0f\x41T_STRUCT_GROUP\x10\t\x12\x12\n\x0e\x41T_UNION_GROUP\x10\n\x12\x13\n\x0f\x41T_SF_TABLE_COL\x10\x0b\x42 \n\x1eorg.secretflow.proto.componentb\x06proto3'
 )
 
 _ATTRTYPE = _descriptor.EnumDescriptor(
   name='AttrType',
   full_name='secretflow.component.AttrType',
   filename=None,
   file=DESCRIPTOR,
@@ -726,8 +726,9 @@
   'DESCRIPTOR' : _COMPLISTDEF,
   '__module__' : 'secretflow.protos.component.comp_pb2'
   # @@protoc_insertion_point(class_scope:secretflow.component.CompListDef)
   })
 _sym_db.RegisterMessage(CompListDef)
 
 
+DESCRIPTOR._options = None
 # @@protoc_insertion_point(module_scope)
```

## secretflow/protos/component/data_pb2.py

```diff
@@ -15,17 +15,17 @@
 from secretflow.protos.component import cluster_pb2 as secretflow_dot_protos_dot_component_dot_cluster__pb2
 
 
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='secretflow/protos/component/data.proto',
   package='secretflow.component',
   syntax='proto3',
-  serialized_options=None,
+  serialized_options=b'\n\036org.secretflow.proto.component',
   create_key=_descriptor._internal_create_key,
-  serialized_pb=b'\n&secretflow/protos/component/data.proto\x12\x14secretflow.component\x1a\x19google/protobuf/any.proto\x1a)secretflow/protos/component/cluster.proto\"W\n\nSystemInfo\x12\x10\n\x08\x61pp_name\x18\x01 \x01(\t\x12\x37\n\nsecretflow\x18\x02 \x01(\x0b\x32#.secretflow.component.SFClusterDesc\"z\n\x0bTableSchema\x12\x0b\n\x03ids\x18\x01 \x03(\t\x12\x10\n\x08\x66\x65\x61tures\x18\x02 \x03(\t\x12\x0e\n\x06labels\x18\x03 \x03(\t\x12\x10\n\x08id_types\x18\x04 \x03(\t\x12\x15\n\rfeature_types\x18\x05 \x03(\t\x12\x13\n\x0blabel_types\x18\x06 \x03(\t\"V\n\rVerticalTable\x12\x32\n\x07schemas\x18\x01 \x03(\x0b\x32!.secretflow.component.TableSchema\x12\x11\n\tnum_lines\x18\x02 \x01(\x03\"W\n\x0fIndividualTable\x12\x31\n\x06schema\x18\x01 \x01(\x0b\x32!.secretflow.component.TableSchema\x12\x11\n\tnum_lines\x18\x02 \x01(\x03\"\xab\x01\n\x16\x44\x65viceObjectCollection\x12G\n\x04objs\x18\x01 \x03(\x0b\x32\x39.secretflow.component.DeviceObjectCollection.DeviceObject\x12\x13\n\x0bpublic_info\x18\x02 \x01(\t\x1a\x33\n\x0c\x44\x65viceObject\x12\x0c\n\x04type\x18\x01 \x01(\t\x12\x15\n\rdata_ref_idxs\x18\x02 \x03(\x05\"\xf0\x01\n\x08\x44istData\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04type\x18\x02 \x01(\t\x12\x32\n\x08sys_info\x18\x03 \x01(\x0b\x32 .secretflow.component.SystemInfo\x12\"\n\x04meta\x18\x04 \x01(\x0b\x32\x14.google.protobuf.Any\x12\x39\n\tdata_refs\x18\x05 \x03(\x0b\x32&.secretflow.component.DistData.DataRef\x1a\x35\n\x07\x44\x61taRef\x12\x0b\n\x03uri\x18\x01 \x01(\t\x12\r\n\x05party\x18\x02 \x01(\t\x12\x0e\n\x06\x66ormat\x18\x03 \x01(\tb\x06proto3'
+  serialized_pb=b'\n&secretflow/protos/component/data.proto\x12\x14secretflow.component\x1a\x19google/protobuf/any.proto\x1a)secretflow/protos/component/cluster.proto\"W\n\nSystemInfo\x12\x10\n\x08\x61pp_name\x18\x01 \x01(\t\x12\x37\n\nsecretflow\x18\x02 \x01(\x0b\x32#.secretflow.component.SFClusterDesc\"z\n\x0bTableSchema\x12\x0b\n\x03ids\x18\x01 \x03(\t\x12\x10\n\x08\x66\x65\x61tures\x18\x02 \x03(\t\x12\x0e\n\x06labels\x18\x03 \x03(\t\x12\x10\n\x08id_types\x18\x04 \x03(\t\x12\x15\n\rfeature_types\x18\x05 \x03(\t\x12\x13\n\x0blabel_types\x18\x06 \x03(\t\"V\n\rVerticalTable\x12\x32\n\x07schemas\x18\x01 \x03(\x0b\x32!.secretflow.component.TableSchema\x12\x11\n\tnum_lines\x18\x02 \x01(\x03\"W\n\x0fIndividualTable\x12\x31\n\x06schema\x18\x01 \x01(\x0b\x32!.secretflow.component.TableSchema\x12\x11\n\tnum_lines\x18\x02 \x01(\x03\"\xab\x01\n\x16\x44\x65viceObjectCollection\x12G\n\x04objs\x18\x01 \x03(\x0b\x32\x39.secretflow.component.DeviceObjectCollection.DeviceObject\x12\x13\n\x0bpublic_info\x18\x02 \x01(\t\x1a\x33\n\x0c\x44\x65viceObject\x12\x0c\n\x04type\x18\x01 \x01(\t\x12\x15\n\rdata_ref_idxs\x18\x02 \x03(\x05\"\xf0\x01\n\x08\x44istData\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04type\x18\x02 \x01(\t\x12\x32\n\x08sys_info\x18\x03 \x01(\x0b\x32 .secretflow.component.SystemInfo\x12\"\n\x04meta\x18\x04 \x01(\x0b\x32\x14.google.protobuf.Any\x12\x39\n\tdata_refs\x18\x05 \x03(\x0b\x32&.secretflow.component.DistData.DataRef\x1a\x35\n\x07\x44\x61taRef\x12\x0b\n\x03uri\x18\x01 \x01(\t\x12\r\n\x05party\x18\x02 \x01(\t\x12\x0e\n\x06\x66ormat\x18\x03 \x01(\tB \n\x1eorg.secretflow.proto.componentb\x06proto3'
   ,
   dependencies=[google_dot_protobuf_dot_any__pb2.DESCRIPTOR,secretflow_dot_protos_dot_component_dot_cluster__pb2.DESCRIPTOR,])
 
 
 
 
 _SYSTEMINFO = _descriptor.Descriptor(
@@ -465,8 +465,9 @@
   '__module__' : 'secretflow.protos.component.data_pb2'
   # @@protoc_insertion_point(class_scope:secretflow.component.DistData)
   })
 _sym_db.RegisterMessage(DistData)
 _sym_db.RegisterMessage(DistData.DataRef)
 
 
+DESCRIPTOR._options = None
 # @@protoc_insertion_point(module_scope)
```

## secretflow/protos/component/evaluation_pb2.py

```diff
@@ -15,17 +15,17 @@
 from secretflow.protos.component import data_pb2 as secretflow_dot_protos_dot_component_dot_data__pb2
 
 
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='secretflow/protos/component/evaluation.proto',
   package='secretflow.component',
   syntax='proto3',
-  serialized_options=None,
+  serialized_options=b'\n\036org.secretflow.proto.component',
   create_key=_descriptor._internal_create_key,
-  serialized_pb=b'\n,secretflow/protos/component/evaluation.proto\x12\x14secretflow.component\x1a&secretflow/protos/component/comp.proto\x1a&secretflow/protos/component/data.proto\"\xc7\x01\n\rNodeEvalParam\x12\x0e\n\x06\x64omain\x18\x01 \x01(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x0f\n\x07version\x18\x03 \x01(\t\x12\x12\n\nattr_paths\x18\x04 \x03(\t\x12.\n\x05\x61ttrs\x18\x05 \x03(\x0b\x32\x1f.secretflow.component.Attribute\x12.\n\x06inputs\x18\x06 \x03(\x0b\x32\x1e.secretflow.component.DistData\x12\x13\n\x0boutput_uris\x18\x07 \x03(\t\"A\n\x0eNodeEvalResult\x12/\n\x07outputs\x18\x01 \x03(\x0b\x32\x1e.secretflow.component.DistDatab\x06proto3'
+  serialized_pb=b'\n,secretflow/protos/component/evaluation.proto\x12\x14secretflow.component\x1a&secretflow/protos/component/comp.proto\x1a&secretflow/protos/component/data.proto\"\xc7\x01\n\rNodeEvalParam\x12\x0e\n\x06\x64omain\x18\x01 \x01(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x0f\n\x07version\x18\x03 \x01(\t\x12\x12\n\nattr_paths\x18\x04 \x03(\t\x12.\n\x05\x61ttrs\x18\x05 \x03(\x0b\x32\x1f.secretflow.component.Attribute\x12.\n\x06inputs\x18\x06 \x03(\x0b\x32\x1e.secretflow.component.DistData\x12\x13\n\x0boutput_uris\x18\x07 \x03(\t\"A\n\x0eNodeEvalResult\x12/\n\x07outputs\x18\x01 \x03(\x0b\x32\x1e.secretflow.component.DistDataB \n\x1eorg.secretflow.proto.componentb\x06proto3'
   ,
   dependencies=[secretflow_dot_protos_dot_component_dot_comp__pb2.DESCRIPTOR,secretflow_dot_protos_dot_component_dot_data__pb2.DESCRIPTOR,])
 
 
 
 
 _NODEEVALPARAM = _descriptor.Descriptor(
@@ -151,8 +151,9 @@
   'DESCRIPTOR' : _NODEEVALRESULT,
   '__module__' : 'secretflow.protos.component.evaluation_pb2'
   # @@protoc_insertion_point(class_scope:secretflow.component.NodeEvalResult)
   })
 _sym_db.RegisterMessage(NodeEvalResult)
 
 
+DESCRIPTOR._options = None
 # @@protoc_insertion_point(module_scope)
```

## secretflow/protos/component/report_pb2.py

```diff
@@ -14,17 +14,17 @@
 from secretflow.protos.component import comp_pb2 as secretflow_dot_protos_dot_component_dot_comp__pb2
 
 
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='secretflow/protos/component/report.proto',
   package='secretflow.component',
   syntax='proto3',
-  serialized_options=None,
+  serialized_options=b'\n\036org.secretflow.proto.component',
   create_key=_descriptor._internal_create_key,
-  serialized_pb=b'\n(secretflow/protos/component/report.proto\x12\x14secretflow.component\x1a&secretflow/protos/component/comp.proto\"\xe5\x01\n\x0c\x44\x65scriptions\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\x36\n\x05items\x18\x03 \x03(\x0b\x32\'.secretflow.component.Descriptions.Item\x1a\x80\x01\n\x04Item\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12,\n\x04type\x18\x03 \x01(\x0e\x32\x1e.secretflow.component.AttrType\x12.\n\x05value\x18\x04 \x01(\x0b\x32\x1f.secretflow.component.Attribute\"\xb6\x02\n\x05Table\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\x37\n\x07headers\x18\x03 \x03(\x0b\x32&.secretflow.component.Table.HeaderItem\x12-\n\x04rows\x18\x04 \x03(\x0b\x32\x1f.secretflow.component.Table.Row\x1aV\n\nHeaderItem\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12,\n\x04type\x18\x03 \x01(\x0e\x32\x1e.secretflow.component.AttrType\x1aQ\n\x03Row\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12.\n\x05items\x18\x03 \x03(\x0b\x32\x1f.secretflow.component.Attribute\"\xfa\x01\n\x03\x44iv\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\x31\n\x08\x63hildren\x18\x03 \x03(\x0b\x32\x1f.secretflow.component.Div.Child\x1a\xa3\x01\n\x05\x43hild\x12\x0c\n\x04type\x18\x01 \x01(\t\x12\x38\n\x0c\x64\x65scriptions\x18\x02 \x01(\x0b\x32\".secretflow.component.Descriptions\x12*\n\x05table\x18\x03 \x01(\x0b\x32\x1b.secretflow.component.Table\x12&\n\x03\x64iv\x18\x04 \x01(\x0b\x32\x19.secretflow.component.Div\"J\n\x03Tab\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\'\n\x04\x64ivs\x18\x03 \x03(\x0b\x32\x19.secretflow.component.Div\"s\n\x06Report\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\'\n\x04tabs\x18\x03 \x03(\x0b\x32\x19.secretflow.component.Tab\x12\x10\n\x08\x65rr_code\x18\x04 \x01(\x05\x12\x12\n\nerr_detail\x18\x05 \x01(\tb\x06proto3'
+  serialized_pb=b'\n(secretflow/protos/component/report.proto\x12\x14secretflow.component\x1a&secretflow/protos/component/comp.proto\"\xc4\x01\n\x0c\x44\x65scriptions\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\x36\n\x05items\x18\x03 \x03(\x0b\x32\'.secretflow.component.Descriptions.Item\x1a`\n\x04Item\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\x0c\n\x04type\x18\x03 \x01(\t\x12.\n\x05value\x18\x04 \x01(\x0b\x32\x1f.secretflow.component.Attribute\"\x96\x02\n\x05Table\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\x37\n\x07headers\x18\x03 \x03(\x0b\x32&.secretflow.component.Table.HeaderItem\x12-\n\x04rows\x18\x04 \x03(\x0b\x32\x1f.secretflow.component.Table.Row\x1a\x36\n\nHeaderItem\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\x0c\n\x04type\x18\x03 \x01(\t\x1aQ\n\x03Row\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12.\n\x05items\x18\x03 \x03(\x0b\x32\x1f.secretflow.component.Attribute\"\xfa\x01\n\x03\x44iv\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\x31\n\x08\x63hildren\x18\x03 \x03(\x0b\x32\x1f.secretflow.component.Div.Child\x1a\xa3\x01\n\x05\x43hild\x12\x0c\n\x04type\x18\x01 \x01(\t\x12\x38\n\x0c\x64\x65scriptions\x18\x02 \x01(\x0b\x32\".secretflow.component.Descriptions\x12*\n\x05table\x18\x03 \x01(\x0b\x32\x1b.secretflow.component.Table\x12&\n\x03\x64iv\x18\x04 \x01(\x0b\x32\x19.secretflow.component.Div\"J\n\x03Tab\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\'\n\x04\x64ivs\x18\x03 \x03(\x0b\x32\x19.secretflow.component.Div\"s\n\x06Report\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x65sc\x18\x02 \x01(\t\x12\'\n\x04tabs\x18\x03 \x03(\x0b\x32\x19.secretflow.component.Tab\x12\x10\n\x08\x65rr_code\x18\x04 \x01(\x05\x12\x12\n\nerr_detail\x18\x05 \x01(\tB \n\x1eorg.secretflow.proto.componentb\x06proto3'
   ,
   dependencies=[secretflow_dot_protos_dot_component_dot_comp__pb2.DESCRIPTOR,])
 
 
 
 
 _DESCRIPTIONS_ITEM = _descriptor.Descriptor(
@@ -47,16 +47,16 @@
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=b"".decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
       name='type', full_name='secretflow.component.Descriptions.Item.type', index=2,
-      number=3, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
+      number=3, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
       name='value', full_name='secretflow.component.Descriptions.Item.value', index=3,
       number=4, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
@@ -71,16 +71,16 @@
   ],
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=208,
-  serialized_end=336,
+  serialized_start=207,
+  serialized_end=303,
 )
 
 _DESCRIPTIONS = _descriptor.Descriptor(
   name='Descriptions',
   full_name='secretflow.component.Descriptions',
   filename=None,
   file=DESCRIPTOR,
@@ -117,15 +117,15 @@
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
   serialized_start=107,
-  serialized_end=336,
+  serialized_end=303,
 )
 
 
 _TABLE_HEADERITEM = _descriptor.Descriptor(
   name='HeaderItem',
   full_name='secretflow.component.Table.HeaderItem',
   filename=None,
@@ -145,16 +145,16 @@
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=b"".decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
     _descriptor.FieldDescriptor(
       name='type', full_name='secretflow.component.Table.HeaderItem.type', index=2,
-      number=3, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
+      number=3, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
       serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
   ],
   extensions=[
   ],
   nested_types=[],
@@ -162,16 +162,16 @@
   ],
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=480,
-  serialized_end=566,
+  serialized_start=447,
+  serialized_end=501,
 )
 
 _TABLE_ROW = _descriptor.Descriptor(
   name='Row',
   full_name='secretflow.component.Table.Row',
   filename=None,
   file=DESCRIPTOR,
@@ -207,16 +207,16 @@
   ],
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=568,
-  serialized_end=649,
+  serialized_start=503,
+  serialized_end=584,
 )
 
 _TABLE = _descriptor.Descriptor(
   name='Table',
   full_name='secretflow.component.Table',
   filename=None,
   file=DESCRIPTOR,
@@ -259,16 +259,16 @@
   ],
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=339,
-  serialized_end=649,
+  serialized_start=306,
+  serialized_end=584,
 )
 
 
 _DIV_CHILD = _descriptor.Descriptor(
   name='Child',
   full_name='secretflow.component.Div.Child',
   filename=None,
@@ -312,16 +312,16 @@
   ],
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=739,
-  serialized_end=902,
+  serialized_start=674,
+  serialized_end=837,
 )
 
 _DIV = _descriptor.Descriptor(
   name='Div',
   full_name='secretflow.component.Div',
   filename=None,
   file=DESCRIPTOR,
@@ -357,16 +357,16 @@
   ],
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=652,
-  serialized_end=902,
+  serialized_start=587,
+  serialized_end=837,
 )
 
 
 _TAB = _descriptor.Descriptor(
   name='Tab',
   full_name='secretflow.component.Tab',
   filename=None,
@@ -403,16 +403,16 @@
   ],
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=904,
-  serialized_end=978,
+  serialized_start=839,
+  serialized_end=913,
 )
 
 
 _REPORT = _descriptor.Descriptor(
   name='Report',
   full_name='secretflow.component.Report',
   filename=None,
@@ -463,23 +463,21 @@
   ],
   serialized_options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=980,
-  serialized_end=1095,
+  serialized_start=915,
+  serialized_end=1030,
 )
 
-_DESCRIPTIONS_ITEM.fields_by_name['type'].enum_type = secretflow_dot_protos_dot_component_dot_comp__pb2._ATTRTYPE
 _DESCRIPTIONS_ITEM.fields_by_name['value'].message_type = secretflow_dot_protos_dot_component_dot_comp__pb2._ATTRIBUTE
 _DESCRIPTIONS_ITEM.containing_type = _DESCRIPTIONS
 _DESCRIPTIONS.fields_by_name['items'].message_type = _DESCRIPTIONS_ITEM
-_TABLE_HEADERITEM.fields_by_name['type'].enum_type = secretflow_dot_protos_dot_component_dot_comp__pb2._ATTRTYPE
 _TABLE_HEADERITEM.containing_type = _TABLE
 _TABLE_ROW.fields_by_name['items'].message_type = secretflow_dot_protos_dot_component_dot_comp__pb2._ATTRIBUTE
 _TABLE_ROW.containing_type = _TABLE
 _TABLE.fields_by_name['headers'].message_type = _TABLE_HEADERITEM
 _TABLE.fields_by_name['rows'].message_type = _TABLE_ROW
 _DIV_CHILD.fields_by_name['descriptions'].message_type = _DESCRIPTIONS
 _DIV_CHILD.fields_by_name['table'].message_type = _TABLE
@@ -559,8 +557,9 @@
   'DESCRIPTOR' : _REPORT,
   '__module__' : 'secretflow.protos.component.report_pb2'
   # @@protoc_insertion_point(class_scope:secretflow.component.Report)
   })
 _sym_db.RegisterMessage(Report)
 
 
+DESCRIPTOR._options = None
 # @@protoc_insertion_point(module_scope)
```

## secretflow/stats/biclassification_eval.py

```diff
@@ -73,15 +73,15 @@
         ), "y_true and y_score should have the same partitions"
         assert len(y_score.partitions) == 1, "y_score should have one partition"
 
         device1 = [*y_score.partitions.keys()][0]
         device2 = [*y_true.partitions.keys()][0]
         assert (
             device1 == device2
-        ), "Currently require the device for two inputs are the same"
+        ), "Currently we requires both inputs belongs to the same party and computation happens locally."
         # Later may use spu
 
         self.device = device1
         if isinstance(y_true, FedNdarray):
             self.y_true = [*y_true.partitions.values()][0]
         else:
             self.y_true = ([*y_true.partitions.values()][0]).data
```

## secretflow/stats/prediction_bias_eval.py

```diff
@@ -65,15 +65,15 @@
     ), "label and prediction should have the same partitions"
     assert len(prediction.partitions) == 1, "y_score should have one partition"
 
     device1 = [*label.partitions.keys()][0]
     device2 = [*prediction.partitions.keys()][0]
     assert (
         device1 == device2
-    ), "Currently require the device for two inputs are the same"
+    ), "Currently we requires both inputs belongs to the same party and computation happens locally."
 
     # Later may use spu
 
     device = device1
     if isinstance(label, FedNdarray):
         label = [*label.partitions.values()][0]
     else:
```

## secretflow/stats/core/biclassification_eval_core.py

```diff
@@ -24,16 +24,14 @@
 
 class Report:
     """Report containing all other reports for bi-classification evaluation
 
     Attributes:
         summary_report: SummaryReport
 
-        group_reports: List[GroupReport]
-
         eq_frequent_bin_report: List[EqBinReport]
 
         eq_range_bin_report: List[EqBinReport]
 
         head_report: List[PrReport]
             reports for fpr = 0.001, 0.005, 0.01, 0.05, 0.1, 0.2
     """
```

## secretflow/utils/compressor.py

```diff
@@ -7,23 +7,24 @@
 #   http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
+import logging
 from abc import ABC, abstractmethod
+from dataclasses import dataclass
 from typing import Any, List, Union
 
 import jax.numpy as jnp
 import numpy as np
 from scipy import sparse
-
 from secretflow.utils.communicate import ForwardData
+from secretflow.utils.errors import InvalidArgumentError
 
 
 class Compressor(ABC):
     """Abstract base class for cross device data compressor"""
 
     @abstractmethod
     def compress(
@@ -111,15 +112,15 @@
         else:
             hidden = data
 
         if isinstance(hidden, (np.ndarray, jnp.ndarray)):
             is_list = False
             hidden = [hidden]
         elif not isinstance(hidden, (list, tuple)):
-            assert False, f'invalid data: {type(hidden)}'
+            raise InvalidArgumentError(f'invalid data: {type(hidden)}')
         out = list(map(lambda d: self._compress_one(d), hidden))
         out = out if is_list else out[0]
         if isinstance(data, ForwardData):
             data.hidden = out
         else:
             data = out
         return data
@@ -142,15 +143,15 @@
         else:
             sparse_hidden = data
 
         if sparse.issparse(sparse_hidden):
             is_list = False
             sparse_hidden = [sparse_hidden]
         elif not isinstance(sparse_hidden, (list, tuple)):
-            assert False, f'invalid data: {type(sparse_hidden)}'
+            raise InvalidArgumentError(f'invalid data: {type(sparse_hidden)}')
         sparse_hidden = list(map(lambda d: d.todense(), sparse_hidden))
         sparse_hidden = sparse_hidden if is_list else sparse_hidden[0]
         if isinstance(data, ForwardData):
             data.hidden = sparse_hidden
         else:
             data = sparse_hidden
         return data
@@ -352,7 +353,177 @@
         data[0], (sp._coo.core.COO, sp._compressed.compressed.GCXS)
     ), 'Sparse encoding method not supporterd, Only COO GCXS supported'
     decode_datas = []
     for datum in data:
         decode_datum = datum.todense()
         decode_datas.append(decode_datum)
     return decode_datas
+
+
+@dataclass
+class QuantizedData:
+    data: Any = None
+    q1: int = None
+    q2: int = None
+    origin_type: Any = None
+
+
+class QuantizedCompressor(Compressor):
+    """Abstract base class for quantized compressor"""
+
+    def __init__(self, quant_bits: int = 8):
+        """Initialize
+
+        Args:
+            quant_bits: the compressed bits length.
+        """
+        super().__init__()
+        self.quant_bits = quant_bits
+        self.np_type = self._infer_np_type(quant_bits)
+
+    def compress(
+        self,
+        data: Union[ForwardData, np.ndarray, List[np.ndarray]],
+    ) -> Union[Any, List[Any]]:
+        is_list = True
+
+        if isinstance(data, ForwardData):
+            hidden = data.hidden
+        else:
+            hidden = data
+
+        if isinstance(hidden, (np.ndarray, jnp.ndarray)):
+            is_list = False
+            hidden = [hidden]
+        elif not isinstance(hidden, (list, tuple)):
+            raise InvalidArgumentError(f'invalid data: {type(hidden)}')
+        out = list(map(lambda x: self._compress_one(x), hidden))
+        out = out if is_list else out[0]
+        if isinstance(data, ForwardData):
+            data.hidden = out
+        else:
+            data = out
+        return data
+
+    def decompress(
+        self,
+        data: Union[ForwardData, np.ndarray, List[np.ndarray], List[QuantizedData]],
+    ) -> Union[np.ndarray, List[np.ndarray]]:
+        is_list = True
+        if isinstance(data, ForwardData):
+            quantized_hidden = data.hidden
+        else:
+            quantized_hidden = data
+
+        if isinstance(quantized_hidden, (np.ndarray, jnp.ndarray, QuantizedData)):
+            is_list = False
+            quantized_hidden = [quantized_hidden]
+
+        quantized_hidden = list(
+            map(lambda x: self._decompress_one(x), quantized_hidden)
+        )
+        quantized_hidden = quantized_hidden if is_list else quantized_hidden[0]
+
+        if isinstance(data, ForwardData):
+            data.hidden = quantized_hidden
+        else:
+            data = quantized_hidden
+
+        return data
+
+    def iscompressed(self, data: Union[Any, List[Any]]) -> Union[bool, List[bool]]:
+        if not isinstance(data, list):
+            return isinstance(data, QuantizedData)
+        is_compressed = list(map(lambda x: isinstance(x, QuantizedData), data))
+        return is_compressed
+
+    def _infer_np_type(self, quant_bits):
+        if self.quant_bits <= 0 or self.quant_bits > 64:
+            logging.error(
+                f"The quantized bits len must be between 0 and 64, got {quant_bits}"
+            )
+            raise RuntimeError(
+                f"The quantized bits len must be between 0 and 64, got {quant_bits}"
+            )
+        recommend_bits = {8: np.int8, 16: np.int16, 32: np.int32, 64: np.int64}
+        if quant_bits in recommend_bits:
+            return recommend_bits[quant_bits]
+        else:
+            logging.warning(
+                f"It is recommended to use 8/16/32/64 as the compression bits (got {self.quant_bits}),"
+                f"That will loss accuracy but not save communication traffic."
+            )
+            for i in range(64):
+                if (quant_bits + i) in recommend_bits:
+                    return recommend_bits[(quant_bits + i)]
+        raise RuntimeError(f"Unknown input quant_bits {quant_bits}")
+
+    @abstractmethod
+    def _compress_one(self, data: np.ndarray) -> QuantizedData:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def _decompress_one(self, data: QuantizedData) -> np.ndarray:
+        raise NotImplementedError()
+
+
+class QuantizedZeroPoint(QuantizedCompressor):
+    """Quantized compressor with strengthen the QuantizedLSTM with replacing 32-bit RQM to 8-bit zero point.
+    The tests show that the QuantizedZeroPoint compressor has higner accuracy than QuantizedLSTM.
+    Reference paper 2017 "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference".
+
+    Link: https://arxiv.org/abs/1712.05877
+    """
+
+    def __init__(self, quant_bits: int = 8):
+        super().__init__(quant_bits)
+
+    def _compress_one(self, data: np.ndarray) -> QuantizedData:
+        _max = np.max(data)
+        _min = np.min(data)
+        qmin = -(int)(1 << (self.quant_bits - 1))
+        qmax = (int)((1 << (self.quant_bits - 1)) - 1)
+
+        scale = (_max - _min) / (qmax - qmin)
+        initial_zero_point = qmin - _min / scale
+        nudged_zero_point = initial_zero_point
+        if initial_zero_point < qmin:
+            nudged_zero_point = qmin
+        elif initial_zero_point > qmax:
+            nudged_zero_point = qmax
+        else:
+            nudged_zero_point = int(initial_zero_point)
+        transformed_val = data / scale + nudged_zero_point
+        clamped_val = np.clip(transformed_val, qmin, qmax)
+        quantized = np.round(clamped_val)
+        return QuantizedData(
+            quantized.astype(self.np_type), scale, nudged_zero_point, data.dtype
+        )
+
+    def _decompress_one(self, data: QuantizedData) -> np.ndarray:
+        return (data.data.astype(data.origin_type) - float(data.q2)) * data.q1
+
+
+class QuantizedLSTM(QuantizedCompressor):
+    """Quantized compressor with LSTM, a basic algorithm which replace float with int.
+
+    Reference paper 2016 "On the efficient representation and execution of deep acoustic models".
+
+    Link: https://arxiv.org/abs/1607.04683
+    """
+
+    def __init__(self, quant_bits: int = 8):
+        super().__init__(quant_bits)
+
+    def _compress_one(self, data: np.ndarray) -> QuantizedData:
+        _max = np.max(data)
+        _min = np.min(data)
+        q_scale = int((1 << self.quant_bits) - 1)
+        q_shift = int(1 << (self.quant_bits - 1))
+
+        q = q_scale / (_max - _min)
+        rqm = int(round(q * _min) + q_shift)
+        quantized = np.round(q * data) - rqm
+        return QuantizedData(quantized.astype(self.np_type), q, rqm, data.dtype)
+
+    def _decompress_one(self, data: QuantizedData) -> np.ndarray:
+        return (data.data.astype(data.origin_type) + float(data.q2)) / float(data.q1)
```

## secretflow/utils/ndarray_bigint.py

```diff
@@ -37,14 +37,15 @@
 
 def zeros(shape):
     return BigintNdArray([0] * math.prod(shape), shape)
 
 
 class BigintNdArray:
     def __init__(self, data, shape):
+        assert len(data) == math.prod(shape), f"{len(data)} != {math.prod(shape)}"
         self.shape = shape
         self.data = data
 
     def resize(self, shape):
         assert math.prod(shape) == math.prod(
             self.shape
         ), f"cannot resize array of size {self.shape} into shape {shape}"
```

## secretflow/utils/simulation/datasets.py

```diff
@@ -92,58 +92,58 @@
         'citeseer.zip',
         'https://secretflow-data.oss-accelerate.aliyuncs.com/datasets/citeseer/citeseer.zip',
         '8f0f1aba42c7be5818dc43d96913713a2ffc1c0d9dc09bef30d0432d2c102b49',
     ),
 }
 
 
-def _unzip(file, extract_path=None):
+def unzip(file, extract_path=None):
     if not extract_path:
         extract_path = str(Path(file).parent)
     with zipfile.ZipFile(file, 'r') as zip_f:
         zip_f.extractall(extract_path)
 
 
-def _download(url: str, filepath: str, sha256: str):
+def download(url: str, filepath: str, sha256: str):
     Path(filepath).parent.mkdir(parents=True, exist_ok=True)
     content = requests.get(url, stream=True).content
     h = hashlib.sha256()
     h.update(content)
     actual_sha256 = h.hexdigest()
     assert (
         sha256 == actual_sha256
     ), f'Failed to check sha256 of {url}, expected {sha256}, got {actual_sha256}.'
 
     with open(filepath, 'wb') as f:
         f.write(content)
 
 
-def _get_dataset(dataset: _Dataset, cache_dir: str = None):
+def get_dataset(dataset: _Dataset, cache_dir: str = None):
     if not cache_dir:
         cache_dir = _CACHE_DIR
 
     filepath = f'{cache_dir}/{dataset.filename}'
     Path(filepath).parent.mkdir(parents=True, exist_ok=True)
 
     import filelock
 
     with filelock.FileLock(f'{filepath}.lock'):
-        need_download = not Path(filepath).exists()
-        if not need_download:
+        needdownload = not Path(filepath).exists()
+        if not needdownload:
             sha256 = sha256sum(filepath)
             if sha256 != dataset.sha256:
                 os.remove(filepath)
-                need_download = True
+                needdownload = True
 
-        if need_download:
+        if needdownload:
             assert (
                 dataset.url
             ), f'{dataset.filename} does not exist locally, please give a download url.'
 
-            _download(dataset.url, filepath, dataset.sha256)
+            download(dataset.url, filepath, dataset.sha256)
         return filepath
 
 
 def dataset(name: str, cache_dir: str = None) -> str:
     """Get the specific dataset file path.
 
     Args:
@@ -151,15 +151,15 @@
             'bank_marketing', 'mnist', 'linear'].
 
     Returns:
         the dataset file path.
     """
     assert name and isinstance(name, str), 'Name shall be a valid string.'
     name = name.lower()
-    return _get_dataset(_DATASETS[name], cache_dir)
+    return get_dataset(_DATASETS[name], cache_dir)
 
 
 def load_iris(
     parts: Union[List[PYU], Dict[PYU, Union[float, Tuple]]],
     axis=0,
     aggregator: Aggregator = None,
     comparator: Comparator = None,
@@ -189,15 +189,15 @@
             please refer to `secretflow.data.horizontal.HDataFrame`.
         comparator:  optional, shall be provided only when axis is 0. For details,
             please refer to `secretflow.data.horizontal.HDataFrame`.
 
     Returns:
         return a HDataFrame if axis is 0 else VDataFrame.
     """
-    filepath = _get_dataset(_DATASETS['iris'])
+    filepath = get_dataset(_DATASETS['iris'])
     return create_df(
         source=filepath,
         parts=parts,
         axis=axis,
         shuffle=False,
         aggregator=aggregator,
         comparator=comparator,
@@ -232,15 +232,15 @@
             please refer to `secretflow.data.horizontal.HDataFrame`.
         comparator:  optional, shall be provided only when axis is 0. For details,
             please refer to `secretflow.data.horizontal.HDataFrame`.
 
     Returns:
         return a HDataFrame if axis is 0 else VDataFrame.
     """
-    filepath = _get_dataset(_DATASETS['dermatology'])
+    filepath = get_dataset(_DATASETS['dermatology'])
     df = pd.read_csv(filepath)
     if class_starts_from_zero:
         df['class'] = df['class'] - 1
     return create_df(
         source=df,
         parts=parts,
         axis=axis,
@@ -279,17 +279,17 @@
         comparator:  optional, shall be provided only when axis is 0. For details,
             please refer to `secretflow.data.horizontal.HDataFrame`.
 
     Returns:
         return a HDataFrame if axis is 0 else VDataFrame.
     """
     if full:
-        filepath = _get_dataset(_DATASETS['bank_marketing_full'])
+        filepath = get_dataset(_DATASETS['bank_marketing_full'])
     else:
-        filepath = _get_dataset(_DATASETS['bank_marketing'])
+        filepath = get_dataset(_DATASETS['bank_marketing'])
     return create_df(
         lambda: pd.read_csv(filepath, sep=';'),
         parts=parts,
         axis=axis,
         shuffle=False,
         aggregator=aggregator,
         comparator=comparator,
@@ -316,15 +316,15 @@
             2) an interval in tuple closed on the left-side and open on the right-side.
         normalized_x: optional, normalize x if True. Default to True.
         categorical_y: optional, do one hot encoding to y if True. Default to True.
 
     Returns:
         A tuple consists of two tuples, (x_train, y_train) and (x_test, y_test).
     """
-    filepath = _get_dataset(_DATASETS['mnist'])
+    filepath = get_dataset(_DATASETS['mnist'])
     with np.load(filepath) as f:
         x_train, y_train = f['x_train'], f['y_train']
         x_test, y_test = f['x_test'], f['y_test']
 
     if normalized_x:
         x_train, x_test = x_train / 255, x_test / 255
 
@@ -361,15 +361,15 @@
             dict {PYU: value}, the value shall be one of the followings.
             1) a float
             2) an interval in tuple closed on the left-side and open on the right-side.
 
     Returns:
         return a VDataFrame.
     """
-    filepath = _get_dataset(_DATASETS['linear'])
+    filepath = get_dataset(_DATASETS['linear'])
     return create_vdf(source=filepath, parts=parts, shuffle=False)
 
 
 def load_cora(
     parts: List[PYU], data_dir: str = None, add_self_loop: bool = True
 ) -> Tuple[
     FedNdarray,
@@ -391,16 +391,16 @@
         A tuple of FedNdarray: edge, x, Y_train, Y_val, Y_valid, index_train,
         index_val, index_test. Note that Y is bound to the first participant.
     """
     assert parts, 'Parts shall not be None or empty!'
     if data_dir is None:
         data_dir = os.path.join(_CACHE_DIR, 'cora')
         if not Path(data_dir).is_dir():
-            filepath = _get_dataset(_DATASETS['cora'])
-            _unzip(filepath, data_dir)
+            filepath = get_dataset(_DATASETS['cora'])
+            unzip(filepath, data_dir)
 
     file_names = [
         os.path.join(data_dir, f'ind.cora.{name}')
         for name in ['y', 'tx', 'ty', 'allx', 'ally', 'graph']
     ]
 
     objects = []
@@ -526,16 +526,16 @@
         A tuple of FedNdarray: edge, x, Y_train, Y_val, Y_valid, index_train,
         index_val, index_test. Note that Y is bound to the first participant.
     """
     assert parts, 'Parts shall not be None or empty!'
     if data_dir is None:
         data_dir = os.path.join(_CACHE_DIR, 'pubmed')
         if not Path(data_dir).is_dir():
-            filepath = _get_dataset(_DATASETS['pubmed'])
-            _unzip(filepath, data_dir)
+            filepath = get_dataset(_DATASETS['pubmed'])
+            unzip(filepath, data_dir)
 
     file_names = [
         os.path.join(data_dir, f'ind.pubmed.{name}')
         for name in ['y', 'tx', 'ty', 'allx', 'ally', 'graph']
     ]
 
     objects = []
@@ -664,16 +664,16 @@
         A tuple of FedNdarray: edge, x, Y_train, Y_val, Y_valid, index_train,
         index_val, index_test. Note that Y is bound to the first participant.
     """
     assert parts, 'Parts shall not be None or empty!'
     if data_dir is None:
         data_dir = os.path.join(_CACHE_DIR, 'citeseer')
         if not Path(data_dir).is_dir():
-            filepath = _get_dataset(_DATASETS['citeseer'])
-            _unzip(filepath, data_dir)
+            filepath = get_dataset(_DATASETS['citeseer'])
+            unzip(filepath, data_dir)
 
     file_names = [
         os.path.join(data_dir, f'ind.citeseer.{name}')
         for name in ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']
     ]
 
     objects = []
@@ -840,16 +840,16 @@
         for c in columns:
             values.append(feature[c])
         return ",".join(values)
 
     if data_dir is None:
         data_dir = os.path.join(_CACHE_DIR, 'ml-1m')
         if not Path(data_dir).is_dir():
-            filepath = _get_dataset(_DATASETS['ml-1m'])
-            _unzip(filepath, data_dir)
+            filepath = get_dataset(_DATASETS['ml-1m'])
+            unzip(filepath, data_dir)
     extract_dir = os.path.join(data_dir, 'ml-1m')
     users_data = _load_data(
         extract_dir + "/users.dat",
         columns=["UserID", "Gender", "Age", "Occupation", "Zip-code"],
     )
     movies_data = _load_data(
         extract_dir + "/movies.dat", columns=["MovieID", "Title", "Genres"]
```

## Comparing `secretflow/ml/boost/sgb_v/core/label_holder/__init__.py` & `secretflow/ml/nn/sl/attack/__init__.py`

 * *Files identical despite different names*

## Comparing `secretflow/ml/boost/sgb_v/core/preprocessing/__init__.py` & `secretflow/ml/nn/sl/attack/torch/__init__.py`

 * *Files identical despite different names*

## Comparing `secretflow/ml/boost/sgb_v/core/split_tree_trainer/order_map_context.py` & `secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_context.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,18 +10,19 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 from typing import List, Tuple
-from secretflow.ml.boost.core.order_map_tools import qcut
 
 import numpy as np
 
+from secretflow.ml.boost.core.order_map_tools import qcut
+
 
 # Deal with order map context of a single partition
 class OrderMapContext:
     """Manage context related to order map, and bucket and split point information derived from it."""
 
     def __init__(self):
         self.order_map = None
```

## Comparing `secretflow/ml/boost/sgb_v/core/split_tree_trainer/shuffler.py` & `secretflow/ml/boost/sgb_v/factory/components/shuffler/shuffler_core.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,15 +13,17 @@
 # limitations under the License.
 
 from typing import List
 
 import jax.tree_util
 import numpy as np
 
-from ..pure_numpy_ops.random import create_permuation_with_last_number_fixed
+from secretflow.ml.boost.sgb_v.core.pure_numpy_ops.random import (
+    create_permuation_with_last_number_fixed,
+)
 
 
 class Shuffler:
     def __init__(self):
         self.reindex_list_map = {}
 
     def create_shuffle_mask(self, key: int, bucket_list: List[int]) -> List[int]:
```

## Comparing `secretflow_lite-1.0.0b3.dist-info/LICENSE` & `secretflow_lite-1.1.0b0.dev0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `secretflow_lite-1.0.0b3.dist-info/METADATA` & `secretflow_lite-1.1.0b0.dev0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 Metadata-Version: 2.1
 Name: secretflow-lite
-Version: 1.0.0b3
+Version: 1.1.0b0.dev0
 Summary: SecretFlow Lite
 Home-page: https://github.com/secretflow/secretflow
 Author: SCI Center
 Author-email: secretflow-contact@service.alipay.com
 License: Apache 2.0
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: clean-text
 Requires-Dist: click
-Requires-Dist: grpcio (<=1.49.1)
+Requires-Dist: grpcio (==1.56.2)
 Requires-Dist: jax (<=0.4.12)
 Requires-Dist: jaxlib (<=0.4.12)
 Requires-Dist: kuscia (==0.0.1b2)
 Requires-Dist: multiprocess
 Requires-Dist: numba (==0.57.0)
 Requires-Dist: numpy (==1.23.5)
 Requires-Dist: pandas (==1.5.3)
 Requires-Dist: protobuf (==3.19.6)
 Requires-Dist: pyarrow (==11.0.0)
 Requires-Dist: scikit-learn (==1.1.3)
 Requires-Dist: secretflow-rayfed (==0.1.1a4)
-Requires-Dist: spu (==0.4.1b1)
+Requires-Dist: spu (==0.4.2b2)
 Requires-Dist: sf-heu (==0.4.4b0)
 Requires-Dist: sdc-apis (==0.1.0b0) ; platform_system != "Darwin"
 Requires-Dist: sdc-sdk (==0.1.0b0) ; platform_system != "Darwin"
 Provides-Extra: dev
 Requires-Dist: pylint ; extra == 'dev'
 
 <div align="center">
```

### html2text {}

```diff
@@ -1,18 +1,18 @@
-Metadata-Version: 2.1 Name: secretflow-lite Version: 1.0.0b3 Summary:
+Metadata-Version: 2.1 Name: secretflow-lite Version: 1.1.0b0.dev0 Summary:
 SecretFlow Lite Home-page: https://github.com/secretflow/secretflow Author: SCI
 Center Author-email: secretflow-contact@service.alipay.com License: Apache 2.0
 Description-Content-Type: text/markdown License-File: LICENSE Requires-Dist:
-clean-text Requires-Dist: click Requires-Dist: grpcio (<=1.49.1) Requires-Dist:
+clean-text Requires-Dist: click Requires-Dist: grpcio (==1.56.2) Requires-Dist:
 jax (<=0.4.12) Requires-Dist: jaxlib (<=0.4.12) Requires-Dist: kuscia
 (==0.0.1b2) Requires-Dist: multiprocess Requires-Dist: numba (==0.57.0)
 Requires-Dist: numpy (==1.23.5) Requires-Dist: pandas (==1.5.3) Requires-Dist:
 protobuf (==3.19.6) Requires-Dist: pyarrow (==11.0.0) Requires-Dist: scikit-
 learn (==1.1.3) Requires-Dist: secretflow-rayfed (==0.1.1a4) Requires-Dist: spu
-(==0.4.1b1) Requires-Dist: sf-heu (==0.4.4b0) Requires-Dist: sdc-apis
+(==0.4.2b2) Requires-Dist: sf-heu (==0.4.4b0) Requires-Dist: sdc-apis
 (==0.1.0b0) ; platform_system != "Darwin" Requires-Dist: sdc-sdk (==0.1.0b0) ;
 platform_system != "Darwin" Provides-Extra: dev Requires-Dist: pylint ; extra
 == 'dev'
                          [docs/_static/logo-light.png]
 --- [![CircleCI](https://dl.circleci.com/status-badge/img/gh/secretflow/
 secretflow/tree/main.svg?style=svg)](https://dl.circleci.com/status-badge/
 redirect/gh/secretflow/secretflow/tree/main)
```

## Comparing `secretflow_lite-1.0.0b3.dist-info/RECORD` & `secretflow_lite-1.1.0b0.dev0.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,39 +1,39 @@
 secretflow/__init__.py,sha256=hNYxuX4jqzIoRuUxKl9OMp9UcbvdsvwROS6i7pHmqJQ,1321
 secretflow/cli.py,sha256=EMNzikFKsXnyK1e5nUhblYF_fqYNc5vgnYWmnJB0sYU,6478
-secretflow/version.py,sha256=8Gj-z50M6YRLqg5xV74qegdg0tVrkS4xC5dG9yLbOkw,607
+secretflow/version.py,sha256=4SFm4nAcJK2DzxYnKoG3Xc5RNG_FKpyZr8rt1Q5PgFU,612
 secretflow/component/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
 secretflow/component/__main__.py,sha256=wYrFQT-Q1RJpZ1F_iKN5Mk-5idUIVZn9pSwiiktKtUY,628
-secretflow/component/component.py,sha256=pHoFdjyZV-xwE4cNIcqmLjxq0Vm3dwvUlr4HEhmPjx0,30621
-secretflow/component/data_utils.py,sha256=xNYvRqm8hgWawXMt3cHVqjZP5KdjKLHPK2oQIpnVrBc,16462
+secretflow/component/component.py,sha256=PrlQREiy9F26DbQkwPvegjNJPuGzDG3oS5D1aVn2HYE,30749
+secretflow/component/data_utils.py,sha256=Di77dglp25dxPQduL70Bq9tE80QFo1otHegZC3dYtVM,16550
 secretflow/component/entry.py,sha256=TghtE6aF32MjsS27b_0A5BecoL3Fd-EwGZ4HwoEP1ZI,3761
-secretflow/component/eval_param_reader.py,sha256=9ui4Cy4G8IWEAfdX3DpZjVgCvxvTlQbelQO2J2YvB5k,9676
+secretflow/component/eval_param_reader.py,sha256=v1-JP__VBT5ZBE3_bfVkC0s_yJBGu9PFgmTLUL6mEVM,9711
 secretflow/component/i18n.py,sha256=T2hYTTN08ueC_-knKzL4SFzkf5WkijVGuvhcQu8wGwA,3377
 secretflow/component/feature/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/component/feature/vert_woe_binning.py,sha256=og61cRO6aYx4gnlO7yuyMb4xrdrUzd0aP_c4KmMzviA,8843
+secretflow/component/feature/vert_woe_binning.py,sha256=HCSCxOmtBBmfaWIo7xtkaRcE3_JucwvurpICS0D9K7k,8835
 secretflow/component/ml/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
 secretflow/component/ml/boost/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
 secretflow/component/ml/boost/sgb/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/component/ml/boost/sgb/sgb.py,sha256=zflT1hNTBel6dAYEUWTdV1qMWCi0McjQhNDjyBK3XXU,12633
+secretflow/component/ml/boost/sgb/sgb.py,sha256=rgn5wk6cOFlsxHBmg2sgnBtdwJtV_RGcQUCklxX0j28,16460
 secretflow/component/ml/boost/ss_xgb/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/component/ml/boost/ss_xgb/ss_xgb.py,sha256=mlZ-lFYaMBXI4LvjkZVW2My0lugn5qtzgJACvsrvu0Q,11694
+secretflow/component/ml/boost/ss_xgb/ss_xgb.py,sha256=mHSxq4UMpWU6JZBD-lUlxsC9XFZHiMMdB5hHItUnJ8o,11712
 secretflow/component/ml/eval/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/component/ml/eval/biclassification_eval.py,sha256=g-hpkE8qFackJKlgZbBh-NNHQHpshARZvb0JOXYZWAs,13883
-secretflow/component/ml/eval/prediction_bias_eval.py,sha256=CuP5XnocNOV6N_GNvgtRZSigrrr0vxRKW9_wIe-R3HI,7298
-secretflow/component/ml/eval/ss_pvalue.py,sha256=Z9yHX6xXMDBwFrpzo3vXoq7nfsMbcfKw08CH6s22dZ4,3798
+secretflow/component/ml/eval/biclassification_eval.py,sha256=YlRncLcvARl9stlblh9jq4vfPujUGJle44kc_iRqiKM,13637
+secretflow/component/ml/eval/prediction_bias_eval.py,sha256=q6-UMIFO-fr3Rhigy_PwrUJtD3aTUlg5ogMM3XM5xUs,7211
+secretflow/component/ml/eval/ss_pvalue.py,sha256=r-Upt6HFoUPGnFBwZX7ACmqvXZ_aP_H6T2Um0u1AqoY,3783
 secretflow/component/ml/linear/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/component/ml/linear/ss_sgd.py,sha256=r8INg_w7hI4zY25wA5LuEXCFeo_FWPTFFzI1VolBp0I,10720
+secretflow/component/ml/linear/ss_sgd.py,sha256=f85IW6B9qjiL3xK07i4jAKPDUI9W-Ft-lzcEr38YaQU,10724
 secretflow/component/preprocessing/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/component/preprocessing/feature_filter.py,sha256=hPQYmYBtg9qbmqxOtWkw9WNDs3DQX_rAmAl2s0jgp80,2748
-secretflow/component/preprocessing/psi.py,sha256=1EiOhnHLwcdaqaLmpuoEwHgtFJROKFHb1OHAjmAOihQ,7266
-secretflow/component/preprocessing/train_test_split.py,sha256=3n8TiGnf3bmSLwMiDuExOrH8ZTvCA-2Ifs3x-jOBybA,3729
+secretflow/component/preprocessing/feature_filter.py,sha256=67AsyuPH8lnQ6slEb38y_WdN6WGCQ5Z_bZ54Txu_9DE,2739
+secretflow/component/preprocessing/psi.py,sha256=lkAjS1f2IXIV5wrVRGPb78LIrz-w_dJiaseZMVYlR5E,7294
+secretflow/component/preprocessing/train_test_split.py,sha256=-pVtHoXMugH_daid1onIU3LonH5RnTE1TTSVgUBkGNQ,3739
 secretflow/component/stats/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/component/stats/ss_pearsonr.py,sha256=qhUurEpX-qh1Ot1HC9XLZ4b_cbLQtkEwy5E2Mf-u5xI,3843
-secretflow/component/stats/ss_vif.py,sha256=HjewlbOxgl8VsIbcRsBrZXAlB-QAphSCUCLO2MByoJ4,3621
-secretflow/component/stats/table_statistics.py,sha256=oLRtS_NY1flcYLxCBPJMB9v1sgbcehS8O0IrW2lmpVQ,3406
+secretflow/component/stats/ss_pearsonr.py,sha256=HFc_42Wl6fwmTrqUk9JfA_UooJ63eWZnogITZjMZeFo,3865
+secretflow/component/stats/ss_vif.py,sha256=qjxVzYLs744HKmoo2BOIWbyAmyNxPhZTFoe5QWr09c4,3637
+secretflow/component/stats/table_statistics.py,sha256=TP9NTH1oo7bfmYaOm9w5mSzYmCORexro-ux-i210uPw,3556
 secretflow/data/__init__.py,sha256=dfSoOj_BQ2QEMiFZXgitx2IHrpJ2qyN4hGfgpKbeKcI,754
 secretflow/data/base.py,sha256=qDwucRS3FHJ0kYYMatUuS_1HL67UxJ2IjKi6UvFBfSw,14806
 secretflow/data/math_utils.py,sha256=wLVcz6eJC7LAYdTXfxERZmwtmxVfchpUkTNKVEW86HA,1108
 secretflow/data/ndarray.py,sha256=DN3GXtz2MAbbohNPvnJvYvSs9Eyy6a2kBJekTNElgh4,24878
 secretflow/data/split.py,sha256=s8HV00DaVKulNvP2-BkH_Y8cWrrnqSc_ANPgKtOQBlY,5420
 secretflow/data/horizontal/__init__.py,sha256=449NmBfIAOGZJO30s-l2ip6Xmzif3PJGdyaTtyEE0Ys,694
 secretflow/data/horizontal/dataframe.py,sha256=Vbc_GmHbDE6o54qEdYSQ12FVRPTc5TPzCdorO5nMZcQ,15042
@@ -44,32 +44,32 @@
 secretflow/data/io/util.py,sha256=fJ0Onq_8Hs8i19MeV1bZgwcLG80OnqCmXUaTHdubEnI,2501
 secretflow/data/mix/__init__.py,sha256=h2bB-DvEkqUeUINstjjMvNzTEOxrwDpp3DML2oD2RoU,691
 secretflow/data/mix/dataframe.py,sha256=vAuf-CVuCrG6U5qUtbcwdv-N2I6ORLzGdHgna7-oRuY,15054
 secretflow/data/vertical/__init__.py,sha256=2ebdxD1yKTCQu5kEZGeQTjCM7t6G-oERwM5bK-rwOiQ,694
 secretflow/data/vertical/dataframe.py,sha256=EAzfauTNycPHfZHteBf-FLVOqiBS0VuLkpzf7mtjF5Q,22944
 secretflow/data/vertical/io.py,sha256=3OFeKanrbIv_BM4j6drGV_WTa8Vo-kYsY_MuzyvIf04,6550
 secretflow/device/__init__.py,sha256=BEn43Wg3C8Pr9-f1ynF-L1HJ4IVKC8R2tU-SqnOOpp4,745
-secretflow/device/driver.py,sha256=ZsK2Re8-AM9qz9oOhgnM7PF_ffA9ssKG825_Qt8DrCM,22241
+secretflow/device/driver.py,sha256=ptyISbRoW-qvrI38VY5jGQA0wq5nmNjHoGmMcr4BA_E,22478
 secretflow/device/global_state.py,sha256=5C6cysrgRG56oB93sBYHr2afkTMjlM3rL1VlD6sBuvw,2981
 secretflow/device/link.py,sha256=s8Dsx-UQ2XcbByFauiix2TXUWIosXhZCvB2T1dtPRJ0,10795
 secretflow/device/proxy.py,sha256=DZTukc-c3-PmdJwpTgoUY7HqaxNWkdGYIepts87akDI,8100
 secretflow/device/device/__init__.py,sha256=WI_jmQfWPY5VQu-b-cUbqK8SqjXHYdRG-NF4iIFTEgc,939
 secretflow/device/device/_utils.py,sha256=H6q--hVRw7lQYzSbIenVM1vcGheQY8kNVK-KOEg-13w,889
 secretflow/device/device/base.py,sha256=kvlkLFmZ0fmAsGQOTR5NC3X6YAnc9NecAPSS_UNoSGQ,2264
-secretflow/device/device/heu.py,sha256=l4F9NUfVE3GSl5J_z0RX30n_XN8fiuOCXAAgAtCybyE,25648
+secretflow/device/device/heu.py,sha256=gdmEtVqidzVLoN4Pjpq7unp25_GkEzgOCg8myrC3Xi8,25805
 secretflow/device/device/heu_object.py,sha256=VS1hEcb8NDV1jW_mn7Wh249BBjEJX14mkNnlshP4ixg,6881
 secretflow/device/device/pyu.py,sha256=Xo3ioCVvB4w8pFdmKGPToTitvUy2uA_f-PvIGj1iVYE,4706
 secretflow/device/device/register.py,sha256=-2PA4a01mX5Rlh-bbNEgto73P_3c4EtaFd1gbdUVICc,3589
-secretflow/device/device/spu.py,sha256=Let3nFqPcSFQ1YX_SAPPpUveC9N9R22VAy4av2pwabA,75769
+secretflow/device/device/spu.py,sha256=CNxJ7F3K0asX-yLj_T9b0h7LpTapfYnrpNUpaD61HYY,85696
 secretflow/device/device/teeu.py,sha256=uUUehACeu-pzEl8zFIcgnzlmv6CC2xG2VbQcDKeoavE,7667
 secretflow/device/device/type_traits.py,sha256=02jIfMCsHRqg9I5_yo6VHh2S7LcG0K4DGz_0IMHcACI,2921
 secretflow/device/kernels/__init__.py,sha256=gWgvZx-2t6_XfGQoVdC9TQNyF-Jm9I9hnn-iVUcNKlE,638
-secretflow/device/kernels/heu.py,sha256=OEVnoebeKU6Jxua1yOCsEPQlhHUlYiQkvpwvvElGkrk,5716
-secretflow/device/kernels/pyu.py,sha256=ftF1UKfABWR84ZjpgvRw5Jnc689SZnOivrpQ4-UQ77c,6842
-secretflow/device/kernels/spu.py,sha256=PCQTMmYptT_uBWBbkhzVCdqmoDtkb1obHtze4AtB3jw,17461
+secretflow/device/kernels/heu.py,sha256=3UHWjilh5rsFrjpBz-HyKMgsNQOUjf1zNYDpLiytS1A,5758
+secretflow/device/kernels/pyu.py,sha256=7Ivfc6DghG-walsT5X9_eRUnx6Ts2uew5GLus3t1s-A,7323
+secretflow/device/kernels/spu.py,sha256=jV6KDjgANl0nYjHbQbsMoaCg4lq_HY3YPVGZ2We-gOc,18937
 secretflow/device/kernels/teeu.py,sha256=o6T8NLSpE9skxJ8eQAqcA4Dk00kC0mMjCzcmkcM1dQc,923
 secretflow/distributed/__init__.py,sha256=lemgXRrzin237GO_Srg0VjaXv0PeJ0daxPVLJT78Qns,784
 secretflow/distributed/primitive.py,sha256=3cSCX6kwIjnB38zzO7TRk9EACZamOhe-NeBg66Yy6p4,6373
 secretflow/kuscia/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
 secretflow/kuscia/datamesh.py,sha256=WotvleSHo3SVI3_i2dovhj54PACHrvdrGwe6G4KYZXI,3329
 secretflow/kuscia/entry.py,sha256=tSJF7Mw4NRbQrsG0Oa_pO30jv87H3lTcUQW5a5d1Lrw,8151
 secretflow/kuscia/ray_config.py,sha256=KVW4VPXLEk2iZBBJf6ZQmHVoYrOn-F2IG1-Y72ARvis,3651
@@ -93,91 +93,83 @@
 secretflow/ml/boost/homo_boost/tree_core/criterion.py,sha256=4JERxTXcVG_Ad_bJP7OgqyOnCkh7H3RN-6gPVtX7rS4,4429
 secretflow/ml/boost/homo_boost/tree_core/decision_tree.py,sha256=1TzZJ8oFAJG8QbkeHHm5tGClrhqN8mRU7yvC0TyIWhU,18576
 secretflow/ml/boost/homo_boost/tree_core/feature_histogram.py,sha256=365Zuy8Y9PczRYMHYbR1QrmxDUBWZ68wca1aQJWBxi0,9371
 secretflow/ml/boost/homo_boost/tree_core/feature_importance.py,sha256=RtYMeKZ4aA8FnihFaTGTfvaeOh0098kvwahY9aj3bY4,2535
 secretflow/ml/boost/homo_boost/tree_core/loss_function.py,sha256=DGW0AuQuZe64a-SQPfr8XKEknMeW3HW-cK4K-ivc_m4,4218
 secretflow/ml/boost/homo_boost/tree_core/node.py,sha256=oogyME57SuPl08PcXtUXtK37Wqsbb7LFSuQ0gp52n-M,2060
 secretflow/ml/boost/homo_boost/tree_core/splitter.py,sha256=pK0qK3Ia5LMziazmdBHTRtum0VjcyohXbCq_nm-qDHA,9846
-secretflow/ml/boost/sgb_v/__init__.py,sha256=8_0WO_-Pu_j0TzFvQVVYgqp74T1mdDtOSQVYOUJPnRY,712
-secretflow/ml/boost/sgb_v/model.py,sha256=B3ikfm8PlnNWAv0zkQv2qoRddUoUSAALMdeiaXK5zU0,7830
-secretflow/ml/boost/sgb_v/sgb.py,sha256=vptPOca1FnWuyfBodFmp22s4mHNhXuj-2061wPrB3Vw,19476
+secretflow/ml/boost/sgb_v/__init__.py,sha256=PlLVBTLKhsH3s_J-hNN79b6fbZSXgt-OXaSdr_D143c,865
+secretflow/ml/boost/sgb_v/model.py,sha256=hyE15TWWFUPlum3iXdZ2SnXO9l7WJ1ipIV7OYAq0Br0,7816
 secretflow/ml/boost/sgb_v/core/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
+secretflow/ml/boost/sgb_v/core/params.py,sha256=q0AEOhMH9IlnzR7TkFpEiOpL55bAtjXGVtSrCazXedE,10152
 secretflow/ml/boost/sgb_v/core/cache/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
 secretflow/ml/boost/sgb_v/core/cache/level_cache.py,sha256=ZgUQVfqg1X-BSnFrJY9R21olyMwjo6tg-ImwgmXJYL4,2642
 secretflow/ml/boost/sgb_v/core/distributed_tree/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
 secretflow/ml/boost/sgb_v/core/distributed_tree/distributed_tree.py,sha256=MIzNQfoqHovdlYahLWNeHjAMkYQXdrFIwCgZizg9-PA,3817
 secretflow/ml/boost/sgb_v/core/distributed_tree/split_tree.py,sha256=mCzz7kCLoreK7U8x3EKzIVLm_DgbmlUTYpj5aRWXOAA,2768
-secretflow/ml/boost/sgb_v/core/label_holder/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/ml/boost/sgb_v/core/label_holder/label_holder.py,sha256=5QFpA8sUU6gikk3FPPN8045RdL5pKdNmEL-ilwBjMGs,9653
-secretflow/ml/boost/sgb_v/core/preprocessing/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/ml/boost/sgb_v/core/preprocessing/params.py,sha256=sX4DS4DPxFF3QsyJBfUuucnlkoMu4pZ6UqfKvnKD9XY,1238
-secretflow/ml/boost/sgb_v/core/preprocessing/preprocessing.py,sha256=lfe99BlRlVBvjz7pvP7DDcgmWZ1XFpQig4IwCuis6rs,2432
 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/boost.py,sha256=yrHE8bJRhML0YQcr0suAJu3FlHATZwPyuUfqzGpKWEw,3706
 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/bucket_sum.py,sha256=4_UqE1Uj1aeXlfjIdo15IjHRCQg0jPmSccqGfnoXL6o,2506
 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/grad.py,sha256=NqxvzxwIsxh7JH_xPmD91RGP2JrfuzFy4UlH4PpQGsc,1705
 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/node_select.py,sha256=8PoaQTt78nSHhZGaFTHlz4Yd6NSSQLKpXKy3Nebi6pI,2807
 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/pred.py,sha256=dJfgo93xDoWcX83mgA0X-uHCGEk3Mp81cN9eExZJ2Wc,1476
 secretflow/ml/boost/sgb_v/core/pure_numpy_ops/random.py,sha256=3tNzEZzMkG1-Gbzfvz0UnEq6dH1gn3Ad0TRHs8mKLko,765
-secretflow/ml/boost/sgb_v/core/split_tree_trainer/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/ml/boost/sgb_v/core/split_tree_trainer/order_map_context.py,sha256=3Bj70hZkuV1IZ8IO2kzGQjEJhwOsE3lk6GcD5Eqvn-o,2851
-secretflow/ml/boost/sgb_v/core/split_tree_trainer/shuffler.py,sha256=OeBZeRYLc1CTbFjeqhlsrs9ZgQDKlygztICVfxuVudg,2787
-secretflow/ml/boost/sgb_v/core/split_tree_trainer/split_tree_trainer.py,sha256=b5Mxw2jwbp9N-MWM-NoO_yFQtAz7WIbATwgbaJGzsHU,4141
-secretflow/ml/boost/sgb_v/core/split_tree_trainer/splitter.py,sha256=be8BquAywE7IH4WUnALuWudtaX6OuTHq1oXRBK91cCQ,5057
 secretflow/ml/boost/sgb_v/factory/__init__.py,sha256=t7GyTozDqc-rZwm3_gwO4I9LMSoU9SHCnQX9Ku37y8Q,644
-secretflow/ml/boost/sgb_v/factory/factory.py,sha256=W1rzpfMBHqH7rp6ROPe1TGO2pu1l1whd_5ugwL3MDeQ,5223
-secretflow/ml/boost/sgb_v/factory/params.py,sha256=IjxmtdtcbDYLxnHR-xP0S6S0mzPg39m0OdL9EJx06E4,5072
+secretflow/ml/boost/sgb_v/factory/factory.py,sha256=Wfx-ovGlnxqAPDqkkr1ETgoPEqGFy-RUj0pSalwcO6c,5543
 secretflow/ml/boost/sgb_v/factory/sgb_actor.py,sha256=nAx5SVdIl2i9dElKZIB-tftQDds9dSGbP1GxRhlx6Po,1450
 secretflow/ml/boost/sgb_v/factory/booster/__init__.py,sha256=MibEqwWWVnUZ4l99himNUDQPAvsfUFtA1LYMjdUK0sk,683
-secretflow/ml/boost/sgb_v/factory/booster/global_ordermap_booster.py,sha256=m8V9WDXsnGf5SNQ_YqfgkToL2VA_I8pVnEUDTT5Uwf8,6065
+secretflow/ml/boost/sgb_v/factory/booster/global_ordermap_booster.py,sha256=0hB_r6DxLHUWgo8IHS4jXYqW-muCVLVxbnXQZZ0lpFg,6635
 secretflow/ml/boost/sgb_v/factory/components/__init__.py,sha256=E0pZSZuufbw_IGmbgPktgBNtWFBFvOLjMDhGbYF5xOA,983
-secretflow/ml/boost/sgb_v/factory/components/component.py,sha256=40kyP4UUm5SxEYDB7x0YtRAsW5c4guyAifC5sc8JmYQ,2284
-secretflow/ml/boost/sgb_v/factory/components/logging.py,sha256=DX-jo3_4iIOTNbtNUr9Z6Z9EcYZfmmUtjFURU2ADc_c,3194
+secretflow/ml/boost/sgb_v/factory/components/component.py,sha256=TH3U3QRdoOIzB0w-2aNaDYw28OC7Fauw6f1Hp4zfayY,2495
+secretflow/ml/boost/sgb_v/factory/components/logging.py,sha256=-RdqAPKJaxSfo9QuAc4igRh2k-mcuHBKm1HVoWO7dMw,3295
 secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/__init__.py,sha256=gcSn648-zTb9QY9WnwAmObWxG2E8DXEwMo8rh4ZI_zw,780
-secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/bucket_sum_calculator.py,sha256=THdJ9QYCJ1j_ezw1_gcQGu9MuBE52i-YdcFzXC4hfu4,6180
-secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/leaf_wise_bucket_sum_calculator.py,sha256=iQnZJhqg5XcUDJPdILfmg4N-PVt1LRhaAEtsSwsQD6Q,6549
+secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/bucket_sum_calculator.py,sha256=T8uAEZPLkqyW8bqHNy7uTpPV4gkoB4642zDrnbL9sWs,6214
+secretflow/ml/boost/sgb_v/factory/components/bucket_sum_calculator/leaf_wise_bucket_sum_calculator.py,sha256=zuvPwiwstoS1nnUlwv7Q2rUQnFWSheRdbAu0kMBGJec,6509
 secretflow/ml/boost/sgb_v/factory/components/cache/__init__.py,sha256=viINyNfu1NuwaOpD2B6jCm8JWlGPcpVf6kKCKaoADf0,661
-secretflow/ml/boost/sgb_v/factory/components/cache/level_wise_cache.py,sha256=w8Ud2OfCh1btRhdWOF6YbF6KA3k815aDRLrnoidaVaE,3580
+secretflow/ml/boost/sgb_v/factory/components/cache/level_wise_cache.py,sha256=hXV7cBtCsulVmtWHVkezgEKsF1n84XATqfeOh-EfDuU,3644
 secretflow/ml/boost/sgb_v/factory/components/cache/node_bucket_sum_cache_internal.py,sha256=mOv_7wMck1sXlnFGGelMDFkxUiw8O-krZ2VqUxnUYew,1934
-secretflow/ml/boost/sgb_v/factory/components/cache/node_wise_bucket_sum_cache.py,sha256=kGVF7eaXriLpKqhm9HrckHTbr8vXpDtCbZqzGy42z6g,3901
+secretflow/ml/boost/sgb_v/factory/components/cache/node_wise_bucket_sum_cache.py,sha256=TVNf7zitdwe6YFC5CcrJeVzf_gaA3IA-ECMXk3dInKw,3959
 secretflow/ml/boost/sgb_v/factory/components/data_preprocessor/__init__.py,sha256=pDTKnLq4lXZ9HcnFW_KbUDWttVyTyNl5Co8EL-Md-5I,666
-secretflow/ml/boost/sgb_v/factory/components/data_preprocessor/data_preprocessor.py,sha256=wE5guGo92xx14ui7UY6XnpDxWTAcJ80Oor1NF1T2kbM,1265
+secretflow/ml/boost/sgb_v/factory/components/data_preprocessor/data_preprocessor.py,sha256=RjqW8z4CYywtgZ5NtEMmOdSnzlbp5BcRUF1c-pC7DY0,1307
 secretflow/ml/boost/sgb_v/factory/components/gradient_encryptor/__init__.py,sha256=mJHnt6biEkZ1MRDSoMnP_ScuOVx4WNAkznK7vA5Tnjk,669
-secretflow/ml/boost/sgb_v/factory/components/gradient_encryptor/gradient_encryptor.py,sha256=JXC4CQBclx6Rg3gc-gK6oAjGPBcLEoZwEo1G6PMn9Uw,6283
+secretflow/ml/boost/sgb_v/factory/components/gradient_encryptor/gradient_encryptor.py,sha256=ue8X7jkdkRzYtnqhBYuAIwHrVyYuUYluPzeBMhhlcxg,6108
 secretflow/ml/boost/sgb_v/factory/components/leaf_manager/__init__.py,sha256=htivChBgoisIQ9radijKCiLE5RrYPxqb-6m7ch8oxNg,651
 secretflow/ml/boost/sgb_v/factory/components/leaf_manager/leaf_actor.py,sha256=H8J3DCswM2WpFKED_2j1XVl3GHmG7lZ9lG--R3tu1Js,1567
-secretflow/ml/boost/sgb_v/factory/components/leaf_manager/leaf_manager.py,sha256=jBzyqKBesd05cB90JZ9n62hhtuXk2y4Mvta9up45Vco,3307
+secretflow/ml/boost/sgb_v/factory/components/leaf_manager/leaf_manager.py,sha256=Nhgb7nomTNcnEF6hx1r8D8B6crVwnkbivsS4zr_9TNk,3184
 secretflow/ml/boost/sgb_v/factory/components/loss_computer/__init__.py,sha256=CQI2V2HnuYGKVDFPW18PFtTrCIr_OeIcDs_sUb18910,654
-secretflow/ml/boost/sgb_v/factory/components/loss_computer/loss_computer.py,sha256=b2gi0uIUuGIbeQbuJOfE4eccYsvN95xKdKAZwju5WIo,8734
+secretflow/ml/boost/sgb_v/factory/components/loss_computer/loss_computer.py,sha256=RSTwHdnFtKWKZlJBV7P_tFaceHanwGFRQteQLjKU6k0,6645
+secretflow/ml/boost/sgb_v/factory/components/loss_computer/loss_computer_actor.py,sha256=hSkhiWEjf_zoEwMK9LF4hc1mHNdfa-mlOr4UbCgSjpE,3748
 secretflow/ml/boost/sgb_v/factory/components/model_builder/__init__.py,sha256=q5AXXsaXtCXvySlnPclrK7-fQL4JrCO_ZMpFRm-hcP4,654
-secretflow/ml/boost/sgb_v/factory/components/model_builder/model_builder.py,sha256=g3c7xBfAU0i9LEtQk2Iy7FoqWXpNtO44G9j_BOmDtnY,2872
+secretflow/ml/boost/sgb_v/factory/components/model_builder/model_builder.py,sha256=qUR9U2uyoprgttS6PlMMxFWz8VOSbLdEy1iWHlkR5pI,2724
 secretflow/ml/boost/sgb_v/factory/components/node_selector/__init__.py,sha256=Qv8b-_-bm0Ozfrz7WVCyYrEHl8Pbz_RcFD7FzoHWnBE,654
-secretflow/ml/boost/sgb_v/factory/components/node_selector/node_selector.py,sha256=LLMg9arQImS-LShLIzrzR8_HR4X9blunNqncPQZwR6c,5985
+secretflow/ml/boost/sgb_v/factory/components/node_selector/node_selector.py,sha256=UWmZl4WLAaaB9TEoP1lEOst4hjMkCBlu9g3wJ-ndAoY,5952
 secretflow/ml/boost/sgb_v/factory/components/order_map_manager/__init__.py,sha256=m8_ADmhhYfJfwX3M6cEf8XUT0QI4GO5zf5jxwgbjuy0,664
-secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_actor.py,sha256=4pb0khjz3mi1QT3_D7U-KzvIUzG2ub63AU1ZoLkvgfQ,4029
-secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_manager.py,sha256=31GpIIMV9o81k00L10_dxiJ4qkhwTegKm6x3GqOxjpM,5568
+secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_actor.py,sha256=5ot6moyUQ0mrcPGqKsjwBjPuvs1HeqGDNgs9ypRhlpI,4002
+secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_context.py,sha256=JumdmmprEjVh1Z1IvDiyoNrQZmkAXdWJRDDt5uLxIFA,2852
+secretflow/ml/boost/sgb_v/factory/components/order_map_manager/order_map_manager.py,sha256=MPh-zodtmdKStsPrYttWLqH_rdxVyz3470UOzmegH6s,5539
 secretflow/ml/boost/sgb_v/factory/components/sampler/__init__.py,sha256=pLaIVelZVEwQc6Xow_BauIPmmZFiD0BZdSA3UO5b6fA,638
-secretflow/ml/boost/sgb_v/factory/components/sampler/sample_actor.py,sha256=NLmwTFMZcRifo57Te1uZxvC982tCVs6wskRzWEgqXFE,2600
-secretflow/ml/boost/sgb_v/factory/components/sampler/sampler.py,sha256=SMH4xoPYNbzFveEpgV2AT1DMGzCrZdir0RpdF4sNyMc,11117
+secretflow/ml/boost/sgb_v/factory/components/sampler/sample_actor.py,sha256=S7ci1_7beler8vcCuJKPQOHF0eFIPCD9qQGtideY-Zs,2606
+secretflow/ml/boost/sgb_v/factory/components/sampler/sampler.py,sha256=AlwfUzZeMYGvf4VnZH9R2Pb8DFS12Bsbti6hGRCP_NU,10776
 secretflow/ml/boost/sgb_v/factory/components/shuffler/__init__.py,sha256=8nV_hO0YjzMQDF3zmf9M6I8LNt_FU28DH_pnFBS2aRI,641
-secretflow/ml/boost/sgb_v/factory/components/shuffler/shuffler.py,sha256=6Jh7IKwAtbk_Nx9mRwtFVpz-hhDgZ2O0-X1pdYw5Jm0,4063
-secretflow/ml/boost/sgb_v/factory/components/shuffler/worker_shuffler.py,sha256=jKkGFP17rzGeq-r6trESaKJ7fw-BbJ55ApUZDxIp5kA,2032
+secretflow/ml/boost/sgb_v/factory/components/shuffler/shuffler.py,sha256=a2mgRM6_eYSbVjD8IRMVlOjTD1XkdTLeIpJXUVAXeNM,4137
+secretflow/ml/boost/sgb_v/factory/components/shuffler/shuffler_core.py,sha256=AvtwlWFR0TC8P59ROukTFGVi9Y64GVApU8odaVunSCE,2825
+secretflow/ml/boost/sgb_v/factory/components/shuffler/worker_shuffler.py,sha256=gpwuM1dXjQCdr6Sldd-XJvN6t9A8II_pC-QtABlx3OM,2010
 secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/__init__.py,sha256=1aC7AXAfe8FlqrXaBIX8nwhS5HzRGufaajcw2BVxZxg,749
-secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_heap.py,sha256=QYGB0HdEfw_Rfbi1uL5thfZkJmCDF5k_Q4P2ZSfwv2M,3286
-secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_manager.py,sha256=NUK0ZXj0ceQxgXxmgs4fkbEh-y-hSG9VzLkXd0tL1CI,3225
+secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_heap.py,sha256=RHyYXcAQB_rLCdi8x9VHoBNkNvYtrucaP9cIN-113OQ,3401
+secretflow/ml/boost/sgb_v/factory/components/split_candidate_manager/split_candidate_manager.py,sha256=r8gZ305lhsICH0sjTnpK2ZTZy3f9MI50NM_YvBay7jI,3409
 secretflow/ml/boost/sgb_v/factory/components/split_finder/__init__.py,sha256=UqJLiIcJs-7gyrYTfAPlKMpnGEsuiky1dmkhlrncucM,651
-secretflow/ml/boost/sgb_v/factory/components/split_finder/leaf_wise_split_finder.py,sha256=66Ad5a1fhGTzPwKm3T3U3JFm36sLc76gxSpr0Vd18pU,3568
-secretflow/ml/boost/sgb_v/factory/components/split_finder/split_finder.py,sha256=QuRdd8eKF1tblC8yKY83PEjAmHBtL0EhZB87nWpsMiY,4482
+secretflow/ml/boost/sgb_v/factory/components/split_finder/leaf_wise_split_finder.py,sha256=10UtNzTFDso1NOy7jMr5K4uxOCGFeZJUDtFjgv617lY,3217
+secretflow/ml/boost/sgb_v/factory/components/split_finder/split_finder.py,sha256=jjbw7GlnTRizlsWSl8-4SnOi359prhY5DvNNKS8hfSQ,4173
 secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/__init__.py,sha256=8zWMI0KJjM_qVg8hlKQrPuIB8rL64mSSv3Zsawwl414,667
 secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/split_tree_actor.py,sha256=zzX3YGR7nQUvAsYVesFyKPlgGG6cVD5SYgb84CLDWaU,4546
-secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/split_tree_builder.py,sha256=vHA13fEyNQUT02jp7_43VyDFtZC67tPAPvlwc_Fc92k,6505
+secretflow/ml/boost/sgb_v/factory/components/split_tree_builder/split_tree_builder.py,sha256=g-ViHG2RexuxtS2gp5RbeG9hHzusJ14Kjqystd0Gsog,6581
 secretflow/ml/boost/sgb_v/factory/components/tree_trainer/__init__.py,sha256=IlYcQbsoiejlqAmOMSd-l0CCRlUI7nmjgqTZfq1YCIc,226
-secretflow/ml/boost/sgb_v/factory/components/tree_trainer/leaf_wise_tree_trainer.py,sha256=TlRE4pC7p2RI8aHoMB7SdKTM5YY0s8T9onZysvYIowk,13819
-secretflow/ml/boost/sgb_v/factory/components/tree_trainer/level_wise_tree_trainer.py,sha256=W65fUdyCPZbMYdoBmt-7PTl0ck7ffERvJU8p-pSatXE,12185
-secretflow/ml/boost/sgb_v/factory/components/tree_trainer/tree_trainer.py,sha256=zcFR9uLG5yS8s6oNOvmvGjcVSarKknHBmZoOzWz8XQQ,1294
+secretflow/ml/boost/sgb_v/factory/components/tree_trainer/leaf_wise_tree_trainer.py,sha256=VSczDKA_5HOteuVNJz_2mNseblUDtql-eyUtVHQpz5M,14378
+secretflow/ml/boost/sgb_v/factory/components/tree_trainer/level_wise_tree_trainer.py,sha256=-Z1qRsFcT1nQrvV88BuhdoaFrn4CiwI6LhGkg_RX1oo,12632
+secretflow/ml/boost/sgb_v/factory/components/tree_trainer/tree_trainer.py,sha256=aNiSVHxIT_zv3uQjQwWOsshtlkmq5Gjc7_AIYiW8Ams,1324
 secretflow/ml/boost/ss_xgb_v/__init__.py,sha256=mWFWnNM-indGXq7-H80y9KisAWayUQtH0MV0nVG0ymE,661
 secretflow/ml/boost/ss_xgb_v/model.py,sha256=kmfdh9D7UOcTYal5SYTrCSAC0623Qce603md3DNT2kQ,19581
 secretflow/ml/boost/ss_xgb_v/core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 secretflow/ml/boost/ss_xgb_v/core/node_split.py,sha256=_DQ01D3dn-SKpJAwS6NNDKJXNp1Cro41Fpip_AzKO1U,8801
 secretflow/ml/boost/ss_xgb_v/core/tree_worker.py,sha256=lUBxaNang_IdoCWwr5ryWE9WOEyMcuiWxAW6iu1tjQM,7747
 secretflow/ml/boost/ss_xgb_v/core/xgb_tree.py,sha256=JNeEdH_Boe7mJ8hNjCu0nnuNm2LIfIdiNHBWmDIPF_w,980
 secretflow/ml/linear/__init__.py,sha256=S6Te5j_pKa2N860u3hrjDaxnN2hiW3c_On6JDidhxQM,1013
@@ -196,15 +188,15 @@
 secretflow/ml/nn/__init__.py,sha256=iOeB3Fi78uSNTBvlkmcpgkRLo1IPw7ipn1yj6rQrXu4,698
 secretflow/ml/nn/metrics.py,sha256=ad-02DE5ljcDYVosAjcH6P4WK2_2nWFmav0JY-9XRbI,8067
 secretflow/ml/nn/utils.py,sha256=h7P7qh8GBCMI0BSR-8vAazY42kPLnB6A_oL2Qj2Hono,3018
 secretflow/ml/nn/applications/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
 secretflow/ml/nn/applications/sl_deep_fm.py,sha256=w252NcasCrQwgiO1pPOjZ4kh9Yrld2HW3-Q9I14-7eE,5261
 secretflow/ml/nn/fl/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
 secretflow/ml/nn/fl/compress.py,sha256=Vuhbq4PCfEy0rsuLEEQNAshlr6Qxl14MBRgQO7h2pLg,1743
-secretflow/ml/nn/fl/fl_model.py,sha256=xAjHm8Y5X1OmJQL6of9MN3NFPmFfHcmZWkh8xz79L3g,30826
+secretflow/ml/nn/fl/fl_model.py,sha256=fYKti_4Zm8LyEK7Vqjd9IjjjqrLF2XCLmw0i3iCQ3dw,31183
 secretflow/ml/nn/fl/strategy_dispatcher.py,sha256=C2uR__I-aepXfC8RlOKue9Y_YBo7wnR5JISWegEZkJE,2152
 secretflow/ml/nn/fl/utils.py,sha256=ZzX_JJqba9f7O7cuc7OhoAksWM7jyfmYD1tFlkLjJMo,3279
 secretflow/ml/nn/fl/backend/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
 secretflow/ml/nn/fl/backend/tensorflow/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
 secretflow/ml/nn/fl/backend/tensorflow/fl_base.py,sha256=TAwxMXhX5euL4jU7PrDXdKAYz5ce3NkTZ4_tQKQ-oYA,12678
 secretflow/ml/nn/fl/backend/tensorflow/sampler.py,sha256=noozDHkSAhHRWyui5YFrwEO9hAtn9mtQyESsF8Ooy7o,4369
 secretflow/ml/nn/fl/backend/tensorflow/strategy/__init__.py,sha256=HkHwvlJJEqZUA1kQ_TtPXDhfRu7cygrdG4X4R9pp6mg,904
@@ -221,30 +213,37 @@
 secretflow/ml/nn/fl/backend/torch/strategy/fed_avg_g.py,sha256=9WkDVZ69S8jm6w1KHMbCSYu5w0ddFiE1xekDo6FSzp4,3482
 secretflow/ml/nn/fl/backend/torch/strategy/fed_avg_u.py,sha256=7oSPq97gpMYJ6QgC-UVk3YkW5j21MOseXOzr0X2PMt0,3324
 secretflow/ml/nn/fl/backend/torch/strategy/fed_avg_w.py,sha256=LpJBwjb6r8QGVbu48ook0DUrqTh2OMNFRkI9aBFYjlA,3381
 secretflow/ml/nn/fl/backend/torch/strategy/fed_prox.py,sha256=DBIdG6LQo3ZXM4YkbSzG3hpUM0TILORyhfi2wF4KF7U,3838
 secretflow/ml/nn/fl/backend/torch/strategy/fed_scr.py,sha256=UMNem1LWcHk8yb0Rx90fMCmIm-LD9GbsnEwVh1LQz3g,5249
 secretflow/ml/nn/fl/backend/torch/strategy/fed_stc.py,sha256=12vUMonxNQxUmcPNYKlfAFfARE94nTifrfxeE0rm9yU,5258
 secretflow/ml/nn/sl/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
-secretflow/ml/nn/sl/sl_model.py,sha256=LeXWmQEgWjZVB-fOlvIRyOdrG5PF4cG8qJvbwyPcyeQ,30963
+secretflow/ml/nn/sl/sl_model.py,sha256=xP_33FDyYzLe6SFNN_CIr78mBSjQdu9yq2oFKGbu_Uc,34368
 secretflow/ml/nn/sl/strategy_dispatcher.py,sha256=1Sac34P5sXSlZKJqYM2OvuJcQNt9KHjcOZ5A74QzJ-0,2395
 secretflow/ml/nn/sl/agglayer/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/ml/nn/sl/agglayer/agg_layer.py,sha256=C_aSahbe3qB9GKUbHv_WFX8VLQ3DR0dLmazKxkm55W8,16475
+secretflow/ml/nn/sl/agglayer/agg_layer.py,sha256=kYQwkmMvunAX01hbFOtDDqbbUfelG8OYWfTvaTpdtFY,19647
 secretflow/ml/nn/sl/agglayer/agg_method.py,sha256=nqKX6MJINLKksUemgp1IEtRIWWne9TqoA8_kNgqNSO4,3450
+secretflow/ml/nn/sl/attack/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
+secretflow/ml/nn/sl/attack/torch/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
+secretflow/ml/nn/sl/attack/torch/feature_inference_attack.py,sha256=eSziX3roVBujxlMvXVMfm8rXhqOW6GePq0ZQU5gDekA,14581
+secretflow/ml/nn/sl/attack/torch/label_inferece_attack.py,sha256=eWFZt6CN4ERejBjc8J5Er9Q_4FKjrLLMH_GID2hNU18,21096
+secretflow/ml/nn/sl/attack/torch/metrics.py,sha256=gK4gDSCkOvBqsQ7oMrPJLmzlTWqT2oHc8Iu7jTVYlr0,2688
 secretflow/ml/nn/sl/backend/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
 secretflow/ml/nn/sl/backend/tensorflow/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
-secretflow/ml/nn/sl/backend/tensorflow/sl_base.py,sha256=wHo-0xpXZ3mdBwDv4XW5362JsB1LbFZwLxflGatXbfk,27962
+secretflow/ml/nn/sl/backend/tensorflow/sl_base.py,sha256=IdLhozRzmT1Ek-Pp_g5w23FjwILhwFzN8_uFto46t7o,30692
 secretflow/ml/nn/sl/backend/tensorflow/utils.py,sha256=My5jPf99yW11QGgp-XRAwMv7TdfsnxL8JOnMorcwiUc,4487
-secretflow/ml/nn/sl/backend/tensorflow/strategy/__init__.py,sha256=Wiv8YukYUdZMHtdjpFdBD7tPt0yiodlUiMkgEdpqWnc,753
-secretflow/ml/nn/sl/backend/tensorflow/strategy/split_async.py,sha256=IsBY3J7mq1vRCXhSBDktMXeLb6OI-OKtsgIDmT1Rvis,6210
+secretflow/ml/nn/sl/backend/tensorflow/strategy/__init__.py,sha256=TJJLcr5DQI_ELsmyrojc7mZK7kOkz3AOynimvkk8yVw,820
+secretflow/ml/nn/sl/backend/tensorflow/strategy/pipeline.py,sha256=dvXVXncQ6DsQbViYA4EIPo0YMmeb03Qjap37zlCTi2A,6768
+secretflow/ml/nn/sl/backend/tensorflow/strategy/split_async.py,sha256=EOjvz1DmE1uBv0hMOdMRCS5jwe_4V3_6JOUjN-M9KDg,6398
 secretflow/ml/nn/sl/backend/tensorflow/strategy/split_state_async.py,sha256=Vbqc5EhsDkkVpIYOUlWi0Q4YL6MnixctBB2I_Y7A_0s,5140
 secretflow/ml/nn/sl/backend/torch/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/ml/nn/sl/backend/torch/sl_base.py,sha256=YVlaF7hkyCVQrBU0sZ312I7jxentFRpgUksLbFOeHm8,24562
+secretflow/ml/nn/sl/backend/torch/callback.py,sha256=ytUQfXoNZMMcVIXXHnpEP8XTwpwlRYv1PP_FKypvVCQ,1131
+secretflow/ml/nn/sl/backend/torch/sl_base.py,sha256=r8C4_2-APBO4OknlowAewwrrNe-c330In7h20Pt-6K0,26221
 secretflow/ml/nn/sl/backend/torch/strategy/__init__.py,sha256=4XLIiLitj8RCWQiQrBuqO5vN4lrZ1z1smdPgaOb-Ky0,652
-secretflow/ml/nn/sl/backend/torch/strategy/split_nn.py,sha256=UAwQZhxlSdahr2QrBAi0tJ9aAzSFOrRWh1A7Xps6v3k,3569
+secretflow/ml/nn/sl/backend/torch/strategy/split_nn.py,sha256=6DhPMwWNe6i2s-PhN4Wddx1Uy2Crzt8cuQeEtHptz50,3749
 secretflow/preprocessing/__init__.py,sha256=WArCKEUzlmqFHl08W4XvmFNSR24fShDAmUH3uF_cexk,923
 secretflow/preprocessing/base.py,sha256=hi3nD9aE7c4gNyfdhrOdHVoZ6KKugP8GgUqziEM2ddI,1231
 secretflow/preprocessing/discretization.py,sha256=U5iTyPDKc13_-ZiaNxXCmImUs4h_DKhR3cSRbobSkh8,11845
 secretflow/preprocessing/encoder.py,sha256=uUPDtydfMUh2gGmx1ZQyQBH5moMTsuvIpkYJEVUOQ8c,13154
 secretflow/preprocessing/scaler.py,sha256=-8xLo-suvMmiDbAONdRR9kbKv9BGTX28UQ1pCooHXlU,14116
 secretflow/preprocessing/transformer.py,sha256=1dlC3DVSoARXphMa2x2_sxbhkiVczRlElsDaAN7-aT8,5162
 secretflow/preprocessing/binning/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
@@ -256,19 +255,19 @@
 secretflow/preprocessing/binning/kernels/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
 secretflow/preprocessing/binning/kernels/base_binning.py,sha256=_0whbDbFJ4wL5lxtacAIQTikDcmCKIhHk9p2ku65shs,2524
 secretflow/preprocessing/binning/kernels/chi_merge.py,sha256=MtsKkYLkt1KCyW48qyh384zO5V65lGRHP0H2M9YD-i0,5281
 secretflow/preprocessing/binning/kernels/quantile_binning.py,sha256=ZZqP7k4vSxyY1eQUvcNZccjda_eFsVWjVddZhACF0aE,5291
 secretflow/preprocessing/binning/kernels/quantile_summaries.py,sha256=wIzrTEr8goAn9sMOdFdhVrSq3npChJrpOXlyQpkzc7E,6517
 secretflow/protos/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
 secretflow/protos/component/__init__.py,sha256=ZUqwQZnz9z36XuNKu6t6TmLqDv7UOsGXmST0SKjZ6hI,585
-secretflow/protos/component/cluster_pb2.py,sha256=hAnomp8SrO7KidUKLw8p_6NZPlZxBiRkH-FnkPwg5z0,22818
-secretflow/protos/component/comp_pb2.py,sha256=TWLYL0sh2sr7j8_K9OwemQohf1VDG-Am8lyi_SuopXk,35269
-secretflow/protos/component/data_pb2.py,sha256=0T65Mgm-G6GpVUF8N4v73l_6S1M0ciREmAiLHGw13ME,20329
-secretflow/protos/component/evaluation_pb2.py,sha256=DfZVEckVDotWkgAoCAYnejAXDyorZI7gmYAPfLZndDY,7199
-secretflow/protos/component/report_pb2.py,sha256=JIihz1WbDKX9HDn7NE97_mlOJpTsBm-_ADlxQZdZTI8,24482
+secretflow/protos/component/cluster_pb2.py,sha256=RJzT1XS3vtQ-Z3IwLUTrhf9ADLsHr8TQZ223NyKnApM,22918
+secretflow/protos/component/comp_pb2.py,sha256=UcfYL2vmvBAvOcHWmNalnQk-0-wmqWZTCg-vY4BRY3c,35369
+secretflow/protos/component/data_pb2.py,sha256=YuaqzWR-mV9qQGUeCh6tkw60tBcLbvQdRoPer7m6rR8,20429
+secretflow/protos/component/evaluation_pb2.py,sha256=8Giy5a3ZhqKmdI21umXp9ajhViI6SWxyapFfb9RytF0,7299
+secretflow/protos/component/report_pb2.py,sha256=R_z_RmsuhxhEwMRJ794HAqsxr1OBUcEDwZrfWcQEI9Q,24311
 secretflow/security/__init__.py,sha256=XSa8vQ3YNjHEWbHVXrM9MwxonaxndtCOi50CipzrU90,941
 secretflow/security/diffie_hellman.py,sha256=uIAICEjHJZKNYP2A50R6nWU6qGkVGSgGNIV9CXn48z0,11579
 secretflow/security/aggregation/__init__.py,sha256=QLB9vulzN399OMy9T_S6ZA0iaeKuVbRS9L61Hpz2F8o,946
 secretflow/security/aggregation/_utils.py,sha256=KgXRCCxr9PQsinDALN4a5Dn0jycSF3x3qYqq2kcTMv4,1039
 secretflow/security/aggregation/aggregator.py,sha256=smdPapUc9I3LdAuneIIbdKOWcGvWKOUeLxTlN47o85g,1072
 secretflow/security/aggregation/plain_aggregator.py,sha256=ETwarnTPe_NTYvCx61SF3lHDEfJOr_-xoc0qtuwWWJM,4474
 secretflow/security/aggregation/secure_aggregator.py,sha256=HiSf8dostWXMxA2N46NpCU2icr23cAsEUWTNK94yfCk,12148
@@ -276,15 +275,15 @@
 secretflow/security/aggregation/spu_aggregator.py,sha256=KYUUmwf99bXamZ2CVxTZGvKtnOCEoBJ262ehdiKegfQ,4179
 secretflow/security/compare/__init__.py,sha256=yVG0XmROdF0Qq3LYeXJb8SzVuXtu-Ii8yoCCWNahoRY,858
 secretflow/security/compare/comparator.py,sha256=3L4W9SKlWi9FDKOidGfOjlPb0SB5XogJWdKbDnBB2mw,1066
 secretflow/security/compare/device_comparator.py,sha256=e5P0yTEqRFTmr1a8fOJtRt-NurXb5pJ-d8bT9BY-Rhk,2220
 secretflow/security/compare/plain_comparator.py,sha256=wpn4UusUHkFeCAUTvUw_SIhQB-7GClaD50z7QcCCMwg,1871
 secretflow/security/compare/spu_comparator.py,sha256=iQCd8z0UpkKD3h5AhPklQ9oP-8KAGkTUNyyRyJ2qvDg,1762
 secretflow/security/privacy/__init__.py,sha256=KSR4jJRon2SyKyMpwLIcmfssZA4vqLIqbrsXpVGhgPQ,948
-secretflow/security/privacy/_lib.cpython-38-x86_64-linux-gnu.so,sha256=ui6OIBLL7N3LsS1PXiFzhok608aCCwaPNlF-pxwbtn4,2222040
+secretflow/security/privacy/_lib.cpython-38-darwin.so,sha256=KoWwdhdmuJqm3PK3afwbdSCmuT-Fi4JG_BEpke8rTN0,2208436
 secretflow/security/privacy/strategy.py,sha256=ojMMuNe-Ciq3QBhBEa-Kig8G5nGrwXeX-d04-sqHXBg,3012
 secretflow/security/privacy/strategy_fl.py,sha256=P4G0f1UVFe7EHt6sVeDPmedX2DWPvh2tIYHUsCKgpzY,1855
 secretflow/security/privacy/accounting/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
 secretflow/security/privacy/accounting/budget_accountant.py,sha256=0rprsewY4wQcbkLSuh4cEOG37O7vnam9FZ2zjGEn-yk,1814
 secretflow/security/privacy/accounting/gdp_accountant.py,sha256=J7pOwbiStkOuX37pIGkx7RnEdH7DwGRgUXE3duskRPk,2508
 secretflow/security/privacy/accounting/log_utils.py,sha256=cbyOYkWMuJxJgdL8zT7DidWHuA-RPl-W77CQJ1aLmVQ,3670
 secretflow/security/privacy/accounting/rdp_accountant.py,sha256=QAowmFm3Vyd-cOH_i0y1mI3YnagbthHtq-Nwnv7yEEo,5702
@@ -292,48 +291,49 @@
 secretflow/security/privacy/mechanism/label_dp.py,sha256=T2bqHNDOLzNERwCdZbymP46n9W0Nzt0pZbNDhFJ3yKA,2567
 secretflow/security/privacy/mechanism/mechanism_fl.py,sha256=BPkkaEAGnkpDLTgz9BNSqEOrSB2K77bgmPPrdULNHfo,5420
 secretflow/security/privacy/mechanism/tensorflow/__init__.py,sha256=u6T9L2TljKVn26sCrg4gek8Z8H4mHsor6A5pwSMsy4k,668
 secretflow/security/privacy/mechanism/tensorflow/layers.py,sha256=yavu1GS4XxyeVcSngBSV_-sM_9mNJh1rzJHpK3KbM_8,2750
 secretflow/security/privacy/mechanism/torch/__init__.py,sha256=08fKvSZRIawpQwSXjiZLSvD0PRwI-hrUSfvhIEvCdMQ,668
 secretflow/security/privacy/mechanism/torch/layers.py,sha256=XTWd4fJJjBdzT_vnrYm6y1A-WV8hZwSxUrUdoOBzFp0,2115
 secretflow/stats/__init__.py,sha256=kLChZTqe1bFsNY-VX4Yy-OxJyfYvBARMm7Xn4jV3Gds,1195
-secretflow/stats/biclassification_eval.py,sha256=rTykQ0O690yT2pRwoT6Eomg8XbNCSaxt6znW2QGJXMc,4320
-secretflow/stats/prediction_bias_eval.py,sha256=jjq6uiu5iY_sMFJXT3zWw19xx_2WuUrk8QaDPr-LL0c,3404
+secretflow/stats/biclassification_eval.py,sha256=zCjJQfids_wkIygt6lmfZGQaZuPcAMBZ6rbGLp0W-iE,4356
+secretflow/stats/prediction_bias_eval.py,sha256=_gn8tU_hZRg_qLWl5nBYbHLHRtoChMZ4oHcej1wqPKw,3440
 secretflow/stats/psi_eval.py,sha256=2ACH222UQj5RtOz4fbCE-gMIbd7SvCuyWv4AgZoOnP8,2386
 secretflow/stats/regression_eval.py,sha256=YdmfF2L5RExr5Yr_UeHT7CMvT-F5AXpiYy8pFUvcSlQ,2995
 secretflow/stats/score_card.py,sha256=P8MyXGvVU1aV11f_Vkd57mEtLVZ3Yz5OFlq3BRekTYA,4421
 secretflow/stats/ss_pearsonr_v.py,sha256=MS5ASr1P_5OoiC5vL2rPGnazA7iFsG1N4CsS6MdybIQ,2421
 secretflow/stats/ss_pvalue_v.py,sha256=Zfvp5gG2_Qgj31AFZkYsTAE2pSj1Re9Q74B6JqFjnSw,7265
 secretflow/stats/ss_vif_v.py,sha256=jN0M-1TTpjK10TEjgkdI1NGvAIM_kiVDdmDIUyWAA-s,3631
 secretflow/stats/table_statistics.py,sha256=kOGf2r2Az3pZoQsFAHuuyJPkMOaGcAYQ4usuzJ9xdVQ,3450
 secretflow/stats/core/__init__.py,sha256=2Zn61l7m0wWJf6-2jJnihjcOnmlcXaxg8r5wiP-VEu4,750
-secretflow/stats/core/biclassification_eval_core.py,sha256=igi37h8UB2yVnho2gdjae4rzrsmyxSamXaBthFhw68Q,21056
+secretflow/stats/core/biclassification_eval_core.py,sha256=vxMOzSfNXkkgBY5CDTc1HKmSFzWhi0xYDdHBxsetkpQ,21014
 secretflow/stats/core/prediction_bias_core.py,sha256=r5YFgQlVZCk7D_owHBq9lTF9_9HiD9XwDcFOx0LacCg,7200
 secretflow/stats/core/psi_core.py,sha256=gYMoHQfNpWh5VFgNv-aA6H6bWIeXIFxtcrZePNLfVuM,2802
 secretflow/stats/core/utils.py,sha256=VL6W4gR6ciggEoRYLEUutViWql5cnsjyjQtSYqdkA24,1776
 secretflow/utils/__init__.py,sha256=LsaruislCBOGxAxIhzEQEuRvZP0bW9-M3OgabZQiwRA,662
 secretflow/utils/cloudpickle.py,sha256=D9Lf_I_I4iJFmahoM2OOBvfQPLcs2McBHbMuMgqr-nk,13370
 secretflow/utils/communicate.py,sha256=hOVwTsAXK2KEeKbnhEg9wKLhyLd0f00DwzkqUuAK5uQ,400
-secretflow/utils/compressor.py,sha256=-3gHp5r6SJ6yMHGWwC9ey1uA268INttkenL_xRxrzCQ,11981
+secretflow/utils/compressor.py,sha256=awk0YAW_vJ0WnRh-GYtiK8w0ECK7wG2A1So-3ioYk6k,18161
 secretflow/utils/errors.py,sha256=rRnEwSIU-ewJ4JXNBkbn_bk_ifGGaWS2CeeiAbYpGHg,1084
 secretflow/utils/hash.py,sha256=aWbXLTdmXc_xOl5FlXQW7Yw93iujdqu2X_01t6LF8cU,870
 secretflow/utils/io.py,sha256=vDesPaH9sKe8X7qdVDEaFhF4RksVzowNIJ-YuvYZ-TY,978
 secretflow/utils/logging.py,sha256=OZHkbOTsiN17do3t1iT6lbjW9JrdqJyBPEx4CCskF-8,964
-secretflow/utils/ndarray_bigint.py,sha256=awneTcQwL2SCMsE5dnv3WXGg9ZIWGM6MrEHteTldCgA,2842
+secretflow/utils/ndarray_bigint.py,sha256=DwQDHVH_ZFWpDjcUph68ju1zOZmomiJF_JATSuLANVc,2925
 secretflow/utils/ndarray_encoding.py,sha256=Fs-q-409SG7Jh2mgBy64uDv7F2kJygrRpgZKFgP9l4A,2489
+secretflow/utils/progress.py,sha256=b_7f03dLgM4M9bo-Y7midVuTvuvo624TdQjesjZ3DLc,402
 secretflow/utils/random.py,sha256=YR5ZysYl6AyOMV0qV0qMxQYzsZ8O7YqYrRnCBseMWIg,803
 secretflow/utils/ray_compatibility.py,sha256=gVO78f_ZzuH0yaZGMtjn5v0NP00p_cZ_BdmRNgZ6XeI,1330
 secretflow/utils/sigmoid.py,sha256=cMw5zBMeJ9yYT4TrJdfsMr6PowQyxkNftCOIgMaSMHY,3930
 secretflow/utils/testing.py,sha256=JsZrw5PMsyeBJp1h-fWQHpzGcNHn13NYxdoHhFCGE8w,3308
 secretflow/utils/simulation/__init__.py,sha256=Wd00rjlQJKH-aY2LbPoVGjBo932U5pCSy9wAKN9ekjw,585
-secretflow/utils/simulation/datasets.py,sha256=RplOCPT9bHwxTDXbyIWwKSXVt67G8wcjypyF-1ntCiU,30743
+secretflow/utils/simulation/datasets.py,sha256=Ts5P5GfLAJRIyhj4lDZ3zIJuijHHrxQaM6Ug-g7m6hs,30720
 secretflow/utils/simulation/tf_gnn_model.py,sha256=385s0qZEOACN97MYQEPJX-vSVNEeQq8xb-nVYHEV7ls,8941
 secretflow/utils/simulation/data/__init__.py,sha256=KD77PGi-0ECpneFzilAxvo-N9fknaBc9CgiB2qlfuZU,769
 secretflow/utils/simulation/data/_utils.py,sha256=YauA2ujZLhodRrVGF1xMLVIHqSrLSbXLB5iOrxxO45I,2548
 secretflow/utils/simulation/data/dataframe.py,sha256=CRcnioLLM0o2bwy8SoLmx21S0icW6GG5yqfnlxkzAvY,5389
 secretflow/utils/simulation/data/ndarray.py,sha256=YkH4ZkIJYd9sKIB0YAueGlgb77iGQ9xEGs_mQTKHCbQ,4093
-secretflow_lite-1.0.0b3.dist-info/LICENSE,sha256=QwcOLU5TJoTeUhuIXzhdCEEDDvorGiC6-3YTOl4TecE,11356
-secretflow_lite-1.0.0b3.dist-info/METADATA,sha256=-ry6mA-FCy4TatYCc9U1vQftBUTZWxUt4Wp-cJyODIQ,3799
-secretflow_lite-1.0.0b3.dist-info/WHEEL,sha256=Q7wC2KUfz4qeyN-PnIiRDulJuXUHOlLP5Wm9pL_uT7s,111
-secretflow_lite-1.0.0b3.dist-info/entry_points.txt,sha256=SHNAJtXkknDpzavWGYDfIpmBVdlRgeJElng358N3cvg,50
-secretflow_lite-1.0.0b3.dist-info/top_level.txt,sha256=gnx5PWbIu_7IpkW_zzqjwN5TFcosT0RCQGqKRUlVt7I,44
-secretflow_lite-1.0.0b3.dist-info/RECORD,,
+secretflow_lite-1.1.0b0.dev0.dist-info/LICENSE,sha256=QwcOLU5TJoTeUhuIXzhdCEEDDvorGiC6-3YTOl4TecE,11356
+secretflow_lite-1.1.0b0.dev0.dist-info/METADATA,sha256=_-6-aKzct0ndo434OIzIlwxokoCQzxaCPIIrbMfFF1c,3804
+secretflow_lite-1.1.0b0.dev0.dist-info/WHEEL,sha256=lbCZQl32rz70_Iou4S1h8sUmNh5Rak_YxaqmnmkwnjQ,108
+secretflow_lite-1.1.0b0.dev0.dist-info/entry_points.txt,sha256=SHNAJtXkknDpzavWGYDfIpmBVdlRgeJElng358N3cvg,50
+secretflow_lite-1.1.0b0.dev0.dist-info/top_level.txt,sha256=gnx5PWbIu_7IpkW_zzqjwN5TFcosT0RCQGqKRUlVt7I,44
+secretflow_lite-1.1.0b0.dev0.dist-info/RECORD,,
```

